---
title: "Analysis of the performance of US-listed securities during COVID-19 Pandemic"
author: "Pui Lam Paulina Suen"
date: "2024-10-27"
output:
  rmdformats::readthedown:     
    
    lightbox: true
    gallery: true
    toc_depth: 2
    fig_width: 9
    fig_height: 9
    highlight: espresso
editor_options: 
  chunk_output_type: console
---

#   Data Preparation
```{r setup, include = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = TRUE) 
```

```{r}
# load library
library(readr)
library(tidyverse)
library(lubridate)
library(zoo)
library(stringr)
```

## Extract user holding from multiple files
```{r, eval=FALSE, echo=TRUE}
# Extract user holding from multiple files

folder_path = "C:/stock_popularity_history"


file_list <- list.files(folder_path, full.names = TRUE)

get_data <- function(file_path) {
  file_name <- str_remove(basename(file_path), "\\..+$")
  data <- read.csv(file_path)

  data <- data %>% mutate(TICKER = file_name)

return(data)
}

data_list <- file_list %>% map_df(~ get_data(.))

user_holding <- bind_rows(data_list)


user_holding$timestamp <- as.POSIXct(user_holding$timestamp, format = "%Y-%m-%d %H:%M:%S")
user_holding$timestamp <- as.Date(user_holding$timestamp, format = "%Y-%m-%d")


user_holding <- user_holding %>% group_by(TICKER, timestamp) %>% mutate(users_holding = mean(users_holding)) %>% distinct(timestamp, TICKER, .keep_all = TRUE)

user_holding <- user_holding %>% filter(timestamp >= '2019-08-20')
write.csv(user_holding, file = "user_holding.csv", row.names = FALSE)
```

## Data import
```{r}
# Task 1 - 3
Stock_data_1 <- read.csv("Stock_data_part1.csv")
risk_free_rate <- read.csv("bill-rates-2002-2023.csv")
industry_code <- read.csv("industry_code.csv")
fama_french_factors <- read.csv("Fama_French_factors_daily.csv")
userholding <- read.csv("user_holding.csv")
```

## Transform date format
```{r}
# Make sure all the "date" are converted to ISO 8601 format
Stock_data_1$date <- as.Date(Stock_data_1$date, format = "%Y-%m-%d")

risk_free_rate$date <- as.Date(risk_free_rate$date, format = "%Y-%m-%d")

fama_french_factors$date <- as.Date(fama_french_factors$date, format = "%Y%m%d")
fama_french_factors$date <- format(fama_french_factors$date, format = "%Y-%m-%d")
fama_french_factors$date <- as.Date(fama_french_factors$date, format = "%Y-%m-%d")

userholding$timestamp <- as.Date(userholding$timestamp, format = "%Y-%m-%d")
```

```{r echo = FALSE}
# Inspect data structure
cat("Inspect dataset structure: \n")
str(Stock_data_1)
```


#   Data Wrangling
## 1. Handle missing values and duplicate data
```{r}
# Clean data
dataset1 <- Stock_data_1 %>%
  mutate(
    PERMNO = as.character(PERMNO),
    PRC = abs(PRC),
    BIDLO = ifelse(BIDLO == 0.0, NA, BIDLO),
    ASKHI = ifelse(ASKHI == 0.0, NA, ASKHI),
    PRC = ifelse(PRC == 0.0, NA, PRC),
    VOL = ifelse(VOL == -99, NA, VOL),
    RET = case_when(
      RET %in% c(-44, -55, -66, -77, -88, -99) ~ NA,  
      RET %in% c(".A", ".B", ".C", ".D") ~ NA,        
      TRUE ~ as.numeric(as.character(RET))
    ),
    ASK = ifelse(ASK - BID < 0, NA, as.numeric(as.character(ASK))),
    BID = ifelse(ASK - BID < 0, NA, as.numeric(as.character(BID))),
    NUMTRD = ifelse(NUMTRD == 99, NA, NUMTRD)
  ) %>%
  
  # Remove duplicate observations
  distinct(PERMNO, date, .keep_all = TRUE)

```

## 2. Remove low-observation securities
```{r}
# Count observation for each unique security
total_count <- dataset1 %>% 
  filter(date < "2020-03-20") %>% 
  group_by(PERMNO) %>%
  summarise(Count = n(), .groups = 'drop')

summary(total_count$Count)
```
Despite the popular frequency is 147 observations, but it may reduce the sample size significantly if using 147 as the threshold, hence, I decided to use the lower bounds in COVID period as the threshold
```{R}
# Define the threshold for low-observation (using the COVID period as the boundary)
obs_covid <- dataset1 %>% 
  filter(date >= "2020-02-14" & date <= "2020-03-20") %>% 
  group_by(PERMNO) %>%
  summarise(Count = n(), .groups = 'drop')
obs_covid_summary <- summary(obs_covid$Count)

summary(obs_covid$Count)

# Calculate lower bounds for observation counts
# Formula: 25th percentile - {1.5 * (75th percentile - 25th percentile)}
obs_lower_bound_covid <- obs_covid_summary["1st Qu."] - 1.5 * (obs_covid_summary["3rd Qu."] - obs_covid_summary["1st Qu."])

# Remove low-observation securities
dataset1_obs_removed <- dataset1 %>%
  filter((PERMNO %in% total_count$PERMNO[total_count$Count >= obs_lower_bound_covid])) %>%
  ungroup()
```

```{r echo = FALSE}
cat("Number of unique security (before remove low-obs): ", length(unique(dataset1$PERMNO)))
cat("Number of unique security (after remove low-obs): ", length(unique(dataset1_obs_removed$PERMNO)))
```

## 3. Assign TICKER to those have PERMNO in the entire period but missing TICKER in some dates
TICKER may be changed due to company name changes, mergers, or acquisition, but PERMNO is the most unique identifier of each security that won't change just due to company movement.
```{r}
# Check how many security is missing TICKER value while have PERMNO value
missing_ticker <- dataset1_obs_removed %>%
  group_by(PERMNO) %>%
  summarise(missing_ticker = any(TICKER == ""), 
            all_dates = n()) %>%
  filter(missing_ticker) %>%
  pull(PERMNO)

cat("Security that have PERMNO but missing TICKER (before cleaning): ", length(missing_ticker))

# Replace missing value TICKER if same PERMNO found
dataset1_cleaned <- dataset1_obs_removed %>%
  group_by(PERMNO) %>%
  mutate(TICKER = ifelse(TICKER == "", 
                          first(na.omit(TICKER[TICKER != ""])), 
                          TICKER)) %>%
  ungroup()

# Check after the replace
missing_ticker_cleaned <- dataset1_cleaned %>%
  group_by(PERMNO) %>%
  summarise(missing_ticker = any(is.na(TICKER)), 
            all_dates = n()) %>%
  filter(missing_ticker) %>%
  pull(PERMNO)

cat("Security that have PERMNO but missing TICKER (after replacing): ", length(missing_ticker_cleaned))
```

## 4. Add (Compute) variables for further analysis

### 4.1 Filter ETFs and Stocks
```{r}
dataset1_filtered <- dataset1_cleaned %>%
  filter(SHRCD == 73 | SHRCD == 11) %>% 
  filter(!is.na(RET) & !is.na(PRC))

cat("Number of unique security in the dataset: ",length(unique(dataset1_filtered$PERMNO)))
```

### 4.2 Compute dollar_vol, bid_ask_spread, turnover_ratio
```{r}
dataset1_df1 <- dataset1_filtered %>%
  arrange(PERMNO, date) %>%
  mutate(
    # Compute the dollar volume
    dollar_vol = PRC * VOL,
    
    # Compute the bid-ask spread (relative)
    bid_ask_spread = (ASK - BID) / ((ASK + BID) / 2),
    
    # Compute the turnover ratio
    turnover_ratio = VOL / (SHROUT * 1000)
  )

```

### 4.3 Add risk-free rate
```{r}
# Add Risk free rate (US Daily Treasury Bill Rates)
dataset1_df2 <- dataset1_df1 %>%
  arrange(date) %>%
  left_join(
    risk_free_rate %>% 
      filter(
        date >= "2019-08-20" & date <= "2020-08-20"
      )
    , by = "date"
  ) %>%
  mutate(
    risk_free_rate = na.locf(risk_free_rate, na.rm = FALSE)
  )
```

### 4.4 Compute Sharpe ratio and volatility
```{r}
# Define the total trading days
trading_days_total <- length(unique(dataset1$date))

dataset1_df3 <- dataset1_df2 %>%
  group_by(PERMNO) %>%
  arrange(date) %>%
  mutate(
    # Compute daily mean return and standard deviation for each PERMNO
    mean_ret = rollmean(RET, k = 30, fill = NA, align = "right", na.rm = TRUE),
    std_ret = rollapply(RET, width = 30, FUN = sd, fill = NA, align = "right", na.rm = TRUE),
    
    # Compute volatility
    volatility = std_ret * sqrt(trading_days_total),
    
    # Compute daily Sharpe Ratio
    sharpe_ratio = ifelse(is.na(std_ret) | std_ret == 0, NA, (mean_ret - risk_free_rate) / std_ret)
    ) %>%
  ungroup() %>% 
  select(-c("mean_ret", "std_ret")) 
```

### 4.5 Add sector categories
```{r}
# Add sector ID
dataset1_df4 <- dataset1_df3 %>%
  left_join(industry_code, by = "TICKER") %>%
  mutate(
    SECTOR = case_when(
      SECTOR == "Health Care" ~ "Health_Care",
      SECTOR == "Information Technology" ~ "Information_Technology",
      SECTOR == "Consumer Discretionary" ~ "Consumer_Discretionary",
      SECTOR == "Consumer Staples" ~ "Consumer_Staples",
      SECTOR == "Real Estate" ~ "Real_Estate",
      SECTOR == "Communication Services" ~ "Communication_Services",
      TRUE ~ SECTOR  
    ),
    # Assign all missing value as Unknown Sector to avoid dropping large amount of data
    SECTOR = ifelse(is.na(SECTOR), "Unknown", SECTOR)
  ) %>% 
  select(-c(SECTOR_ID, INDUSTRY))
```

### 4.6 Add market share
```{r}
# Compute the sum of user holding of all securities
user_holding_summary <- userholding %>%
  group_by(timestamp) %>%
  summarise(total_holding = sum(users_holding, na.rm = TRUE), .groups = 'drop')

# Add total user holdings to the original dataset
user_holding_1 <- userholding %>%
  left_join(user_holding_summary, by = "timestamp")

# Compute market share
user_holding_2 <- user_holding_1 %>%
  group_by(TICKER, timestamp) %>% 
  mutate(
    market_share = users_holding / total_holding * 100) 

```

```{r}
# Add the market_share to the dataset
dataset1_df5 <- dataset1_df4 %>%
  left_join(user_holding_2, by = c("TICKER" = "TICKER", "date" = "timestamp")) %>% 
  select(-c("users_holding", "total_holding"))
```

### 4.7 Add market capitalisation and catogrise company size
```{r}
dataset1_df6 <- dataset1_df5 %>%
  filter(!is.na(SHROUT)) %>% 
   # Compute market capitalisation
  mutate(
    market_cap = SHROUT * PRC * 1000,
    #  Categorise company size for the security based on market capitalisation
    company_size = case_when(
      market_cap <= 2e9 ~ "Small",
      market_cap > 2e9 & market_cap <= 1e10 ~ "Medium",
      market_cap > 1e10 ~ "Large"
    )
  )

# Inspect the unique security count of each company size
print(dataset1_df6 %>%
  group_by(company_size) %>%
  summarise(distinct_tickers = n_distinct(PERMNO)))
```

### 4.8 Add absolute correlation between RET and sprtrn
```{r}
dataset1_df7 <- dataset1_df6 %>%
  group_by(PERMNO) %>%
  mutate(
    abs_corr_sp = abs(cor(RET, sprtrn, use = "complete.obs")))
```

### 4.9 Add dummy variables to identify ETFs or stocks
```{r}
dataset1_df8 <- dataset1_df7 %>%
  mutate(security_type = case_when(
      SHRCD == 73 ~ "ETF",
      SHRCD == 11 ~ "Stock")
      )
```

### 4.10 Tracking error (ETF)
To see how closely an investment portfolio's returns align with the return of a benchmark index. The standard deviation of the difference between returns and benchmark returns.
```{r}
# Tracking error
dataset1_df9 <- dataset1_df8 %>% 
  group_by(PERMNO) %>% 
  mutate(
    tracking_error = ifelse(is.na(sprtrn), NA, sd(RET - sprtrn))
  )
```

### Inspect the data
To make sure no PERMNO accidentally dropped during data wrangling
```{r echo = FALSE}
# Number of unique security
cat("Number of unique security before data wrangling: ", length(unique(dataset1_df1$PERMNO)))
cat("Number of unique security after data wrangling: ", length(unique(dataset1_df9$PERMNO)))
cat("No unique security dropped during data wrangling: ", length(unique(dataset1_df1$PERMNO)) == length(unique(dataset1_df9$PERMNO)))

# Inspect final dataset
cat("Preview of cleaned dataset:\n")
head(dataset1_df9 %>% arrange(date), n = 5)

# Summary of dataset
cat("Summary of dataset:\n")
summary(dataset1_df9)
```

## 5. Create function to save the plots
```{r}
# To reduce the workload of manually saving every plot, create a function to save plot
save_plot <- function(plot, title) {
  # Define the file path and name
  file_name <- paste0(title, ".png")
  file_path <- file.path(getwd(), file_name)
  
  # Define the format
  ggsave(filename = file_path, 
         plot = plot, 
         width = 12, 
         height = 6, 
         units = "in", 
         dpi = 600)
  }
```


#   1 Descriptive statistics & visualisation
```{r}
# load library
library(e1071)
library(purrr)
library(ggplot2)
library(RColorBrewer)
library(plotly)
library(scales)
```

## Descriptive Statistics

### Overall
#### Transform the dataset into a time series dataset:
```{r}
# Group by date and summarise for each variable by aggregation
task1_overall <- dataset1_df9 %>% 
  group_by(date)  %>%
  summarise(
    RET = mean(RET, na.rm = TRUE),
    volatility = mean(volatility, na.rm = TRUE),
    sharpe_ratio = mean(sharpe_ratio, na.rm = TRUE),
    dollar_vol = sum(dollar_vol, na.rm = TRUE),
    number_of_trade = sum(NUMTRD, na.rm = TRUE),
    bid_ask_spread = mean(bid_ask_spread, na.rm = TRUE),
    turnover_ratio = mean(turnover_ratio, na.rm = TRUE),
    .groups = "drop"
  ) %>% 
  # Standardise variable to ensure they are in same scale
  mutate(
    scaled_RET = scale(RET),
    scaled_sharpe_ratio = scale(sharpe_ratio),
    scaled_volatility = scale(volatility),
    scaled_spread = scale(bid_ask_spread),
    scaled_dollar_vol = scale(dollar_vol),
    scaled_numtrd = scale(number_of_trade),
    scaled_turnover_ratio = scale(turnover_ratio)
  )

head(task1_overall, n = 5)
```

#### Create the descriptive statistics summary for all variables for both period:
```{r}
# Non-COVID period
task1_overall_noncovid <- task1_overall %>% 
  filter(date >= "2019-10-14" & date <= "2019-11-20") %>%
  # Exclude the first 29 days' observation to avoid the 30-day rolling window affect the summary statistics result
  filter(date >= "2019-10-01") %>%  
  select(-date, -starts_with("scaled_")) %>% 
  summarise(across(everything(), list(
    min = ~ min(.x, na.rm = TRUE),
    q25 = ~ quantile(.x, 0.25, na.rm = TRUE),
    mean = ~ mean(.x, na.rm = TRUE),
    median = ~ median(.x, na.rm = TRUE),
    q75 = ~ quantile(.x, 0.75, na.rm = TRUE),
    max = ~ max(.x, na.rm = TRUE),
    sd = ~ sd(.x, na.rm = TRUE)
  ),
  .names = "{.col}_{.fn}")) 

# COVID period
task1_overall_covid <- task1_overall %>% 
  filter(date >= "2020-02-14" & date <= "2020-03-20") %>% 
  # Exclude the first 29 days' observation to avoid the 30-day rolling window affect the summary statistics result
  filter(date >= "2019-10-01") %>%  
  select(-date, -starts_with("scaled_")) %>% 
  summarise(across(everything(), list(
    min = ~ min(.x, na.rm = TRUE),
    q25 = ~ quantile(.x, 0.25, na.rm = TRUE),
    mean = ~ mean(.x, na.rm = TRUE),
    median = ~ median(.x, na.rm = TRUE),
    q75 = ~ quantile(.x, 0.75, na.rm = TRUE),
    max = ~ max(.x, na.rm = TRUE),
    sd = ~ sd(.x, na.rm = TRUE)
  ),
  .names = "{.col}_{.fn}"))

print(task1_overall_noncovid)
print(task1_overall_covid)
```

### By Industry Sector
#### Transform the dataset into a panel (Date & Industry)
```{r}
# Group by date and SECTOR, summarise for each variable by aggregation
task1_sector <- dataset1_df9 %>% 
  group_by(SECTOR, date) %>%
  summarise(
    RET = mean(RET, na.rm = TRUE),
    volatility = mean(volatility, na.rm = TRUE),
    sharpe_ratio = mean(sharpe_ratio, na.rm = TRUE),
    dollar_vol = sum(dollar_vol, na.rm = TRUE),
    number_of_trade = sum(NUMTRD, na.rm = TRUE),
    bid_ask_spread = mean(bid_ask_spread, na.rm = TRUE),
    turnover_ratio = mean(turnover_ratio, na.rm = TRUE),
    .groups = "drop"
  ) %>% 
  # Standardise variable to ensure they are in same scale
  mutate(
    scaled_RET = scale(RET),
    scaled_sharpe_ratio = scale(sharpe_ratio),
    scaled_volatility = scale(volatility),
    scaled_spread = scale(bid_ask_spread),
    scaled_dollar_vol = scale(dollar_vol),
    scaled_numtrd = scale(number_of_trade),
    scaled_turnover_ratio = scale(turnover_ratio)
  ) 

head(task1_sector, n = 5)
```

#### Create the descriptive statistics summary for all variables for both period
```{r}
# Non-COVID period
task1_sector_noncovid <- task1_sector %>% 
  filter(date >= "2019-10-14" & date <= "2019-11-20") %>% 
  # Exclude the first 29 days' observation to avoid the 30-day rolling window affect the summary statistics result
  filter(date >= "2019-10-01") %>%  
  select(-date, -starts_with("scaled_")) %>% 
  group_by(SECTOR) %>%
  summarise(across(everything(), list(
    min = ~ min(.x, na.rm = TRUE),
    q25 = ~ quantile(.x, 0.25, na.rm = TRUE),
    mean = ~ mean(.x, na.rm = TRUE),
    median = ~ median(.x, na.rm = TRUE),
    q75 = ~ quantile(.x, 0.75, na.rm = TRUE),
    max = ~ max(.x, na.rm = TRUE),
    sd = ~ sd(.x, na.rm = TRUE)
  ),
  .names = "{.col}_{.fn}")) %>% 
  arrange(SECTOR)

# COVID period
task1_sector_covid <- task1_sector %>% 
  filter(date >= "2020-02-14" & date <= "2020-03-20") %>% 
  # Exclude the first 29 days' observation to avoid the 30-day rolling window affect the summary statistics result
  filter(date >= "2019-10-01") %>%  
  select(-date, -starts_with("scaled_")) %>% 
  group_by(SECTOR) %>%
  summarise(across(everything(), list(
    min = ~ min(.x, na.rm = TRUE),
    q25 = ~ quantile(.x, 0.25, na.rm = TRUE),
    mean = ~ mean(.x, na.rm = TRUE),
    median = ~ median(.x, na.rm = TRUE),
    q75 = ~ quantile(.x, 0.75, na.rm = TRUE),
    max = ~ max(.x, na.rm = TRUE),
    sd = ~ sd(.x, na.rm = TRUE)
  ),
  .names = "{.col}_{.fn}")) %>% 
  arrange(SECTOR)

print(task1_sector_noncovid)
print(task1_sector_covid)
```

### By Company size
#### Transform the dataset into a panel (Date & Company Size)
```{r}
# Group by date and company_size, summarise for each variable by aggregation
task1_size <- dataset1_df9 %>% 
  group_by(company_size, date) %>%
  summarise(
    RET = mean(RET, na.rm = TRUE),
    volatility = mean(volatility, na.rm = TRUE),
    sharpe_ratio = mean(sharpe_ratio, na.rm = TRUE),
    dollar_vol = sum(dollar_vol, na.rm = TRUE),
    number_of_trade = sum(NUMTRD, na.rm = TRUE),
    bid_ask_spread = mean(bid_ask_spread, na.rm = TRUE),
    turnover_ratio = mean(turnover_ratio, na.rm = TRUE),
    .groups = "drop"
  ) %>% 
  # Standardise variable to ensure they are in same scale
  mutate(
    scaled_RET = scale(RET),
    scaled_sharpe_ratio = scale(sharpe_ratio),
    scaled_volatility = scale(volatility),
    scaled_spread = scale(bid_ask_spread),
    scaled_dollar_vol = scale(dollar_vol),
    scaled_numtrd = scale(number_of_trade),
    scaled_turnover_ratio = scale(turnover_ratio)
  ) %>% 
  arrange(desc(company_size), date)

head(task1_size, n = 5)
```

#### Create the descriptive statistics summary for all variables for both period
```{r}
# Non-COVID period
task1_size_noncovid <- task1_size %>% 
  filter(date >= "2019-10-14" & date <= "2019-11-20") %>% 
  # Exclude the first 29 days' observation to avoid the 30-day rolling window affect the summary statistics result
  select(-date, -starts_with("scaled_")) %>% 
  group_by(company_size) %>%
  summarise(across(everything(), list(
    min = ~ min(.x, na.rm = TRUE),
    q25 = ~ quantile(.x, 0.25, na.rm = TRUE),
    mean = ~ mean(.x, na.rm = TRUE),
    median = ~ median(.x, na.rm = TRUE),
    q75 = ~ quantile(.x, 0.75, na.rm = TRUE),
    max = ~ max(.x, na.rm = TRUE),
    sd = ~ sd(.x, na.rm = TRUE)
  ),
  .names = "{.col}_{.fn}")) %>% 
  arrange(desc(company_size))

# COVID period
task1_size_covid <- task1_size %>% 
  # Exclude the first 29 days' observation to avoid the 30-day rolling window affect the summary statistics result
  filter(date >= "2020-02-14" & date <= "2020-03-20") %>% 
  select(-date, -starts_with("scaled_")) %>% 
  group_by(company_size) %>%
  summarise(across(everything(), list(
    min = ~ min(.x, na.rm = TRUE),
    q25 = ~ quantile(.x, 0.25, na.rm = TRUE),
    mean = ~ mean(.x, na.rm = TRUE),
    median = ~ median(.x, na.rm = TRUE),
    q75 = ~ quantile(.x, 0.75, na.rm = TRUE),
    max = ~ max(.x, na.rm = TRUE),
    sd = ~ sd(.x, na.rm = TRUE)
  ),
  .names = "{.col}_{.fn}")) %>% 
  arrange(desc(company_size))

print(task1_size_noncovid)
print(task1_size_covid)
```

### By Security Types
#### Transform the dataset into a panel (Date & Security Type)
```{r}
# Group by date and security_type, summarise for each variable by aggregation
task1_type <- dataset1_df9 %>% 
  group_by(security_type, date) %>%
  summarise(
    RET = mean(RET, na.rm = TRUE),
    volatility = mean(volatility, na.rm = TRUE),
    sharpe_ratio = mean(sharpe_ratio, na.rm = TRUE),
    dollar_vol = sum(dollar_vol, na.rm = TRUE),
    number_of_trade = sum(NUMTRD, na.rm = TRUE),
    bid_ask_spread = mean(bid_ask_spread, na.rm = TRUE),
    turnover_ratio = mean(turnover_ratio, na.rm = TRUE),
    .groups = "drop"
  ) %>% 
  # Standardise variable to ensure they are in same scale
  mutate(
    scaled_RET = scale(RET),
    scaled_sharpe_ratio = scale(sharpe_ratio),
    scaled_volatility = scale(volatility),
    scaled_spread = scale(bid_ask_spread),
    scaled_dollar_vol = scale(dollar_vol),
    scaled_numtrd = scale(number_of_trade),
    scaled_turnover_ratio = scale(turnover_ratio)
  ) %>% arrange(desc(security_type), date)

head(task1_type, n = 5)
```

#### Create the descriptive statistics summary for all variables for both period
```{r}
# Non-COVID period
task1_type_noncovid <- task1_type %>% 
  filter(date >= "2019-10-14" & date <= "2019-11-20") %>% 
  # Exclude the first 29 days' observation to avoid the 30-day rolling window affect the summary statistics result
  select(-date, -starts_with("scaled_")) %>% 
  group_by(security_type) %>%
  summarise(across(everything(), list(
    min = ~ min(.x, na.rm = TRUE),
    q25 = ~ quantile(.x, 0.25, na.rm = TRUE),
    mean = ~ mean(.x, na.rm = TRUE),
    median = ~ median(.x, na.rm = TRUE),
    q75 = ~ quantile(.x, 0.75, na.rm = TRUE),
    max = ~ max(.x, na.rm = TRUE),
    sd = ~ sd(.x, na.rm = TRUE)
  ),
  .names = "{.col}_{.fn}")) 

# COVID period
task1_type_covid <- task1_type %>% 
  filter(date >= "2020-02-14" & date <= "2020-03-20") %>% 
  # Exclude the first 29 days' observation to avoid the 30-day rolling window affect the summary statistics result
  select(-date, -starts_with("scaled_")) %>% 
  group_by(security_type) %>%
  summarise(across(everything(), list(
    min = ~ min(.x, na.rm = TRUE),
    q25 = ~ quantile(.x, 0.25, na.rm = TRUE),
    mean = ~ mean(.x, na.rm = TRUE),
    median = ~ median(.x, na.rm = TRUE),
    q75 = ~ quantile(.x, 0.75, na.rm = TRUE),
    max = ~ max(.x, na.rm = TRUE),
    sd = ~ sd(.x, na.rm = TRUE)
  ),
  .names = "{.col}_{.fn}")) 

print(task1_type_noncovid)
print(task1_type_covid)
```

### Save the descriptive summary in csv files (For report)
```{r}
# Overall
write.csv(task1_overall_noncovid, file = "task1_overall_noncovid.csv", row.names = FALSE)
write.csv(task1_overall_covid, file = "task1_overall_covid.csv", row.names = FALSE)

# By Sector
write.csv(task1_sector_noncovid, file = "task1_sector_noncovid.csv", row.names = FALSE)
write.csv(task1_sector_covid, file = "task1_sector_covid.csv", row.names = FALSE)

# By Company size
write.csv(task1_size_noncovid, file = "task1_size_noncovid.csv", row.names = FALSE)
write.csv(task1_size_covid, file = "task1_size_covid.csv", row.names = FALSE)

# By security type
write.csv(task1_type_noncovid, file = "task1_type_noncovid.csv", row.names = FALSE)
write.csv(task1_type_covid, file = "task1_type_covid.csv", row.names = FALSE)
```

## Time series plot
### Function to create time series plot (Overall)
```{r}
# Dataset for this plot is time series
time_series_plot_overall <- function(data, variables, var_names, periods) {
  
  # Convert the dataset to long format
  data_long <- data %>%
    pivot_longer(cols = all_of(variables), 
                 names_to = "variable", 
                 values_to = "value") %>%
    mutate(variable = factor(variable, 
                             levels = variables, 
                             labels = var_names)
           )
  
  # Create shaded areas for showing comparison periods
  shaded_areas <- bind_rows(
    lapply(periods, function(p) {
      data.frame(xmin = as.Date(p$start), 
                 xmax = as.Date(p$end), 
                 ymin = -Inf, 
                 ymax = Inf, 
                 fill = p$color, 
                 label = p$label)
    })
  )
  
  # Create plots for each variable
  create_plot <- function(var_name, dataset_name) {
    plot_data <- data_long %>% 
      filter(variable == var_name)
    
    p <- ggplot(plot_data, aes(x = date, 
                               y = value)) +
      
      geom_rect(data = shaded_areas, 
                aes(xmin = xmin, 
                    xmax = xmax, 
                    ymin = ymin, 
                    ymax = ymax, 
                    fill = label),
                alpha = 0.2, 
                inherit.aes = FALSE) +
      
      geom_line(aes(color = "Data"), 
                size = 1) +
      
      scale_fill_manual(values = setNames(unique(shaded_areas$fill),
                                          unique(shaded_areas$label)),
                        name = "Period") +
      
      guides(fill = guide_legend(title = "Periods for Comparison"),
             color = guide_legend(title = "Metrics")) +
      
      labs(title = paste("Overall Performance of", var_name, "during 20 August 2019 to 20 August 2020"),
           x = "Date", 
           y = paste("Standardised", var_name)) +
      
      scale_x_date(date_labels = "%Y-%m-%d", date_breaks = "1 month") +
      
      theme_minimal() +
      
      theme(axis.text.x = element_text(angle = 55, hjust = 1)) +
      
      scale_y_continuous(expand = expansion(mult = c(0.05, 0.1))) +
      
      scale_color_manual(name = "Metrics",
                         values = c("Data" = "red"),
                         labels = c("Data" = var_name))

    # Save plot for report
    save_plot(p, paste0(dataset_name, "_", var_name))
    
    return(p)
  }
  
  # To keep the saved plot file name easy to read
  dataset_name <- deparse(substitute(data))
  
  # Create a list of plots
  plots <- lapply(var_names, function(var_name) {
    create_plot(var_name, dataset_name)
  })
  
  # Print plots
  for (plot in plots) {
    print(plot)
  }
}
```

### Function to create time series plot (By group)
```{r}
time_series_plot_group <- function(data, variables, var_names, label_name, periods, group, color_map) {
  
  # Convert the data to long format
  data_long <- data %>%
    pivot_longer(cols = all_of(variables), 
                 names_to = "variable", 
                 values_to = "value") %>%
    mutate(variable = factor(variable, 
                             levels = variables, 
                             labels = var_names)) %>%
    filter(!is.na(!!sym(group)))
  
  # Define color palette for the group variable
  group_colors <- brewer.pal(n = length(unique(data_long[[group]])), 
                             name = color_map)
  names(group_colors) <- unique(data_long[[group]])
  
  # Create shaded areas for showing comparison periods
  shaded_areas <- do.call(rbind, 
                          lapply(periods, function(p) {
                            data.frame(xmin = as.Date(p$start),
                                       xmax = as.Date(p$end), 
                                       ymin = -Inf, 
                                       ymax = Inf, 
                                       fill = p$color, 
                                       label = p$label)
                          })
  )
  
  # Create plots for each variable
  create_plot <- function(var_name, dataset_name) {
    plot_data <- data_long %>% 
      filter(variable == var_name)
    
    # Initialise the plot
    p <- ggplot() +
      
      # Loop over each group and plot its points and lines in order
      lapply(unique(plot_data[[group]]), function(g) {
        list(
          geom_point(data = plot_data %>% filter(!!sym(group) == g),
                     aes(x = date, y = value, color = !!sym(group)), size = 2, alpha = 0.9),
          geom_line(data = plot_data %>% filter(!!sym(group) == g),
                    aes(x = date, y = value, color = !!sym(group)), size = 1)
        )
      }) +
      
      # Create shaded comparison periods
      geom_rect(data = shaded_areas, 
                aes(xmin = xmin, 
                    xmax = xmax, 
                    ymin = ymin, 
                    ymax = ymax, 
                    fill = label),
                alpha = 0.2, 
                inherit.aes = FALSE) +
      
      # Customise colors and legends
      scale_color_manual(values = group_colors, 
                         name = label_name) +
      scale_fill_manual(values = setNames(unique(shaded_areas$fill),
                                          unique(shaded_areas$label)),
                        name = "Period") +
      
      # Customise labels and titles
      labs(title = paste("Overall Performance of", var_name, "during 20 August 2019 to 20 August 2020"),
           subtitle = paste("( Group by", label_name, ")"),
           x = "Date", 
           y = paste("Standardised", var_name)) +
      
      # Customise the x-axis and y-axis
      scale_x_date(date_labels = "%Y-%m-%d", 
                   date_breaks = "1 month") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 55, 
                                       hjust = 1)) +
      scale_y_continuous(expand = expansion(mult = c(0.05, 0.1)))
    
    # Save the plot
    save_plot(p, paste0(dataset_name, "_", var_name))
    
    return(p)
  }
  
  dataset_name <- deparse(substitute(data))
  
  plots <- lapply(var_names, function(var_name) {
    create_plot(var_name, dataset_name)
  })
  
  # Print plots
  for (plot in plots) {
    print(plot)
  }
}

```

### Call function to create plot(s)
```{r}
# Define the variables and their corresponding names
variables_scaled <- c('scaled_RET', 'scaled_volatility', 'scaled_sharpe_ratio', 'scaled_spread', 'scaled_dollar_vol', 'scaled_numtrd', 'scaled_turnover_ratio')
var_names <- c('Returns', 'Volatility', 'Sharpe Ratio', 'Bid-Ask Spread', 'Dollar Volume', 'Number of Trade', 'Turnover Ratio')

# Define the dates and colors for shaded areas
periods_group <- list(
  list(start = "2019-10-14", end = "2019-11-20", color = "#937613", label = "Non-COVID Period\n(2019-10-14 to 2019-11-20)"),
  list(start = "2020-02-14", end = "2020-03-20", color = "black", label = "COVID Period\n(2020-02-14 to 2020-03-20)")
  )

# Overall Performance
time_series_plot_overall(task1_overall, variables_scaled, var_names, periods_group)

# By security type
time_series_plot_group(task1_type, variables_scaled, var_names, "Security Types", periods_group, 'security_type', color_map = "Dark2")

# By industry sector
time_series_plot_group(task1_sector, variables_scaled, var_names,"Industry Sectors", periods_group, 'SECTOR', color_map = "Paired")

# By Company size
time_series_plot_group(task1_size, variables_scaled, var_names, "Company sizes", periods_group, 'company_size', color_map = "Set1")
```

### Function to create time series plot (Market Performance and Risk Metrics)
```{r}
market_risk <- function(data, variables, var_names, periods) {
  
  # Convert data to long format
  data_long <- data %>%
    pivot_longer(cols = all_of(variables), 
                 names_to = "variable", 
                 values_to = "value") %>%
    mutate(variable = factor(variable, 
                             levels = variables, 
                             labels = var_names)
           )
  
  # Create shaded areas data frame
  shaded_areas <- do.call(rbind, 
                          lapply(periods, function(p) {
                            data.frame(xmin = as.Date(p$start), 
                                       xmax = as.Date(p$end), 
                                       ymin = -Inf, 
                                       ymax = Inf, 
                                       fill = p$color, 
                                       label = p$label)
                            })
                          )
  
  # Define color mapping for each variable
  color_mapping <- c(
    "Returns" = "#f39906",
    "Volatility" = "#6402bb",
    "Sharpe Ratio" = "#32cd4c"
    )
  
# Define line type mapping
  linetype_mapping <- c(
    "Returns" = "solid",
    "Volatility" = "solid",
    "Sharpe Ratio" = "solid"
    )
  
  # Create the plot
  ggplot() +
    
    # Plot primary variables with unique colors and line types
    geom_line(data = data_long, 
              aes(x = date, 
                  y = value, 
                  color = variable, 
                  linetype = variable)
              ) +
    
    # Add shaded areas
    geom_rect(data = shaded_areas, 
              aes(xmin = xmin, 
                  xmax = xmax, 
                  ymin = ymin, 
                  ymax = ymax, 
                  fill = label),
              alpha = 0.2, 
              inherit.aes = FALSE) +
    
    # Define color and line type scales
    scale_color_manual(name = "Metrics",
                       values = color_mapping) +
    
    scale_linetype_manual(name = "Metrics", 
                          values = linetype_mapping) +
    
    # Define fill scales for shaded areas
    scale_fill_manual(values = setNames(unique(shaded_areas$fill),
                                        unique(shaded_areas$label)),
                      name = "Period") +
    
    # Define axis labels and limits
    labs(title = "Market Performance and Risk Metrics: August 20, 2019 to August 20, 2020", 
         x = "Date", 
         y = "Standardised Values of Performance Metrics") +
    
    scale_x_date(date_labels = "%Y-%m-%d", date_breaks = "1 month") +
    
    scale_y_continuous(limits = c(-5.5, 5.5), breaks = seq(-5.0, 5.0, by = 2)) +
    
    theme_minimal() +
    
    theme(axis.text.x = element_text(angle = 55, hjust = 1))
}

# Define variables
variables_risk <- c('scaled_RET', 'scaled_volatility', 'scaled_sharpe_ratio')
var_names_risk <- c('Returns', 'Volatility', 'Sharpe Ratio')

# Call function to create plot
performance_risk <- market_risk(task1_overall, variables_risk, var_names_risk, periods_group)

print(performance_risk)

# Save the graph
save_plot(performance_risk, "Time Series Plot for Market Performance and Risk Metrics")
```

### Function to create time series plot (Market liquidity and trading behavior Metrics)
```{r}
market_liquidity <- function(data, variables, var_names, periods) {
  
  # Convert data to long format
  data_long <- data %>%
    pivot_longer(cols = all_of(variables), 
                 names_to = "variable", 
                 values_to = "value") %>%
    mutate(variable = factor(variable, 
                             levels = variables, 
                             labels = var_names)
           )

  
  # Create shaded areas data frame
  shaded_areas <- do.call(rbind, 
                          lapply(periods, function(p) {
                            data.frame(xmin = as.Date(p$start), 
                                       xmax = as.Date(p$end), 
                                       ymin = -Inf, 
                                       ymax = Inf, 
                                       fill = p$color, 
                                       label = p$label)
                            })
                          )
  
  # Define color mapping for each variable
  color_mapping <- c(
    "Bid-Ask Spread" = "#f39906",
    "Dollar Volume" = "#6402bb",
    "Number of Trade" = "#e85697",
    "Turnover Ratio" = "#32cd4c",
    "Volatility" = "black"
    )
  
# Define line type mapping (solid for all except Max Drawdown)
  linetype_mapping <- c(
    "Bid-Ask Spread" = "solid",
    "Dollar Volume" = "solid",
    "Turnover Ratio" = "solid",
    "Number of Trade" = "solid",
    "Volatility" = "dashed"
    )
  
  # Create the plot
  ggplot() +
    
    # Plot primary variables with unique colors and line types
    geom_line(data = data_long, 
              aes(x = date, 
                  y = value, 
                  color = variable, 
                  linetype = variable)
              ) +
    
    # Add shaded areas
    geom_rect(data = shaded_areas, 
              aes(xmin = xmin, 
                  xmax = xmax, 
                  ymin = ymin, 
                  ymax = ymax, 
                  fill = label),
              alpha = 0.2, 
              inherit.aes = FALSE) +
    
    # Define color and line type scales
    scale_color_manual(name = "Metrics",
                       values = color_mapping) +
    
    scale_linetype_manual(name = "Metrics", 
                          values = linetype_mapping) +
    
    # Define fill scales for shaded areas
    scale_fill_manual(values = setNames(unique(shaded_areas$fill),
                                        unique(shaded_areas$label)),
                      name = "Period") +
    
    # Define axis labels and limits
    labs(title = "Market Liquidity and Trading Behavior Metrics: August 20, 2019 to August 20, 2020",
         x = "Date", 
         y = "Standardised Values of Performance Metrics") +
    
    scale_x_date(date_labels = "%Y-%m-%d", date_breaks = "1 month") +
    
    scale_y_continuous(limits = c(-3, 6), breaks = seq(-3, 6, by = 2)) +
    
    theme_minimal() +
    
    theme(axis.text.x = element_text(angle = 55, hjust = 1))
}

# Define variables
variables_liquidity <- c('scaled_volatility', 'scaled_spread', 'scaled_dollar_vol', 'scaled_numtrd', 'scaled_turnover_ratio')

var_names_liquidity <- c('Volatility', 'Bid-Ask Spread', 'Dollar Volume', 'Number of Trade', 'Turnover Ratio')

# Call function to create plot
liquidity_trading <- market_liquidity(task1_overall, variables_liquidity, var_names_liquidity, periods_group)
print(liquidity_trading)
# Save the graph
save_plot(liquidity_trading, "Time Series Plot for Market Liquidity and Trading Behavior Metrics")
```


#   2 OLS Regressions
## Data Preparation
```{r}
# load library
library(broom)
library(caret)
library(reshape2)
library(car)
```

### Calculate Fama-French Factor for Non-COVID period
```{r}
task2 <- dataset1_df9

# Prepare dataset
noncovid_filtered <- task2 %>% 
  filter(date < "2019-12-14") %>% 
  select(PERMNO, date, RET) %>% 
  left_join(fama_french_factors, by = "date")

# Initialise an empty list to store beta values
beta_list <- list()

# Loop through each unique PERMNO
for (permno in unique(noncovid_filtered$PERMNO)) {
  # Filter the data for the current PERMNO
  FFF_beta_noncovid <- subset(noncovid_filtered, PERMNO == permno)
  
  # Run the regression to calculate betas
  model_beta <- lm(RET ~ Mkt.RF + SMB + HML, data = FFF_beta_noncovid)
  
  # Store the betas with the PERMNO
  beta_list[[permno]] <- coef(model_beta)
}

# Convert the list of betas to a data frame
# Convert list to data frame with PERMNO names as row names
FFF_beta_noncovid <- do.call(rbind, lapply(names(beta_list), function(x) {
  as.data.frame(t(beta_list[[x]]), row.names = x)
}))

# Reset row names to a column
FFF_beta_noncovid <- tibble::rownames_to_column(FFF_beta_noncovid, var = "PERMNO")

# Rename columns to reflect beta coefficients
FFF_beta_noncovid <- FFF_beta_noncovid %>%
  rename(
    Mkt_beta = Mkt.RF,
    SMB_beta = SMB,
    HML_beta = HML
  ) %>% 
  select(-"(Intercept)")

FFF_beta_noncovid$PERMNO <- as.character(FFF_beta_noncovid$PERMNO)

print(head(FFF_beta_noncovid, n = 5))
```

### Calculate Fama-French Factor for COVID period
```{r}
# Prepare dataset
covid_filtered <- task2 %>% 
  filter(date < "2020-02-14") %>% 
  select(PERMNO, date, RET) %>% 
  left_join(fama_french_factors, by = "date")

# Initialise an empty list to store beta values
beta_list <- list()

# Loop through each unique PERMNO
for (permno in unique(covid_filtered$PERMNO)) {
  # Filter the data for the current PERMNO
  FFF_beta_covid <- subset(covid_filtered, PERMNO == permno)
  
  # Run the regression to calculate betas
  model_beta <- lm(RET ~ Mkt.RF + SMB + HML, data = FFF_beta_covid)
  
  # Store the betas with the PERMNO
  beta_list[[permno]] <- coef(model_beta)
}

# Convert the list of betas to a data frame
# Convert list to data frame with PERMNO names as row names
FFF_beta_covid <- do.call(rbind, lapply(names(beta_list), function(x) {
  as.data.frame(t(beta_list[[x]]), row.names = x)
}))

# Reset row names to a column
FFF_beta_covid <- tibble::rownames_to_column(FFF_beta_covid, var = "PERMNO")

# Rename columns to reflect beta coefficients
FFF_beta_covid <- FFF_beta_covid %>%
  rename(
    Mkt_beta = Mkt.RF,
    SMB_beta = SMB,
    HML_beta = HML
  ) %>% 
  select(-"(Intercept)")

FFF_beta_covid$PERMNO <- as.character(FFF_beta_covid$PERMNO)

print(head(FFF_beta_covid, n = 5))
```

### Transform cross-sectional dataset function
```{r}
transform_dataset_for_task2 <- function(data, start_date_RET, end_date_RET, start_date_others, end_date_others, type_of_security, FFF_dataset) {
  epsilon <- 1e-6
  
  # Find the date with the most PERMNOs having the highest price
  highest_change <- data %>%
    filter(date >= as.Date(start_date_RET) & date <= as.Date(end_date_RET)) %>%
    group_by(PERMNO) %>%
    filter(PRC == max(PRC)) %>%
    ungroup() %>%
    group_by(date) %>%
    summarise(num_highest = n()) %>%
    arrange(desc(num_highest)) 
  
  highest_date <- highest_change$date[1]
  cat("Highest Date: ")
  print(highest_change$date[1])

  # Find the date with the most PERMNOs having the lowest price
  lowest_change <- data %>%
    filter(date >= as.Date(start_date_RET) & date <= as.Date(end_date_RET)) %>%
    group_by(PERMNO) %>%
    filter(PRC == min(PRC)) %>%
    ungroup() %>%
    group_by(date) %>%
    summarise(num_lowest = n()) %>%
    arrange(desc(num_lowest))
  
  lowest_date <- lowest_change$date[1]
  cat("\nLowest Date: ")
  print(lowest_change$date[1])
  
  # Calculate the return using the lowest_date's and highest_date's price
  data_RET <- data %>%
    filter(date %in% c(highest_date, lowest_date)) %>%
    select(PERMNO, date, PRC) %>%
    spread(key = date, value = PRC, fill = NA) %>%
    rename(price_highest = !!as.character(highest_date), 
           price_lowest = !!as.character(lowest_date)) %>%
    mutate(returns = (price_highest - price_lowest) / price_lowest) %>%
    select(PERMNO, returns)
  
  cat("\nNumber of PERMNO in the dataset that calculated returns: ", length(unique(data_RET$PERMNO)), "\n")
  
  # Filter the data for two periods 
  data_filtered <- data %>%
    filter(date >= as.Date(start_date_others) & date <= as.Date(end_date_others)) %>%
    arrange(PERMNO, date)

  # Filter securities type and calculate aggregated variables
  data_others <- data_filtered %>%
    filter(
      type_of_security == "both" | 
      (type_of_security == "ETFs" & SHRCD == 73) |
      (type_of_security == "stocks" & SHRCD == 11)
    ) %>%
    group_by(PERMNO) %>%
    summarise(
      SHRCD = first(SHRCD),
      TICKER = first(TICKER),
      SECTOR = first(SECTOR),
      company_size = first(company_size),
      VOL = sum(VOL, na.rm = TRUE),
      dollar_vol = sum(dollar_vol, na.rm = TRUE),
      bid_ask_spread = mean(bid_ask_spread, na.rm = TRUE),
      turnover_ratio = mean(turnover_ratio, na.rm = TRUE),
      volatility = mean(volatility, na.rm = TRUE),
      sharpe_ratio = mean(sharpe_ratio, na.rm = TRUE),
      market_share = sum(market_share, na.rm = TRUE),
      market_cap = sum(market_cap, na.rm = TRUE),
      abs_corr_sp = mean(abs_corr_sp, na.rm = TRUE),
      security_type = first(security_type),
      tracking_error = mean(tracking_error, na.rm = TRUE),
      log_vol = log(sum(VOL + epsilon, na.rm = TRUE)),
      log_dollar_vol= log(sum(dollar_vol + epsilon, na.rm = TRUE)),
      log_spread = log(mean(bid_ask_spread + epsilon, na.rm = TRUE)),
      log_turnover_ratio = log(mean(turnover_ratio + epsilon, na.rm = TRUE)),
      log_volatility = log(mean(volatility + epsilon, na.rm = TRUE)),
      log_market_share = log(sum(market_share + epsilon, na.rm = TRUE)),
      log_market_cap = log(sum(market_cap + epsilon, na.rm = TRUE)),
      log_tracking_error = log(mean(tracking_error + epsilon, na.rm = TRUE)),
      vol_cap_ratio = VOL / market_cap,
      log_vol_cap_ratio = log(vol_cap_ratio),
      .groups = "drop"
    ) %>%
    mutate(
      scaled_vol = scale(VOL),
      scaled_dollar_vol = scale(dollar_vol),
      scaled_spread = scale(bid_ask_spread),
      scaled_turnover_ratio = scale(turnover_ratio),
      scaled_volatility = scale(volatility),
      scaled_sharpe_ratio = scale(sharpe_ratio),
      scaled_market_share = scale(market_share),
      scaled_market_cap = scale(market_cap),
      scaled_abs_corr_sp = scale(abs_corr_sp),
      scaled_tracking_error = scale(tracking_error)
    ) 
  
  # Check for duplicates
  duplicates <- data_others %>%
    group_by(PERMNO) %>%
    filter(n() > 1) %>%
    summarise(count = n())
  
  # Debugging
  if(nrow(duplicates) > 0) {
    warning("There are duplicates: ", paste(duplicates$PERMNO, collapse = ", "))
  }
  
  cat("Number of PERMNO that are in the dataset that aggregate x variables: ", length(unique(data_others$PERMNO)), "\n")
  
  cat("Number of PERMNO that are in the both datasets: ", length(unique(data_RET$PERMNO[data_RET$PERMNO %in% data_others$PERMNO])), "\n")
  
  # Join the returns values with other variables
  final_data <- data_RET %>%
    filter(PERMNO %in% data_others$PERMNO) %>%
    left_join(data_others, by = "PERMNO") %>%
    ungroup() %>%  
    left_join(FFF_dataset, by = "PERMNO") %>% 
    filter(returns != 0)
  
  cat("Number of PERMNO that are in the final dataset: ", length(unique(final_data$PERMNO)), "\n")
  
  return(final_data)
}
```

### Function to remove outlier
```{r}
#Function to remove outlier
remove_outliers_all <- function(data, variables) {
  # Initialise a logical vector to keep track of rows to retain
  keep_rows <- rep(TRUE, nrow(data))
  
  # Loop over each variable
  for (variable in variables) {
    # Compute outlier bounds
    bounds <- data %>%
      summarise(
        Q1 = quantile(.data[[variable]], 0.25, na.rm = TRUE),
        Q3 = quantile(.data[[variable]], 0.75, na.rm = TRUE),
        IQR = Q3 - Q1,
        lower_bound = Q1 - 1.5 * IQR,
        upper_bound = Q3 + 1.5 * IQR
      )
    
    # Apply the filtering based on computed bounds
    keep_rows <- keep_rows & (data[[variable]] >= bounds$lower_bound & data[[variable]] <= bounds$upper_bound)
  }
  
  # Return the cleaned data
  return(data[keep_rows, ])
}
```

## Model 1: Returns: 2019-12-14 to 2020-01-20 & X variables: before 2019-02-14
```{r}
# Period 1 - Dataset 1
task2_p1_df1_1 <- transform_dataset_for_task2(task2, "2019-12-14", "2020-01-20", "2019-08-20", "2019-12-14", "both", FFF_beta_noncovid) 

# Remove outlier for returns
task2_p1_df1_1 <- remove_outliers_all(task2_p1_df1_1, "returns")

# Inspect the distribution of Returns
print(
  ggplot(task2_p1_df1_1, aes(x = returns)) +
    geom_histogram(bins = 30, fill = "blue", color = "black", alpha = 0.7) +
    theme_minimal() +
    labs(title = "Histogram of Returns", x = "Returns", y = "Frequency")
)

cat("Sample size after remove outlier: ", length(unique(task2_p1_df1_1$PERMNO)))

# Create dummy variable for categorical variable
task2_reg_p1_df1 <- task2_p1_df1_1 %>%
  filter(!is.na(volatility)) %>% 
  mutate(company_size = factor(company_size, levels = unique(task2_p1_df1_1$company_size)),
         security_type = factor(security_type, levels = unique(task2_p1_df1_1$security_type)),
         SECTOR = factor(SECTOR, levels = unique(task2_p1_df1_1$SECTOR))) %>%
  bind_cols(as_tibble(model.matrix(~ company_size - 1, data = .))) %>%
  bind_cols(as_tibble(model.matrix(~ security_type - 1, data = .))) %>% 
  bind_cols(as_tibble(model.matrix(~ SECTOR - 1, data = .))) 
```

```{r echo = FALSE}
str(task2_reg_p1_df1)
head(task2_reg_p1_df1, n = 10)
```


### Make sure returns are accurately calculated
```{r echo=FALSE}
# Find the date with the most PERMNOs having the highest price change
highest_change_noncovid <- task2 %>%
  filter(date >= "2019-12-14" & date <= "2020-01-20") %>%
  group_by(PERMNO) %>%
  filter(PRC == max(PRC)) %>%
  ungroup() %>%
  group_by(date) %>%
  summarise(num_highest = n()) %>%
  arrange(desc(num_highest))

# Find the date with the most PERMNOs having the lowest price change
lowest_change_noncovid <- task2 %>%
  filter(date >= "2019-12-14" & date <= "2020-01-20") %>%
  group_by(PERMNO) %>%
  filter(PRC == min(PRC)) %>%
  ungroup() %>%
  group_by(date) %>%
  summarise(num_lowest = n()) %>%
  arrange(desc(num_lowest))

# Inspect the results
returns_noncovid <- task2 %>%
  filter(date == highest_change_noncovid$date[1] | date == lowest_change_noncovid$date[1]) %>% 
  group_by(PERMNO) %>% 
  summarise(max_change = PRC[date == "2020-01-17"],
    min_change = PRC[date == "2020-12-16"],
        returns_noncovid = (max_change - min_change) / min_change) %>% 
    select(PERMNO, returns_noncovid)

comparison_noncovid <- returns_noncovid  %>%
  left_join(task2_reg_p1_df1, by = "PERMNO") %>%
  mutate(difference_noncovid = returns_noncovid != returns) %>%
  filter(difference_noncovid) 
```

```{r echo=FALSE}
cat("The date that most PERMNOs has highest price change:", as.character(highest_change_noncovid$date[1]), "\n")
cat("The date that most PERMNOs has lowest price change:", as.character(lowest_change_noncovid$date[1]), "\n")

cat("Number of PERMNOs in the dataset:", 
    nrow(task2 %>% distinct(PERMNO)), "\n")
cat("Number of PERMNOs on the date that most PERMNOs has highest price change:", 
    nrow(task2 %>% filter(date == highest_change_noncovid$date[1]) %>% distinct(PERMNO)), "\n")
cat("Number of PERMNOs on the date that most PERMNOs has lowest price change:",
    nrow(task2 %>% filter(date == lowest_change_noncovid$date[1]) %>% distinct(PERMNO)), "\n")

# Print the total and the differing PERMNOs
cat("Number of PERMNO that should be include in the dataset but did not: ", nrow(comparison_noncovid))
cat("That PERMNO is: ", comparison_noncovid$PERMNO)
```

### Correlation of variable
```{r}
#Find the correlation between retruns and each potential explanatory variables
cor_p1_m1 <- cor(task2_reg_p1_df1 %>% 
                   select(-c(PERMNO, SHRCD, TICKER, SECTOR, company_size, security_type)), 
                 use = "pairwise.complete.obs")

# Correlation over absolute 0.7 = high risk of multicollinearity
high_corr_p1_m1 <- which(abs(cor_p1_m1) > 0.7, arr.ind = TRUE)
high_corr_pairs_p1_m1 <- data.frame(
  Feature1 = rownames(cor_p1_m1)[high_corr_p1_m1[,1]],
  Feature2 = colnames(cor_p1_m1)[high_corr_p1_m1[,2]],
  Correlation = cor_p1_m1[high_corr_p1_m1]
)

high_corr_pairs_p1_m1 <- high_corr_pairs_p1_m1 %>% 
  filter(Correlation < 1)

# Inspect the variables with multicollinearity
head(high_corr_pairs_p1_m1)

# Inspect all correlations
head(melt(cor_p1_m1))
```

### Build Model 1
```{r}
# Splitting the dataset into training and testing sets
set.seed(696)  

# Non-COVID Period
X_p1_m1 <- task2_reg_p1_df1 %>% 
  select(Mkt_beta, SMB_beta, HML_beta, log_turnover_ratio, scaled_spread, log_volatility, log_market_share, vol_cap_ratio, company_sizeSmall, company_sizeLarge, SECTORMaterials, SECTORIndustrials, SECTORFinancials, SECTORInformation_Technology, SECTORConsumer_Discretionary, SECTORHealth_Care, SECTOREnergy, SECTORUtilities, SECTORReal_Estate, SECTORCommunication_Services, SECTORUnknown)

y_p1_m1 <- task2_reg_p1_df1$returns

splitIndex_p1_m1 <- createDataPartition(y_p1_m1, 
                                        p = 0.8, 
                                        list = FALSE)
X_train_data_p1_m1 <- X_p1_m1[splitIndex_p1_m1, ]
X_test_data_p1_m1 <- X_p1_m1[-splitIndex_p1_m1, ]
y_train_data_p1_m1 <- y_p1_m1[splitIndex_p1_m1]
y_test_data_p1_m1 <- y_p1_m1[-splitIndex_p1_m1]

train_data_p1_m1 <- cbind(X_train_data_p1_m1, returns = y_train_data_p1_m1)

# Build Model
model_1_p1 <- lm(returns ~ ., data = train_data_p1_m1)

# Prediction for training sample
pred_train_p1_m1 <- predict(model_1_p1, newdata = X_train_data_p1_m1)

metrics_train_p1_m1 <- postResample(pred = pred_train_p1_m1, 
                                    obs = train_data_p1_m1$returns)
metrics_table_train_p1_m1 <- data.frame(
  Metric = c('MSE', 'RMSE', 'MAE', 'R-squared'),
  Value = c(metrics_train_p1_m1["RMSE"]^2,
            metrics_train_p1_m1["RMSE"], 
            metrics_train_p1_m1["MAE"], 
            metrics_train_p1_m1["Rsquared"])
)

# Prediction for testing sample
pred_test_p1_m1 <- predict(model_1_p1, newdata = X_test_data_p1_m1)
metrics_test_p1_m1 <- postResample(pred = pred_test_p1_m1, 
                                   obs = y_test_data_p1_m1)
metrics_table_test_p1_m1 <- data.frame(
  Metric = c('MSE', 'RMSE', 'MAE', 'R-squared'),
  Value = c(metrics_test_p1_m1["RMSE"]^2
, metrics_test_p1_m1["RMSE"], metrics_test_p1_m1["MAE"], metrics_test_p1_m1["Rsquared"])
)
```

```{r echo=FALSE}
cat("Performance Metrics for the Training Dataset:\n")
print(metrics_table_train_p1_m1)
cat("Performance Metrics for the Testing Dataset:\n")
print(metrics_table_test_p1_m1)

# Summary of model
cat("Regression coefficient of Model 1:\n")
summary(model_1_p1)

cat("To make sure the variable are not multicollinear (VIF < 5)")
print(vif(model_1_p1))
cat("Despite SectorUnknown has a VIF over 5, considering it is actually the group of data with missing value in Sector, while some other industry categories are important to the model. After trial and error, it is better to keep the industry variable in the model.")
```


```{r}
# Scatterplot of Prediction vs Actual
plot_data_p1_m1 <- data.frame(
  Predicted = pred_test_p1_m1,
  Actual = y_test_data_p1_m1
)

ggplot(data = plot_data_p1_m1,
       aes(x = Predicted,
           y = Actual)) +
  
  geom_point() +
  
  geom_smooth(method = "lm",
              se = FALSE,
              color = "blue",
              aes(linetype = "Regression Line")) +
  
  scale_linetype_manual(name = NULL,
                        values = "solid",
                        labels = "Regression Line") +
  
  theme_minimal() +
  
  theme(legend.position = "bottom") +
  
  labs(title = "Evaluating Model 1's Performance: Predicted vs. Actual Returns in the Non-COVID Period",
       x = "Predicted returns",
       y = "Actual returns")
```

### Important variable
```{r}
# Extract coefficients
coef_m1 <- coef(model_1_p1)

importance_m1 <- data.frame(
  Factors = names(coef_m1),
  Coefficient = coef_m1
)

importance_m1 <- importance_m1[importance_m1$Factor != "(Intercept)", ]

# Plot with positive and negative impacts
plot_m1_p1_factors <- ggplot(importance_m1, 
                             aes(x = reorder(Factors, Coefficient), 
                                 y = Coefficient, fill = Coefficient > 0)) +
  
  geom_bar(stat = "identity") +
  
  geom_text(aes(label = round(Coefficient, 3)),
            vjust = ifelse(importance_m1$Coefficient > 0, 0.5, 0.3),
            color = "black") +
  
  coord_flip() +
  
  labs(title = "Impact of factors on Returns for Model 1",
       x = "Factors",
       y = "Coefficient") +
  
  theme_minimal() +
  
  scale_fill_manual(
    values = c("TRUE" = "steelblue", "FALSE" = "coral"),
    labels = c("TRUE" = "Positive Impact", 
               "FALSE" = "Negative Impact"),
    name = ""
  ) +
  
  theme(
    axis.title.y = element_text(color = 'black'),
    axis.text.y = element_text(color = 'black'),
    legend.position = 'bottom'
  ) 

print(plot_m1_p1_factors)
```

### Group by Sector
```{r}
# Apply the model particularly by industry
pred_sector_p1_m1  <- cbind(X_test_data_p1_m1, SECTOR = task2_reg_p1_df1$SECTOR[-splitIndex_p1_m1])

data_sector_p1_m1 <- data.frame(
  Predicted = pred_test_p1_m1,
  Actual = y_test_data_p1_m1,
  SECTOR = pred_sector_p1_m1$SECTOR
)

data_sector_p1_m1$SECTOR <- as.character(data_sector_p1_m1$SECTOR)

# Calculate evaluation metrics to see the performance of model apply on SECTOR
metrics_table_sector_p1_m1 <- data_sector_p1_m1 %>% 
  group_by(SECTOR) %>% 
  summarise(
    MSE = mean((Actual - Predicted)^2),
    RMSE = sqrt(MSE),
    MAE = mean(abs(Actual - Predicted)),
    residuals = sum((Actual - Predicted)^2),
    total = sum((Actual - mean(Actual))^2),
    R_Squared = 1 - (residuals / total)
  ) %>% 
  select(-c(residuals, total)) 

print(metrics_table_sector_p1_m1)

# Plot the prediction vs actual by SECTOR
plot_m1_p1_sector <- ggplot(data = data_sector_p1_m1, 
                     aes(x = Predicted, 
                         y = Actual,
                         color = SECTOR)) +
  
  geom_point(size = 2) +
  
  geom_smooth(method = "lm", 
              se = FALSE, 
              color = "blue", 
              aes(linetype = "Regression Line")) + 
  
  scale_linetype_manual(name = NULL, 
                        values = "solid", 
                        labels = "Regression Line") + 
  
  theme_minimal() +
  
  theme(legend.position = "bottom",
        plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 12),
        strip.text = element_text(size = 12, face = "bold")) + 
  
  labs(title = "Evaluating Model 1's Performance: Predicted vs. Actual Returns",
       subtitle = "During the Non-COVID Period (Grouped by Industry Sector)",  
       x = "Predicted returns", 
       y = "Actual returns",
       color = "Industry Sector")


print(plot_m1_p1_sector)
```

### Group by Security Type
```{r}
# Apply the model particularly by security type
pred_type_p1_m1  <- cbind(X_test_data_p1_m1, security_type = task2_reg_p1_df1$security_type[-splitIndex_p1_m1])

data_type_p1_m1 <- data.frame(
  Predicted = pred_test_p1_m1,
  Actual = y_test_data_p1_m1,
  security_type = pred_type_p1_m1$security_type
)

# Calculate evaluation metrics to see the performance of model apply on security type
metrics_table_type_p1_m1 <- data_type_p1_m1 %>% 
  group_by(security_type) %>% 
  summarise(
    MSE = mean((Actual - Predicted)^2),
    RMSE = sqrt(MSE),
    MAE = mean(abs(Actual - Predicted)),
    residuals = sum((Actual - Predicted)^2),
    total = sum((Actual - mean(Actual))^2),
    R_Squared = 1 - (residuals / total)
  ) %>% 
  select(-c(residuals, total))

print(metrics_table_type_p1_m1)

# Plot the prediction vs actual by security type
plot_m1_p1_type <- ggplot(data = data_type_p1_m1, 
                     aes(x = Predicted, 
                         y = Actual,
                         color = security_type)) +
  geom_point(size = 2) +
  geom_smooth(method = "lm", 
              se = FALSE, 
              color = "blue", 
              aes(linetype = "Regression Line")) + 
  scale_linetype_manual(name = NULL, 
                        values = "solid", 
                        labels = "Regression Line") + 
  theme_minimal() +
  theme(legend.position = "bottom",
        plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 12),
        strip.text = element_text(size = 12, face = "bold")) +  
  labs(title = "Evaluating Model 1's Performance: Predicted vs. Actual Returns",
       subtitle = "During the Non-COVID Period (Grouped by Security Type)",  
       x = "Predicted returns", 
       y = "Actual returns",
       color = "Security Type")


print(plot_m1_p1_type)
```

### Group by Company size
```{r}
# Apply the model particularly by company size
pred_size_p1_m1  <- cbind(X_test_data_p1_m1, company_size = task2_reg_p1_df1$company_size[-splitIndex_p1_m1])

data_size_p1_m1 <- data.frame(
  Predicted = pred_test_p1_m1,
  Actual = y_test_data_p1_m1,
  company_size = pred_size_p1_m1$company_size
)

# Calculate evaluation metrics to see the performance of model apply on company size
metrics_table_size_p1_m1 <- data_size_p1_m1 %>% 
  group_by(company_size) %>% 
  summarise(
    MSE = mean((Actual - Predicted)^2),
    RMSE = sqrt(MSE),
    MAE = mean(abs(Actual - Predicted)),
    residuals = sum((Actual - Predicted)^2),
    total = sum((Actual - mean(Actual))^2),
    R_Squared = 1 - (residuals / total)
  ) %>% 
  select(-c(residuals, total))

print(metrics_table_size_p1_m1)

# Plot the prediction vs actual by company size
plot_m1_p1_size <- ggplot(data = data_size_p1_m1, 
                     aes(x = Predicted, 
                         y = Actual,
                         color = company_size)) +
  geom_point(size = 2, alpha = 0.7) +
  geom_smooth(method = "lm", 
              se = FALSE, 
              color = "blue", 
              aes(linetype = "Regression Line")) + 
  scale_linetype_manual(name = NULL, 
                        values = "solid", 
                        labels = "Regression Line") + 
  theme_minimal() +
  theme(legend.position = "bottom",
        plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 12),
        strip.text = element_text(size = 12, face = "bold")) +  
  labs(title = "Evaluating Model 1's Performance: Predicted vs. Actual Returns",
       subtitle = "During the Non-COVID Period (Grouped by Company Size)",  
       x = "Predicted returns", 
       y = "Actual returns",
       color = "Company Size")


print(plot_m1_p1_size)
```


## Model 2: Returns: 2020-02-14 to 2020-03-20 & X variables: before 2019-02-14
```{r}
# Period 1 - Dataset 1
task2_p2_df1_1 <- transform_dataset_for_task2(task2, "2020-02-14", "2020-03-20", "2019-08-20", "2020-02-14", "both", FFF_beta_covid)

# Remove outlier for returns
task2_p2_df1_1 <- remove_outliers_all(task2_p2_df1_1, "returns")

# Inspect the distribution of Returns
print(
  ggplot(task2_p2_df1_1, aes(x = returns)) +
    geom_histogram(bins = 30, fill = "blue", color = "black", alpha = 0.7) +
    theme_minimal() +
    labs(title = "Histogram of Returns", x = "Returns", y = "Frequency")
)

cat("Sample size after remove outlier: ", length(unique(task2_p2_df1_1$PERMNO)))

# Create dummy variable for categorical variable
task2_reg_p2_df1 <- task2_p2_df1_1 %>%
  filter(!is.na(volatility)) %>% 
  mutate(company_size = factor(company_size, levels = unique(task2_p2_df1_1$company_size)),
         security_type = factor(security_type, levels = unique(task2_p2_df1_1$security_type)),
         SECTOR = factor(SECTOR, levels = unique(task2_p2_df1_1$SECTOR))) %>%
  bind_cols(as_tibble(model.matrix(~ company_size - 1, data = .))) %>%
  bind_cols(as_tibble(model.matrix(~ security_type - 1, data = .))) %>% 
  bind_cols(as_tibble(model.matrix(~ SECTOR - 1, data = .))) 
```

```{r echo = FALSE}
str(task2_reg_p2_df1)
head(task2_reg_p2_df1, n = 10)
```

### To make sure returns are accurately calculated
```{R}
# Find the date with the most PERMNOs having the highest price change
highest_change_covid <- task2 %>%
  filter(date >= "2020-02-14" & date <= "2020-03-20") %>%
  group_by(PERMNO) %>%
  filter(PRC == max(PRC)) %>%
  ungroup() %>%
  group_by(date) %>%
  summarise(num_highest = n()) %>%
  arrange(desc(num_highest))

# Find the date with the most PERMNOs having the lowest price change
lowest_change_covid <- task2 %>%
  filter(date >= "2020-02-14" & date <= "2020-03-20") %>%
  group_by(PERMNO) %>%
  filter(PRC == min(PRC)) %>%
  ungroup() %>%
  group_by(date) %>%
  summarise(num_lowest = n()) %>%
  arrange(desc(num_lowest))

# Inspect the results
returns_covid <- task2 %>%
  filter(date == highest_change_covid$date[1] | date == lowest_change_covid$date[1]) %>% 
  group_by(PERMNO) %>% 
  summarise(max_change = PRC[date == highest_change_covid$date[1]],
    min_change = PRC[date == lowest_change_covid$date[1]],
    returns_covid = (max_change - min_change) / min_change) %>% 
    select(PERMNO, returns_covid)

comparison_covid <- returns_covid  %>%
  left_join(task2_reg_p2_df1, by = "PERMNO") %>%
  mutate(difference_covid = returns_covid != returns) %>%
  filter(difference_covid) 
```

```{r echo=FALSE}
cat("The date that most PERMNOs has highest price change:", as.character(highest_change_covid$date[1]), "\n")
cat("The date that most PERMNOs has lowest price change:", as.character(lowest_change_covid$date[1]), "\n")

cat("Number of PERMNOs in the dataset:", 
    nrow(task2 %>% distinct(PERMNO)), "\n")
cat("Number of PERMNOs on the date that most PERMNOs has highest price change:", 
    nrow(task2 %>% filter(date == highest_change_covid$date[1]) %>% distinct(PERMNO)), "\n")
cat("Number of PERMNOs on the date that most PERMNOs has lowest price change:",
    nrow(task2 %>% filter(date == lowest_change_covid$date[1]) %>% distinct(PERMNO)), "\n")

# Print the total and the differing PERMNOs
cat("Number of PERMNO that should be include in the dataset but did not: ", nrow(comparison_covid))
cat("That PERMNO is: ", comparison_covid$PERMNO)
```

### Correlation of variable
```{r}
#Find the correlation between retruns and each potential explanatory variables
cor_p2_m1 <- cor(task2_reg_p2_df1%>% select(-c(PERMNO, SHRCD, TICKER, SECTOR, company_size, security_type)), use = "pairwise.complete.obs")

# Correlation over absolute 0.7 = high risk of multicollinearity
high_corr_p2_m1 <- which(abs(cor_p2_m1) > 0.7, arr.ind = TRUE)
high_corr_pairs_p2_m1 <- data.frame(
  Feature1 = rownames(cor_p2_m1)[high_corr_p2_m1[,1]],
  Feature2 = colnames(cor_p2_m1)[high_corr_p2_m1[,2]],
  Correlation = cor_p2_m1[high_corr_p2_m1]
)
high_corr_pairs_p2_m1 <- high_corr_pairs_p2_m1 %>% filter(Correlation < 1)

# Inspect the variables with multicollinearity
head(high_corr_pairs_p2_m1)

# Inspect all correlations
head(melt(cor_p2_m1))
```

### Build Model 2
```{r}
# Splitting the dataset into training and testing sets
set.seed(781)  

# COVID Period
X_p2_m2 <- task2_reg_p2_df1 %>% 
  select(Mkt_beta, SMB_beta, HML_beta, log_volatility, log_spread, scaled_market_share, scaled_abs_corr_sp, scaled_dollar_vol, SECTORMaterials, SECTORIndustrials, SECTORFinancials, SECTORInformation_Technology, SECTORConsumer_Discretionary, SECTORHealth_Care, SECTOREnergy, SECTORUtilities, SECTORReal_Estate, SECTORCommunication_Services, SECTORUnknown)

y_p2_m2 <- task2_reg_p2_df1$returns

splitIndex_p2_m2 <- createDataPartition(y_p2_m2, 
                                        p = 0.8, 
                                        list = FALSE)
X_train_data_p2_m2 <- X_p2_m2[splitIndex_p2_m2, ]
X_test_data_p2_m2 <- X_p2_m2[-splitIndex_p2_m2, ]
y_train_data_p2_m2 <- y_p2_m2[splitIndex_p2_m2]
y_test_data_p2_m2 <- y_p2_m2[-splitIndex_p2_m2]

train_data_p2_m2 <- cbind(X_train_data_p2_m2, returns = y_train_data_p2_m2)

# Build Model
model_2_p2 <- lm(returns ~ ., data = train_data_p2_m2)

# Prediction for training sample - Covid
pred_train_p2_m2 <- predict(model_2_p2, newdata = X_train_data_p2_m2)

metrics_train_p2_m2 <- postResample(pred = pred_train_p2_m2, 
                                    obs = train_data_p2_m2$returns)
metrics_table_train_p2_m2 <- data.frame(
  Metric = c('MSE', 'RMSE', 'MAE', 'R-squared'),
  Value = c(metrics_train_p2_m2["RMSE"]^2,
            metrics_train_p2_m2["RMSE"], 
            metrics_train_p2_m2["MAE"], 
            metrics_train_p2_m2["Rsquared"])
)

# Prediction for testing sample - covid
pred_test_p2_m2 <- predict(model_2_p2, newdata = X_test_data_p2_m2)
metrics_test_p2_m2 <- postResample(pred = pred_test_p2_m2, 
                                   obs = y_test_data_p2_m2)
metrics_table_test_p2_m2 <- data.frame(
  Metric = c('MSE', 'RMSE', 'MAE', 'R-squared'),
  Value = c(metrics_test_p2_m2["RMSE"]^2
, metrics_test_p2_m2["RMSE"], metrics_test_p2_m2["MAE"], metrics_test_p2_m2["Rsquared"])
)
```

```{r echo=FALSE}
cat("Performance Metrics for the Training Dataset:\n")
print(metrics_table_train_p2_m2)
cat("Performance Metrics for the Testing Dataset:\n")
print(metrics_table_test_p2_m2)

# Summary of model
cat("Regression coefficient of Model 1:\n")
summary(model_2_p2)

cat("To make sure the variable are not multicollinear (VIF < 5)")
print(vif(model_2_p2))
cat("Despite SectorUnknown has a VIF over 5, considering it is actually the group of data with missing value in Sector, while some other industry categories are important to the model. After trial and error, it is better to keep the industry variable in the model.")
```

```{r}
# Scatterplot of Prediction vs Actual
plot_data_p2_m2 <- data.frame(
  Predicted = pred_test_p2_m2,
  Actual = y_test_data_p2_m2
)

plot_m2_p2 <- ggplot(data = plot_data_p2_m2,
                     aes(x = Predicted,
                         y = Actual)) +
  geom_point() +
  geom_smooth(method = "lm",
              se = FALSE,
              color = "blue",
              aes(linetype = "Regression Line")) +
  scale_linetype_manual(name = NULL,
                        values = "solid",
                        labels = "Regression Line") +
  theme_minimal() +
  theme(legend.position = "bottom") +
  labs(title = "Evaluating Model 2's Performance: Predicted vs. Actual Returns in the Non-covid Period",
       x = "Predicted returns",
       y = "Actual returns")

print(plot_m2_p2)
```

### Important variable
```{r}
# Extract coefficients
coef_m2 <- coef(model_2_p2)

importance_m2 <- data.frame(
  Factors = names(coef_m2),
  Coefficient = coef_m2
)
importance_m2 <- importance_m2[importance_m2$Factor != "(Intercept)", ]
# Plot with positive and negative impacts
plot_m2_p2_factors <- ggplot(importance_m2, aes(x = reorder(Factors, Coefficient), y = Coefficient, fill = Coefficient > 0)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(Coefficient, 3)),
            vjust = ifelse(importance_m2$Coefficient > 0, 0.5, 0.3),
            color = "black") +
  coord_flip() +
  labs(title = "Impact of factors on returns for Model 2",
       x = "Factors",
       y = "Coefficient") +
  theme_minimal() +
  scale_fill_manual(
    values = c("TRUE" = "steelblue", "FALSE" = "coral"),
    labels = c("TRUE" = "Positive Impact", 
               "FALSE" = "Negative Impact"),
    name = ""
  ) +
  theme(
    axis.title.y = element_text(color = 'black'),
    axis.text.y = element_text(color = 'black'),
    legend.position = 'bottom'
  ) 
```

### Group by Sector
```{r}
pred_sector_p2_m2  <- cbind(X_test_data_p2_m2, SECTOR = task2_reg_p2_df1$SECTOR[-splitIndex_p2_m2])

data_sector_p2_m2 <- data.frame(
  Predicted = pred_test_p2_m2,
  Actual = y_test_data_p2_m2,
  SECTOR = pred_sector_p2_m2$SECTOR
)

data_sector_p2_m2$SECTOR <- as.character(data_sector_p2_m2$SECTOR)

# Calculating performance metrics by sector
metrics_table_sector_p2_m2 <- data_sector_p2_m2 %>% 
  group_by(SECTOR) %>% 
  summarise(
    MSE = mean((Actual - Predicted)^2),
    RMSE = sqrt(MSE),
    MAE = mean(abs(Actual - Predicted)),
    residuals = sum((Actual - Predicted)^2),
    total = sum((Actual - mean(Actual))^2),
    R_Squared = 1 - (residuals / total)
  ) %>% 
  select(-c(residuals, total))

print(metrics_table_sector_p2_m2)

plot_m2_p2_sector <- ggplot(data = data_sector_p2_m2, 
                     aes(x = Predicted, 
                         y = Actual,
                         color = SECTOR)) +
  geom_point(size = 2) +
  geom_smooth(method = "lm", 
              se = FALSE, 
              color = "blue", 
              aes(linetype = "Regression Line")) + 
  scale_linetype_manual(name = NULL, 
                        values = "solid", 
                        labels = "Regression Line") + 
  theme_minimal() +
  theme(legend.position = "bottom",
        plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 12),
        strip.text = element_text(size = 12, face = "bold")) +  
  labs(title = "Evaluating Model 2's Performance: Predicted vs. Actual Returns",
       subtitle = "During the COVID Period (Grouped by Industry Sector)",  
       x = "Predicted returns", 
       y = "Actual returns",
       color = "Industry Sector")


print(plot_m2_p2_sector)
```

### Group by Security Type
```{r}
pred_type_p2_m2  <- cbind(X_test_data_p2_m2, security_type = task2_reg_p2_df1$security_type[-splitIndex_p2_m2])

data_type_p2_m2 <- data.frame(
  Predicted = pred_test_p2_m2,
  Actual = y_test_data_p2_m2,
  security_type = pred_type_p2_m2$security_type
)

# Calculating performance metrics by sector
metrics_table_type_p2_m2 <- data_type_p2_m2 %>% 
  group_by(security_type) %>% 
  summarise(
    MSE = mean((Actual - Predicted)^2),
    RMSE = sqrt(MSE),
    MAE = mean(abs(Actual - Predicted)),
    residuals = sum((Actual - Predicted)^2),
    total = sum((Actual - mean(Actual))^2),
    R_Squared = 1 - (residuals / total)
  ) %>% 
  select(-c(residuals, total))

print(metrics_table_type_p2_m2)

plot_m2_p2_type <- ggplot(data = data_type_p2_m2, 
                     aes(x = Predicted, 
                         y = Actual,
                         color = security_type)) +
  geom_point(size = 2) +
  geom_smooth(method = "lm", 
              se = FALSE, 
              color = "blue", 
              aes(linetype = "Regression Line")) + 
  scale_linetype_manual(name = NULL, 
                        values = "solid", 
                        labels = "Regression Line") + 
  theme_minimal() +
  theme(legend.position = "bottom",
        plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 12),
        strip.text = element_text(size = 12, face = "bold")) +  
  labs(title = "Evaluating Model 2's Performance: Predicted vs. Actual Returns",
       subtitle = "During the COVID Period (Grouped by Security Type)",  
       x = "Predicted returns", 
       y = "Actual returns",
       color = "Security Type")


print(plot_m2_p2_type)
```

### Group by Company size
```{r}
pred_size_p2_m2  <- cbind(X_test_data_p2_m2, company_size = task2_reg_p2_df1$company_size[-splitIndex_p2_m2])

data_size_p2_m2 <- data.frame(
  Predicted = pred_test_p2_m2,
  Actual = y_test_data_p2_m2,
  company_size = pred_size_p2_m2$company_size
)

# Calculating performance metrics by sector
metrics_table_size_p2_m2 <- data_size_p2_m2 %>% 
  group_by(company_size) %>% 
  summarise(
    MSE = mean((Actual - Predicted)^2),
    RMSE = sqrt(MSE),
    MAE = mean(abs(Actual - Predicted)),
    residuals = sum((Actual - Predicted)^2),
    total = sum((Actual - mean(Actual))^2),
    R_Squared = 1 - (residuals / total)
  ) %>% 
  select(-c(residuals, total))

print(metrics_table_size_p2_m2)

plot_m2_p2_size <- ggplot(data = data_size_p2_m2, 
                     aes(x = Predicted, 
                         y = Actual,
                         color = company_size)) +
  geom_point(size = 2, alpha = 0.7) +
  geom_smooth(method = "lm", 
              se = FALSE, 
              color = "blue", 
              aes(linetype = "Regression Line")) + 
  scale_linetype_manual(name = NULL, 
                        values = "solid", 
                        labels = "Regression Line") + 
  theme_minimal() +
  theme(legend.position = "bottom",
        plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 12),
        strip.text = element_text(size = 12, face = "bold")) +  
  labs(title = "Evaluating Model 2's Performance: Predicted vs. Actual Returns",
       subtitle = "During the COVID Period (Grouped by Company Size)",  
       x = "Predicted returns", 
       y = "Actual returns",
       color = "Company Size")


print(plot_m2_p2_size)
```

## Model 3: (ETFs) Returns: 2019-12-14 to 2020-01-20 & X variables: before 2019-02-14
```{r}
# Period 1 - Dataset 2
task2_p1_df2_1 <- transform_dataset_for_task2(task2, "2019-12-14", "2020-01-20", "2019-08-20", "2019-12-14", "ETFs", FFF_beta_noncovid) 

# Remove outlier for returns
task2_p1_df2_1 <- remove_outliers_all(task2_p1_df2_1, "returns")

# Inspect the distribution of Returns
print(
  ggplot(task2_p1_df2_1, aes(x = returns)) +
    geom_histogram(bins = 30, fill = "blue", color = "black", alpha = 0.7) +
    theme_minimal() +
    labs(title = "Histogram of Returns", x = "Returns", y = "Frequency")
)

cat("Sample size after remove outlier: ", length(unique(task2_p1_df2_1$PERMNO)))


# Create dummy variable for categorical variable
task2_reg_p1_df2 <- task2_p1_df2_1 %>%
  filter(!is.na(volatility)) %>% 
  mutate(company_size = factor(company_size, levels = unique(task2_p1_df2_1$company_size)),
         SECTOR = factor(SECTOR, levels = unique(task2_p1_df2_1$SECTOR))) %>%
  bind_cols(as_tibble(model.matrix(~ company_size - 1, data = .))) %>%
  bind_cols(as_tibble(model.matrix(~ SECTOR - 1, data = .))) 
```

```{r echo = FALSE}
str(task2_reg_p1_df2)
head(task2_reg_p1_df2, n = 10)
```

### Correlation of variable
```{r}
#Find the correlation between retruns and each potential explanatory variables
cor_p1_m3 <- cor(task2_reg_p1_df2%>% select(-c(PERMNO, SHRCD, TICKER, SECTOR, company_size, security_type)), use = "pairwise.complete.obs")

# Correlation over absolute 0.7 = high risk of multicollinearity
high_corr_p1_m3 <- which(abs(cor_p1_m3) > 0.7, arr.ind = TRUE)
high_corr_pairs_p1_m3 <- data.frame(
  Feature1 = rownames(cor_p1_m3)[high_corr_p1_m3[,1]],
  Feature2 = colnames(cor_p1_m3)[high_corr_p1_m3[,2]],
  Correlation = cor_p1_m3[high_corr_p1_m3]
)
high_corr_pairs_p1_m3 <- high_corr_pairs_p1_m3 %>% filter(Correlation < 1)

# Inspect the variables with multicollinearity
head(high_corr_pairs_p1_m3)

# Inspect all correlations
head(melt(cor_p1_m3))
```

### Build Model 3
```{r}
# Splitting the dataset into training and testing sets
set.seed(847)  

# Non-COVID Period
X_p1_m3 <- task2_reg_p1_df2 %>% 
  select(Mkt_beta, log_dollar_vol, log_tracking_error, log_market_share, log_turnover_ratio, log_volatility)

y_p1_m3 <- task2_reg_p1_df2$returns

splitIndex_p1_m3 <- createDataPartition(y_p1_m3, 
                                        p = 0.8, 
                                        list = FALSE)
X_train_data_p1_m3 <- X_p1_m3[splitIndex_p1_m3, ]
X_test_data_p1_m3 <- X_p1_m3[-splitIndex_p1_m3, ]
y_train_data_p1_m3 <- y_p1_m3[splitIndex_p1_m3]
y_test_data_p1_m3 <- y_p1_m3[-splitIndex_p1_m3]

train_data_p1_m3 <- cbind(X_train_data_p1_m3, returns = y_train_data_p1_m3)

# Build Model
model_3_p1 <- lm(returns ~ ., data = train_data_p1_m3)

# Prediction for training sample
pred_train_p1_m3 <- predict(model_3_p1, newdata = X_train_data_p1_m3)

metrics_train_p1_m3 <- postResample(pred = pred_train_p1_m3, 
                                    obs = train_data_p1_m3$returns)
metrics_table_train_p1_m3 <- data.frame(
  Metric = c('MSE', 'RMSE', 'MAE', 'R-squared'),
  Value = c(metrics_train_p1_m3["RMSE"]^2,
            metrics_train_p1_m3["RMSE"], 
            metrics_train_p1_m3["MAE"], 
            metrics_train_p1_m3["Rsquared"])
)

# Prediction for testing sample
pred_test_p1_m3 <- predict(model_3_p1, newdata = X_test_data_p1_m3)
metrics_test_p1_m3 <- postResample(pred = pred_test_p1_m3, 
                                   obs = y_test_data_p1_m3)
metrics_table_test_p1_m3 <- data.frame(
  Metric = c('MSE', 'RMSE', 'MAE', 'R-squared'),
  Value = c(metrics_test_p1_m3["RMSE"]^2
, metrics_test_p1_m3["RMSE"], metrics_test_p1_m3["MAE"], metrics_test_p1_m3["Rsquared"])
)
```

```{r echo=FALSE}
cat("Performance Metrics for the Training Dataset:\n")
print(metrics_table_train_p1_m3)
cat("Performance Metrics for the Testing Dataset:\n")
print(metrics_table_test_p1_m3)

# Summary of model
cat("Regression coefficient of Model 1:\n")
summary(model_3_p1)

cat("To make sure the variable are not multicollinear (VIF < 5)")
print(vif(model_3_p1))
```

```{r}
# Scatterplot of Prediction vs Actual
plot_data_p1_m3 <- data.frame(
  Predicted = pred_test_p1_m3,
  Actual = y_test_data_p1_m3
)

plot_m3_p1 <- ggplot(data = plot_data_p1_m3,
                     aes(x = Predicted,
                         y = Actual)) +
  geom_point() +
  geom_smooth(method = "lm",
              se = FALSE,
              color = "blue",
              aes(linetype = "Regression Line")) +
  scale_linetype_manual(name = NULL,
                        values = "solid",
                        labels = "Regression Line") +
  theme_minimal() +
  theme(legend.position = "bottom") +
  labs(title = "Evaluating Model 3's Performance: Predicted vs. Actual Returns in the Non-COVID Period",
       x = "Predicted returns",
       y = "Actual returns")

print(plot_m3_p1)
```

### Important variable
```{r}
# Extract coefficients
coef_m3 <- coef(model_3_p1)

importance_m3 <- data.frame(
  Factors = names(coef_m3),
  Coefficient = coef_m3
)
importance_m3 <- importance_m3[importance_m3$Factor != "(Intercept)", ]

# Plot with positive and negative impacts
plot_m3_p1_factors <- ggplot(importance_m3, aes(x = reorder(Factors, Coefficient), y = Coefficient, fill = Coefficient > 0)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(Coefficient, 3)),
            vjust = ifelse(importance_m3$Coefficient > 0, 0.5, 0.3),
            color = "black") +
  coord_flip() +
  labs(title = "Impact of factors on Returns for Model 3",
       x = "Factors",
       y = "Coefficient") +
  theme_minimal() +
  scale_fill_manual(
    values = c("TRUE" = "steelblue", "FALSE" = "coral"),
    labels = c("TRUE" = "Positive Impact", 
               "FALSE" = "Negative Impact"),
    name = ""
  ) +
  theme(
    axis.title.y = element_text(color = 'black'),
    axis.text.y = element_text(color = 'black'),
    legend.position = 'bottom'
  ) 

print(plot_m3_p1_factors)
```

### Group by Sector
```{r}
# Apply the model particularly by industry
pred_sector_p1_m3  <- cbind(X_test_data_p1_m3, SECTOR = task2_reg_p1_df2$SECTOR[-splitIndex_p1_m3])

data_sector_p1_m3 <- data.frame(
  Predicted = pred_test_p1_m3,
  Actual = y_test_data_p1_m3,
  SECTOR = pred_sector_p1_m3$SECTOR
)

data_sector_p1_m3$SECTOR <- as.character(data_sector_p1_m3$SECTOR)

# Calculate evaluation metrics to see the performance of model apply on SECTOR
metrics_table_sector_p1_m3 <- data_sector_p1_m3 %>% 
  group_by(SECTOR) %>% 
  summarise(
    MSE = mean((Actual - Predicted)^2),
    RMSE = sqrt(MSE),
    MAE = mean(abs(Actual - Predicted)),
    residuals = sum((Actual - Predicted)^2),
    total = sum((Actual - mean(Actual))^2),
    R_Squared = 1 - (residuals / total)
  ) %>% 
  select(-c(residuals, total)) 

print(metrics_table_sector_p1_m3)

# Plot the prediction vs actual by SECTOR
plot_m3_p1_sector <- ggplot(data = data_sector_p1_m3, 
                     aes(x = Predicted, 
                         y = Actual,
                         color = SECTOR)) +
  geom_point(size = 2) +
  geom_smooth(method = "lm", 
              se = FALSE, 
              color = "blue", 
              aes(linetype = "Regression Line")) + 
  scale_linetype_manual(name = NULL, 
                        values = "solid", 
                        labels = "Regression Line") + 
  theme_minimal() +
  theme(legend.position = "bottom",
        plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 12),
        strip.text = element_text(size = 12, face = "bold")) +  
  labs(title = "Evaluating Model 3's Performance: Predicted vs. Actual Returns",
       subtitle = "During the Non-COVID Period (Grouped by Industry Sector)",  
       x = "Predicted returns", 
       y = "Actual returns",
       color = "Industry Sector")


print(plot_m3_p1_sector)
```

### Group by Company size
```{r}
# Apply the model particularly by company size
pred_size_p1_m3  <- cbind(X_test_data_p1_m3, company_size = task2_reg_p1_df2$company_size[-splitIndex_p1_m3])

data_size_p1_m3 <- data.frame(
  Predicted = pred_test_p1_m3,
  Actual = y_test_data_p1_m3,
  company_size = pred_size_p1_m3$company_size
)

# Calculate evaluation metrics to see the performance of model apply on company size
metrics_table_size_p1_m3 <- data_size_p1_m3 %>% 
  group_by(company_size) %>% 
  summarise(
    MSE = mean((Actual - Predicted)^2),
    RMSE = sqrt(MSE),
    MAE = mean(abs(Actual - Predicted)),
    residuals = sum((Actual - Predicted)^2),
    total = sum((Actual - mean(Actual))^2),
    R_Squared = 1 - (residuals / total)
  ) %>% 
  select(-c(residuals, total))

print(metrics_table_size_p1_m3)

# Plot the prediction vs actual by company size
plot_m3_p1_size <- ggplot(data = data_size_p1_m3, 
                     aes(x = Predicted, 
                         y = Actual,
                         color = company_size)) +
  geom_point(size = 2, alpha = 0.7) +
  geom_smooth(method = "lm", 
              se = FALSE, 
              color = "blue", 
              aes(linetype = "Regression Line")) + 
  scale_linetype_manual(name = NULL, 
                        values = "solid", 
                        labels = "Regression Line") + 
  theme_minimal() +
  theme(legend.position = "bottom",
        plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 12),
        strip.text = element_text(size = 12, face = "bold")) +  
  labs(title = "Evaluating Model 3's Performance: Predicted vs. Actual Returns",
       subtitle = "During the Non-COVID Period (Grouped by Company Size)",  
       x = "Predicted returns", 
       y = "Actual returns",
       color = "Company Size")


print(plot_m3_p1_size)
```

## Model 4: (ETF) Returns: 2020-02-14 to 2020-03-20 & X variables: before 2019-02-14
```{r}
# Period 2 - Dataset 2
task2_p2_df2_1 <- transform_dataset_for_task2(task2, "2020-02-14", "2020-03-20", "2019-08-20", "2020-02-14", "ETFs", FFF_beta_covid)

# Remove outlier for returns
task2_p2_df2_1 <- remove_outliers_all(task2_p2_df2_1, "returns")

# Inspect the distribution of Returns
print(
  ggplot(task2_p2_df2_1, aes(x = returns)) +
    geom_histogram(bins = 30, fill = "blue", color = "black", alpha = 0.7) +
    theme_minimal() +
    labs(title = "Histogram of Returns", x = "Returns", y = "Frequency")
)

cat("Sample size after remove outlier: ", length(unique(task2_p2_df2_1$PERMNO)))

# Create dummy variable for categorical variable
task2_reg_p2_df2 <- task2_p2_df2_1 %>%
  filter(!is.na(volatility)) %>% 
  mutate(company_size = factor(company_size, levels = unique(task2_p2_df2_1$company_size)),
         SECTOR = factor(SECTOR, levels = unique(task2_p2_df2_1$SECTOR))) %>%
  bind_cols(as_tibble(model.matrix(~ company_size - 1, data = .))) %>%
  bind_cols(as_tibble(model.matrix(~ SECTOR - 1, data = .))) 

# Inspect any missing value in important variables
colSums(is.na(task2_reg_p2_df2))
```

```{r echo = FALSE}
str(task2_reg_p2_df2)
head(task2_reg_p2_df2, n = 10)
```

### Correlation of variable
```{r}
#Find the correlation between retruns and each potential explanatory variables
cor_p2_m4 <- cor(task2_reg_p2_df2%>% select(-c(PERMNO, SHRCD, TICKER, SECTOR, company_size, security_type)), use = "pairwise.complete.obs")

# Correlation over absolute 0.7 = high risk of multicollinearity
high_corr_p2_m4 <- which(abs(cor_p2_m4) > 0.7, arr.ind = TRUE)
high_corr_pairs_p2_m4 <- data.frame(
  Feature1 = rownames(cor_p2_m4)[high_corr_p2_m4[,1]],
  Feature2 = colnames(cor_p2_m4)[high_corr_p2_m4[,2]],
  Correlation = cor_p2_m4[high_corr_p2_m4]
)
high_corr_pairs_p2_m4 <- high_corr_pairs_p2_m4 %>% filter(Correlation < 1)

# Inspect the variables with multicollinearity
head(high_corr_pairs_p2_m4)

# Inspect all correlations
head(melt(cor_p2_m4))
```

### Build Model 4
```{r}
# Splitting the dataset into training and testing sets
set.seed(847)  

# COVID Period
X_p2_m4 <- task2_reg_p2_df2 %>% 
  select(Mkt_beta, log_dollar_vol, log_tracking_error, log_turnover_ratio, log_volatility, scaled_abs_corr_sp)

y_p2_m4 <- task2_reg_p2_df2$returns

splitIndex_p2_m4 <- createDataPartition(y_p2_m4, 
                                        p = 0.8, 
                                        list = FALSE)
X_train_data_p2_m4 <- X_p2_m4[splitIndex_p2_m4, ]
X_test_data_p2_m4 <- X_p2_m4[-splitIndex_p2_m4, ]
y_train_data_p2_m4 <- y_p2_m4[splitIndex_p2_m4]
y_test_data_p2_m4 <- y_p2_m4[-splitIndex_p2_m4]

train_data_p2_m4 <- cbind(X_train_data_p2_m4, returns = y_train_data_p2_m4)

# Build Model
model_4_p2 <- lm(returns ~ ., data = train_data_p2_m4)

# Prediction for training sample
pred_train_p2_m4 <- predict(model_4_p2, newdata = X_train_data_p2_m4)

metrics_train_p2_m4 <- postResample(pred = pred_train_p2_m4, 
                                    obs = train_data_p2_m4$returns)
metrics_table_train_p2_m4 <- data.frame(
  Metric = c('MSE', 'RMSE', 'MAE', 'R-squared'),
  Value = c(metrics_train_p2_m4["RMSE"]^2,
            metrics_train_p2_m4["RMSE"], 
            metrics_train_p2_m4["MAE"], 
            metrics_train_p2_m4["Rsquared"])
)

# Prediction for testing sample
pred_test_p2_m4 <- predict(model_4_p2, newdata = X_test_data_p2_m4)
metrics_test_p2_m4 <- postResample(pred = pred_test_p2_m4, 
                                   obs = y_test_data_p2_m4)
metrics_table_test_p2_m4 <- data.frame(
  Metric = c('MSE', 'RMSE', 'MAE', 'R-squared'),
  Value = c(metrics_test_p2_m4["RMSE"]^2
, metrics_test_p2_m4["RMSE"], metrics_test_p2_m4["MAE"], metrics_test_p2_m4["Rsquared"])
)

print("Performance Metrics for the Training Dataset:")
metrics_table_train_p2_m4
print("Performance Metrics for the Testing Dataset:")
metrics_table_test_p2_m4

# Summary of model
summary(model_4_p2)

# To make sure the variable are not multicollinear (VIF < 5)
print(vif(model_4_p2))
```

```{r echo=FALSE}
cat("Performance Metrics for the Training Dataset:\n")
print(metrics_table_train_p2_m4)
cat("Performance Metrics for the Testing Dataset:\n")
print(metrics_table_test_p2_m4)

# Summary of model
cat("Regression coefficient of Model 1:\n")
summary(model_4_p2)

cat("To make sure the variable are not multicollinear (VIF < 5)")
print(vif(model_4_p2))
```

```{r}
# Scatterplot of Prediction vs Actual
plot_data_p2_m4 <- data.frame(
  Predicted = pred_test_p2_m4,
  Actual = y_test_data_p2_m4
)

plot_m4_p2 <- ggplot(data = plot_data_p2_m4,
                     aes(x = Predicted,
                         y = Actual)) +
  geom_point() +
  geom_smooth(method = "lm",
              se = FALSE,
              color = "blue",
              aes(linetype = "Regression Line")) +
  scale_linetype_manual(name = NULL,
                        values = "solid",
                        labels = "Regression Line") +
  theme_minimal() +
  theme(legend.position = "bottom") +
  labs(title = "Evaluating Model 4's Performance: Predicted vs. Actual Returns in the COVID Period",
       x = "Predicted returns",
       y = "Actual returns")

print(plot_m4_p2)
```

### Important variable
```{r}
# Extract coefficients
coef_m4 <- coef(model_4_p2)

importance_m4 <- data.frame(
  Factors = names(coef_m4),
  Coefficient = coef_m4
)
importance_m4 <- importance_m4[importance_m4$Factor != "(Intercept)", ]

# Plot with positive and negative impacts
plot_m4_p2_factors <- ggplot(importance_m4, aes(x = reorder(Factors, Coefficient), y = Coefficient, fill = Coefficient > 0)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(Coefficient, 3)),
            vjust = ifelse(importance_m4$Coefficient > 0, 0.5, 0.3),
            color = "black") +
  coord_flip() +
  labs(title = "Impact of factors on Returns for Model 4",
       x = "Factors",
       y = "Coefficient") +
  theme_minimal() +
  scale_fill_manual(
    values = c("TRUE" = "steelblue", "FALSE" = "coral"),
    labels = c("TRUE" = "Positive Impact", 
               "FALSE" = "Negative Impact"),
    name = ""
  ) +
  theme(
    axis.title.y = element_text(color = 'black'),
    axis.text.y = element_text(color = 'black'),
    legend.position = 'bottom'
  ) 

print(plot_m4_p2_factors)
```

### Group by Sector
```{r}
# Apply the model particularly by industry
pred_sector_p2_m4  <- cbind(X_test_data_p2_m4, SECTOR = task2_reg_p2_df2$SECTOR[-splitIndex_p2_m4])

data_sector_p2_m4 <- data.frame(
  Predicted = pred_test_p2_m4,
  Actual = y_test_data_p2_m4,
  SECTOR = pred_sector_p2_m4$SECTOR
)

data_sector_p2_m4$SECTOR <- as.character(data_sector_p2_m4$SECTOR)

# Calculate evaluation metrics to see the performance of model apply on SECTOR
metrics_table_sector_p2_m4 <- data_sector_p2_m4 %>% 
  group_by(SECTOR) %>% 
  summarise(
    MSE = mean((Actual - Predicted)^2),
    RMSE = sqrt(MSE),
    MAE = mean(abs(Actual - Predicted)),
    residuals = sum((Actual - Predicted)^2),
    total = sum((Actual - mean(Actual))^2),
    R_Squared = 1 - (residuals / total)
  ) %>% 
  select(-c(residuals, total)) 

print(metrics_table_sector_p2_m4)

# Plot the prediction vs actual by SECTOR
plot_m4_p2_sector <- ggplot(data = data_sector_p2_m4, 
                     aes(x = Predicted, 
                         y = Actual,
                         color = SECTOR)) +
  geom_point(size = 2) +
  geom_smooth(method = "lm", 
              se = FALSE, 
              color = "blue", 
              aes(linetype = "Regression Line")) + 
  scale_linetype_manual(name = NULL, 
                        values = "solid", 
                        labels = "Regression Line") + 
  theme_minimal() +
  theme(legend.position = "bottom",
        plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 12),
        strip.text = element_text(size = 12, face = "bold")) +  
  labs(title = "Evaluating Model 4's Performance: Predicted vs. Actual Returns",
       subtitle = "During the COVID Period (Grouped by Industry Sector)",  
       x = "Predicted returns", 
       y = "Actual returns",
       color = "Industry Sector")


print(plot_m4_p2_sector)
```

### Group by Company size
```{r}
# Apply the model particularly by company size
pred_size_p2_m4  <- cbind(X_test_data_p2_m4, company_size = task2_reg_p2_df2$company_size[-splitIndex_p2_m4])

data_size_p2_m4 <- data.frame(
  Predicted = pred_test_p2_m4,
  Actual = y_test_data_p2_m4,
  company_size = pred_size_p2_m4$company_size
)

# Calculate evaluation metrics to see the performance of model apply on company size
metrics_table_size_p2_m4 <- data_size_p2_m4 %>% 
  group_by(company_size) %>% 
  summarise(
    MSE = mean((Actual - Predicted)^2),
    RMSE = sqrt(MSE),
    MAE = mean(abs(Actual - Predicted)),
    residuals = sum((Actual - Predicted)^2),
    total = sum((Actual - mean(Actual))^2),
    R_Squared = 1 - (residuals / total)
  ) %>% 
  select(-c(residuals, total))

print(metrics_table_size_p2_m4)

# Plot the prediction vs actual by company size
plot_m4_p2_size <- ggplot(data = data_size_p2_m4, 
                     aes(x = Predicted, 
                         y = Actual,
                         color = company_size)) +
  geom_point(size = 2, alpha = 0.7) +
  geom_smooth(method = "lm", 
              se = FALSE, 
              color = "blue", 
              aes(linetype = "Regression Line")) + 
  scale_linetype_manual(name = NULL, 
                        values = "solid", 
                        labels = "Regression Line") + 
  theme_minimal() +
  theme(legend.position = "bottom",
        plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 12),
        strip.text = element_text(size = 12, face = "bold")) +  
  labs(title = "Evaluating Model 4's Performance: Predicted vs. Actual Returns",
       subtitle = "During the COVID Period (Grouped by Company Size)",  
       x = "Predicted returns", 
       y = "Actual returns",
       color = "Company Size")


print(plot_m4_p2_size)
```

## Model 5: (Stocks) Returns: 2019-12-14 to 2020-01-20 & X variables: before 2019-02-14
```{r}
# Period 1 - Dataset 3
task2_p1_df3_1 <- transform_dataset_for_task2(task2, "2019-12-14", "2020-01-20", "2019-08-20", "2019-12-14", "stocks", FFF_beta_noncovid) 

# Remove outlier for returns
task2_p1_df3_1 <- remove_outliers_all(task2_p1_df3_1, "returns")

# Inspect the distribution of Returns
print(
  ggplot(task2_p1_df3_1, aes(x = returns)) +
    geom_histogram(bins = 30, fill = "blue", color = "black", alpha = 0.7) +
    theme_minimal() +
    labs(title = "Histogram of Returns", x = "Returns", y = "Frequency")
)

cat("Sample size after remove outlier: ", length(unique(task2_p1_df3_1$PERMNO)))

# Create dummy variable for categorical variable
task2_reg_p1_df3 <- task2_p1_df3_1 %>%
  filter(!is.na(volatility)) %>% 
  mutate(company_size = factor(company_size, levels = unique(task2_p1_df3_1$company_size)),
         SECTOR = factor(SECTOR, levels = unique(task2_p1_df3_1$SECTOR))) %>%
  bind_cols(as_tibble(model.matrix(~ company_size - 1, data = .))) %>%
  bind_cols(as_tibble(model.matrix(~ SECTOR - 1, data = .))) 
```

```{r echo = FALSE}
str(task2_reg_p1_df3)
head(task2_reg_p1_df3, n = 10)
```

### Correlation of variable
```{r}
#Find the correlation between retruns and each potential explanatory variables
cor_p1_m5 <- cor(task2_reg_p1_df3%>% select(-c(PERMNO, SHRCD, TICKER, SECTOR, company_size, security_type)), use = "pairwise.complete.obs")

# Correlation over absolute 0.7 = high risk of multicollinearity
high_corr_p1_m5 <- which(abs(cor_p1_m5) > 0.7, arr.ind = TRUE)
high_corr_pairs_p1_m5 <- data.frame(
  Feature1 = rownames(cor_p1_m5)[high_corr_p1_m5[,1]],
  Feature2 = colnames(cor_p1_m5)[high_corr_p1_m5[,2]],
  Correlation = cor_p1_m5[high_corr_p1_m5]
)
high_corr_pairs_p1_m5 <- high_corr_pairs_p1_m5 %>% filter(Correlation < 1)

# Inspect the variables with multicollinearity
head(high_corr_pairs_p1_m5)

# Inspect all correlations
head(melt(cor_p1_m5))
```

### Build Model 5
```{r}
# Splitting the dataset into training and testing sets
set.seed(847)  

# Non-COVID Period
X_p1_m5 <- task2_reg_p1_df3 %>% 
  select(Mkt_beta, SMB_beta, HML_beta, log_turnover_ratio, log_volatility, vol_cap_ratio, SECTORMaterials, SECTORIndustrials, SECTORFinancials, SECTORInformation_Technology, SECTORConsumer_Discretionary, SECTORHealth_Care, SECTOREnergy, SECTORUtilities, SECTORReal_Estate, SECTORCommunication_Services, SECTORUnknown)

y_p1_m5 <- task2_reg_p1_df3$returns

splitIndex_p1_m5 <- createDataPartition(y_p1_m5, 
                                        p = 0.8, 
                                        list = FALSE)
X_train_data_p1_m5 <- X_p1_m5[splitIndex_p1_m5, ]
X_test_data_p1_m5 <- X_p1_m5[-splitIndex_p1_m5, ]
y_train_data_p1_m5 <- y_p1_m5[splitIndex_p1_m5]
y_test_data_p1_m5 <- y_p1_m5[-splitIndex_p1_m5]

train_data_p1_m5 <- cbind(X_train_data_p1_m5, returns = y_train_data_p1_m5)

# Build Model
model_5_p1 <- lm(returns ~ ., data = train_data_p1_m5)

# Prediction for training sample
pred_train_p1_m5 <- predict(model_5_p1, newdata = X_train_data_p1_m5)

metrics_train_p1_m5 <- postResample(pred = pred_train_p1_m5, 
                                    obs = train_data_p1_m5$returns)
metrics_table_train_p1_m5 <- data.frame(
  Metric = c('MSE', 'RMSE', 'MAE', 'R-squared'),
  Value = c(metrics_train_p1_m5["RMSE"]^2,
            metrics_train_p1_m5["RMSE"], 
            metrics_train_p1_m5["MAE"], 
            metrics_train_p1_m5["Rsquared"])
)

# Prediction for testing sample
pred_test_p1_m5 <- predict(model_5_p1, newdata = X_test_data_p1_m5)
metrics_test_p1_m5 <- postResample(pred = pred_test_p1_m5, 
                                   obs = y_test_data_p1_m5)
metrics_table_test_p1_m5 <- data.frame(
  Metric = c('MSE', 'RMSE', 'MAE', 'R-squared'),
  Value = c(metrics_test_p1_m5["RMSE"]^2
, metrics_test_p1_m5["RMSE"], metrics_test_p1_m5["MAE"], metrics_test_p1_m5["Rsquared"])
)
```

```{r echo=FALSE}
cat("Performance Metrics for the Training Dataset:\n")
print(metrics_table_train_p1_m5)
cat("Performance Metrics for the Testing Dataset:\n")
print(metrics_table_test_p1_m5)

# Summary of model
cat("Regression coefficient of Model 1:\n")
summary(model_5_p1)

cat("To make sure the variable are not multicollinear (VIF < 5)")
print(vif(model_5_p1))
cat("Despite SectorUnknown has a VIF over 5, considering it is actually the group of data with missing value in Sector, while some other industry categories are important to the model. After trial and error, it is better to keep the industry variable in the model.")
```

```{r}
# Scatterplot of Prediction vs Actual
plot_data_p1_m5 <- data.frame(
  Predicted = pred_test_p1_m5,
  Actual = y_test_data_p1_m5
)

plot_m5_p1 <- ggplot(data = plot_data_p1_m5,
                     aes(x = Predicted,
                         y = Actual)) +
  geom_point() +
  geom_smooth(method = "lm",
              se = FALSE,
              color = "blue",
              aes(linetype = "Regression Line")) +
  scale_linetype_manual(name = NULL,
                        values = "solid",
                        labels = "Regression Line") +
  theme_minimal() +
  theme(legend.position = "bottom") +
  labs(title = "Evaluating Model 5's Performance: Predicted vs. Actual Returns in the Non-COVID Period",
       x = "Predicted returns",
       y = "Actual returns")

print(plot_m5_p1)
```

### Important variable
```{r}
# Extract coefficients
coef_m5 <- coef(model_5_p1)

importance_m5 <- data.frame(
  Factors = names(coef_m5),
  Coefficient = coef_m5
)
importance_m5 <- importance_m5[importance_m5$Factor != "(Intercept)", ]

# Plot with positive and negative impacts
plot_m5_p1_factors <- ggplot(importance_m5, aes(x = reorder(Factors, Coefficient), y = Coefficient, fill = Coefficient > 0)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(Coefficient, 3)),
            vjust = ifelse(importance_m5$Coefficient > 0, 0.5, 0.3),
            color = "black") +
  coord_flip() +
  labs(title = "Impact of factors on Returns for Model 5",
       x = "Factors",
       y = "Coefficient") +
  theme_minimal() +
  scale_fill_manual(
    values = c("TRUE" = "steelblue", "FALSE" = "coral"),
    labels = c("TRUE" = "Positive Impact", 
               "FALSE" = "Negative Impact"),
    name = ""
  ) +
  theme(
    axis.title.y = element_text(color = 'black'),
    axis.text.y = element_text(color = 'black'),
    legend.position = 'bottom'
  ) 

print(plot_m5_p1_factors)
```

### Group by Sector
```{r}
# Apply the model particularly by industry
pred_sector_p1_m5  <- cbind(X_test_data_p1_m5, SECTOR = task2_reg_p1_df3$SECTOR[-splitIndex_p1_m5])

data_sector_p1_m5 <- data.frame(
  Predicted = pred_test_p1_m5,
  Actual = y_test_data_p1_m5,
  SECTOR = pred_sector_p1_m5$SECTOR
)

data_sector_p1_m5$SECTOR <- as.character(data_sector_p1_m5$SECTOR)

# Calculate evaluation metrics to see the performance of model apply on SECTOR
metrics_table_sector_p1_m5 <- data_sector_p1_m5 %>% 
  group_by(SECTOR) %>% 
  summarise(
    MSE = mean((Actual - Predicted)^2),
    RMSE = sqrt(MSE),
    MAE = mean(abs(Actual - Predicted)),
    residuals = sum((Actual - Predicted)^2),
    total = sum((Actual - mean(Actual))^2),
    R_Squared = 1 - (residuals / total)
  ) %>% 
  select(-c(residuals, total)) 

print(metrics_table_sector_p1_m5)

# Plot the prediction vs actual by SECTOR
plot_m5_p1_sector <- ggplot(data = data_sector_p1_m5, 
                     aes(x = Predicted, 
                         y = Actual,
                         color = SECTOR)) +
  geom_point(size = 2) +
  geom_smooth(method = "lm", 
              se = FALSE, 
              color = "blue", 
              aes(linetype = "Regression Line")) + 
  scale_linetype_manual(name = NULL, 
                        values = "solid", 
                        labels = "Regression Line") + 
  theme_minimal() +
  theme(legend.position = "bottom",
        plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 12),
        strip.text = element_text(size = 12, face = "bold")) +  
  labs(title = "Evaluating Model 5's Performance: Predicted vs. Actual Returns",
       subtitle = "During the Non-COVID Period (Grouped by Industry Sector)",  
       x = "Predicted returns", 
       y = "Actual returns",
       color = "Industry Sector")


print(plot_m5_p1_sector)
```

### Group by Company size
```{r}
# Apply the model particularly by company size
pred_size_p1_m5  <- cbind(X_test_data_p1_m5, company_size = task2_reg_p1_df3$company_size[-splitIndex_p1_m5])

data_size_p1_m5 <- data.frame(
  Predicted = pred_test_p1_m5,
  Actual = y_test_data_p1_m5,
  company_size = pred_size_p1_m5$company_size
)

# Calculate evaluation metrics to see the performance of model apply on company size
metrics_table_size_p1_m5 <- data_size_p1_m5 %>% 
  group_by(company_size) %>% 
  summarise(
    MSE = mean((Actual - Predicted)^2),
    RMSE = sqrt(MSE),
    MAE = mean(abs(Actual - Predicted)),
    residuals = sum((Actual - Predicted)^2),
    total = sum((Actual - mean(Actual))^2),
    R_Squared = 1 - (residuals / total)
  ) %>% 
  select(-c(residuals, total))

print(metrics_table_size_p1_m5)

# Plot the prediction vs actual by company size
plot_m5_p1_size <- ggplot(data = data_size_p1_m5, 
                     aes(x = Predicted, 
                         y = Actual,
                         color = company_size)) +
  geom_point(size = 2, alpha = 0.7) +
  geom_smooth(method = "lm", 
              se = FALSE, 
              color = "blue", 
              aes(linetype = "Regression Line")) + 
  scale_linetype_manual(name = NULL, 
                        values = "solid", 
                        labels = "Regression Line") + 
  theme_minimal() +
  theme(legend.position = "bottom",
        plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 12),
        strip.text = element_text(size = 12, face = "bold")) +  
  labs(title = "Evaluating Model 5's Performance: Predicted vs. Actual Returns",
       subtitle = "During the Non-COVID Period (Grouped by Company Size)",  
       x = "Predicted returns", 
       y = "Actual returns",
       color = "Company Size")


print(plot_m5_p1_size)
```

## Model 6: (Stock) Returns: 2020-02-14 to 2020-03-20 & X variables: before 2019-02-14
```{r}
# Period 2 - Dataset 3
task2_p2_df3_1 <- transform_dataset_for_task2(task2, "2020-02-14", "2020-03-20", "2019-08-20", "2020-02-14", "stocks", FFF_beta_covid)

# Remove outlier for returns
task2_p2_df3_1 <- remove_outliers_all(task2_p2_df3_1, "returns")

# Inspect the distribution of Returns
print(
  ggplot(task2_p2_df3_1, aes(x = returns)) +
    geom_histogram(bins = 30, fill = "blue", color = "black", alpha = 0.7) +
    theme_minimal() +
    labs(title = "Histogram of Returns", x = "Returns", y = "Frequency")
)

cat("Sample size after remove outlier: ", length(unique(task2_p2_df3_1$PERMNO)))

# Create dummy variable for categorical variable
task2_reg_p2_df3 <- task2_p2_df3_1 %>%
  filter(!is.na(volatility)) %>% 
  mutate(company_size = factor(company_size, levels = unique(task2_p2_df3_1$company_size)),
         SECTOR = factor(SECTOR, levels = unique(task2_p2_df3_1$SECTOR))) %>%
  bind_cols(as_tibble(model.matrix(~ company_size - 1, data = .))) %>%
  bind_cols(as_tibble(model.matrix(~ SECTOR - 1, data = .))) 

# Inspect any missing value in important variables
colSums(is.na(task2_reg_p2_df3))

print(task2_reg_p2_df3, n = 10)
```

```{r echo = FALSE}
str(task2_reg_p2_df3)
head(task2_reg_p2_df3, n = 10)
```

### Correlation of variable
```{r}
#Find the correlation between retruns and each potential explanatory variables
cor_p2_m6 <- cor(task2_reg_p2_df2%>% select(-c(PERMNO, SHRCD, TICKER, SECTOR, company_size, security_type)), use = "pairwise.complete.obs")

# Correlation over absolute 0.7 = high risk of multicollinearity
high_corr_p2_m6 <- which(abs(cor_p2_m6) > 0.7, arr.ind = TRUE)
high_corr_pairs_p2_m6 <- data.frame(
  Feature1 = rownames(cor_p2_m6)[high_corr_p2_m6[,1]],
  Feature2 = colnames(cor_p2_m6)[high_corr_p2_m6[,2]],
  Correlation = cor_p2_m6[high_corr_p2_m6]
)
high_corr_pairs_p2_m6 <- high_corr_pairs_p2_m6 %>% filter(Correlation < 1)

# Inspect the variables with multicollinearity
head(high_corr_pairs_p2_m6)

# Inspect all correlations
head(melt(cor_p2_m6))
```

### Build Model 6
```{r}
# Splitting the dataset into training and testing sets
set.seed(847)  

# COVID Period
X_p2_m6 <- task2_reg_p2_df3 %>% 
  select(Mkt_beta, SMB_beta, HML_beta, log_volatility, log_vol_cap_ratio, scaled_abs_corr_sp, company_sizeSmall, company_sizeLarge, SECTORMaterials, SECTORIndustrials, SECTORFinancials, SECTORInformation_Technology, SECTORConsumer_Discretionary, SECTORHealth_Care, SECTOREnergy, SECTORUtilities, SECTORReal_Estate, SECTORCommunication_Services, SECTORUnknown)

y_p2_m6 <- task2_reg_p2_df3$returns

splitIndex_p2_m6 <- createDataPartition(y_p2_m6, 
                                        p = 0.8, 
                                        list = FALSE)
X_train_data_p2_m6 <- X_p2_m6[splitIndex_p2_m6, ]
X_test_data_p2_m6 <- X_p2_m6[-splitIndex_p2_m6, ]
y_train_data_p2_m6 <- y_p2_m6[splitIndex_p2_m6]
y_test_data_p2_m6 <- y_p2_m6[-splitIndex_p2_m6]

train_data_p2_m6 <- cbind(X_train_data_p2_m6, returns = y_train_data_p2_m6)

# Build Model
model_6_p2 <- lm(returns ~ ., data = train_data_p2_m6)

# Prediction for training sample
pred_train_p2_m6 <- predict(model_6_p2, newdata = X_train_data_p2_m6)

metrics_train_p2_m6 <- postResample(pred = pred_train_p2_m6, 
                                    obs = train_data_p2_m6$returns)
metrics_table_train_p2_m6 <- data.frame(
  Metric = c('MSE', 'RMSE', 'MAE', 'R-squared'),
  Value = c(metrics_train_p2_m6["RMSE"]^2,
            metrics_train_p2_m6["RMSE"], 
            metrics_train_p2_m6["MAE"], 
            metrics_train_p2_m6["Rsquared"])
)

# Prediction for testing sample
pred_test_p2_m6 <- predict(model_6_p2, newdata = X_test_data_p2_m6)
metrics_test_p2_m6 <- postResample(pred = pred_test_p2_m6, 
                                   obs = y_test_data_p2_m6)
metrics_table_test_p2_m6 <- data.frame(
  Metric = c('MSE', 'RMSE', 'MAE', 'R-squared'),
  Value = c(metrics_test_p2_m6["RMSE"]^2
, metrics_test_p2_m6["RMSE"], metrics_test_p2_m6["MAE"], metrics_test_p2_m6["Rsquared"])
)
```

```{r echo=FALSE}
cat("Performance Metrics for the Training Dataset:\n")
print(metrics_table_train_p2_m6)
cat("Performance Metrics for the Testing Dataset:\n")
print(metrics_table_test_p2_m6)

# Summary of model
cat("Regression coefficient of Model 1:\n")
summary(model_6_p2)

cat("To make sure the variable are not multicollinear (VIF < 5)")
print(vif(model_6_p2))
cat("Despite SectorUnknown has a VIF over 5, considering it is actually the group of data with missing value in Sector, while some other industry categories are important to the model. After trial and error, it is better to keep the industry variable in the model.")
```

```{r}
# Scatterplot of Prediction vs Actual
plot_data_p2_m6 <- data.frame(
  Predicted = pred_test_p2_m6,
  Actual = y_test_data_p2_m6
)

plot_m6_p2 <- ggplot(data = plot_data_p2_m6,
                     aes(x = Predicted,
                         y = Actual)) +
  geom_point() +
  geom_smooth(method = "lm",
              se = FALSE,
              color = "blue",
              aes(linetype = "Regression Line")) +
  scale_linetype_manual(name = NULL,
                        values = "solid",
                        labels = "Regression Line") +
  theme_minimal() +
  theme(legend.position = "bottom") +
  labs(title = "Evaluating Model 6's Performance: Predicted vs. Actual Returns in the COVID Period",
       x = "Predicted returns",
       y = "Actual returns")

print(plot_m6_p2)
```

### Important variable
```{r}
# Extract coefficients
coef_m6 <- coef(model_6_p2)

importance_m6 <- data.frame(
  Factors = names(coef_m6),
  Coefficient = coef_m6
)
importance_m6 <- importance_m6[importance_m6$Factor != "(Intercept)", ]

# Plot with positive and negative impacts
plot_m6_p2_factors <- ggplot(importance_m6, aes(x = reorder(Factors, Coefficient), y = Coefficient, fill = Coefficient > 0)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(Coefficient, 3)),
            vjust = ifelse(importance_m6$Coefficient > 0, 0.5, 0.3),
            color = "black") +
  coord_flip() +
  labs(title = "Impact of factors on Returns for Model 6",
       x = "Factors",
       y = "Coefficient") +
  theme_minimal() +
  scale_fill_manual(
    values = c("TRUE" = "steelblue", "FALSE" = "coral"),
    labels = c("TRUE" = "Positive Impact", 
               "FALSE" = "Negative Impact"),
    name = ""
  ) +
  theme(
    axis.title.y = element_text(color = 'black'),
    axis.text.y = element_text(color = 'black'),
    legend.position = 'bottom'
  ) 

print(plot_m6_p2_factors)
```

### Group by Sector
```{r}
# Apply the model particularly by industry
pred_sector_p2_m6  <- cbind(X_test_data_p2_m6, SECTOR = task2_reg_p2_df3$SECTOR[-splitIndex_p2_m6])

data_sector_p2_m6 <- data.frame(
  Predicted = pred_test_p2_m6,
  Actual = y_test_data_p2_m6,
  SECTOR = pred_sector_p2_m6$SECTOR
)

data_sector_p2_m6$SECTOR <- as.character(data_sector_p2_m6$SECTOR)

# Calculate evaluation metrics to see the performance of model apply on SECTOR
metrics_table_sector_p2_m6 <- data_sector_p2_m6 %>% 
  group_by(SECTOR) %>% 
  summarise(
    MSE = mean((Actual - Predicted)^2),
    RMSE = sqrt(MSE),
    MAE = mean(abs(Actual - Predicted)),
    residuals = sum((Actual - Predicted)^2),
    total = sum((Actual - mean(Actual))^2),
    R_Squared = 1 - (residuals / total)
  ) %>% 
  select(-c(residuals, total)) 

print(metrics_table_sector_p2_m6)

# Plot the prediction vs actual by SECTOR
plot_m6_p2_sector <- ggplot(data = data_sector_p2_m6, 
                     aes(x = Predicted, 
                         y = Actual,
                         color = SECTOR)) +
  geom_point(size = 2) +
  geom_smooth(method = "lm", 
              se = FALSE, 
              color = "blue", 
              aes(linetype = "Regression Line")) + 
  scale_linetype_manual(name = NULL, 
                        values = "solid", 
                        labels = "Regression Line") + 
  theme_minimal() +
  theme(legend.position = "bottom",
        plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 12),
        strip.text = element_text(size = 12, face = "bold")) +  
  labs(title = "Evaluating Model 6's Performance: Predicted vs. Actual Returns",
       subtitle = "During the COVID Period (Grouped by Industry Sector)",  
       x = "Predicted returns", 
       y = "Actual returns",
       color = "Industry Sector")


print(plot_m6_p2_sector)
```

### Group by Company size
```{r}
# Apply the model particularly by company size
pred_size_p2_m6  <- cbind(X_test_data_p2_m6, company_size = task2_reg_p2_df3$company_size[-splitIndex_p2_m6])

data_size_p2_m6 <- data.frame(
  Predicted = pred_test_p2_m6,
  Actual = y_test_data_p2_m6,
  company_size = pred_size_p2_m6$company_size
)

# Calculate evaluation metrics to see the performance of model apply on company size
metrics_table_size_p2_m6 <- data_size_p2_m6 %>% 
  group_by(company_size) %>% 
  summarise(
    MSE = mean((Actual - Predicted)^2),
    RMSE = sqrt(MSE),
    MAE = mean(abs(Actual - Predicted)),
    residuals = sum((Actual - Predicted)^2),
    total = sum((Actual - mean(Actual))^2),
    R_Squared = 1 - (residuals / total)
  ) %>% 
  select(-c(residuals, total))

print(metrics_table_size_p2_m6)

# Plot the prediction vs actual by company size
plot_m6_p2_size <- ggplot(data = data_size_p2_m6, 
                     aes(x = Predicted, 
                         y = Actual,
                         color = company_size)) +
  geom_point(size = 2, alpha = 0.7) +
  geom_smooth(method = "lm", 
              se = FALSE, 
              color = "blue", 
              aes(linetype = "Regression Line")) + 
  scale_linetype_manual(name = NULL, 
                        values = "solid", 
                        labels = "Regression Line") + 
  theme_minimal() +
  theme(legend.position = "bottom",
        plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 12),
        strip.text = element_text(size = 12, face = "bold")) +  
  labs(title = "Evaluating Model 6's Performance: Predicted vs. Actual Returns",
       subtitle = "During the COVID Period (Grouped by Company Size)",  
       x = "Predicted returns", 
       y = "Actual returns",
       color = "Company Size")


print(plot_m6_p2_size)
```

## Different variable in each model
```{r}
get_variables <- function(model) {
  # Extract variables from the model
  terms <- attr(model$terms, "term.labels")
  return(terms)
}

ols_m1_v <- get_variables(model_1_p1)
ols_m2_v <- get_variables(model_2_p2)
ols_m3_v <- get_variables(model_3_p1)
ols_m4_v <- get_variables(model_4_p2)
ols_m5_v <- get_variables(model_5_p1)
ols_m6_v <- get_variables(model_6_p2)

ols_v_list <- list(
  Model_1 = as.character(ols_m1_v),
  Model_2 = as.character(ols_m2_v),
  Model_3 = as.character(ols_m3_v),
  Model_4 = as.character(ols_m4_v),
  Model_5 = as.character(ols_m5_v),
  Model_6 = as.character(ols_m6_v)
)

# Initialize empty data frame
ols_model_data <- data.frame(
  model = character(),
  variable = character(),
  order = integer(),  
  stringsAsFactors = FALSE
)

# Populate the dataframe with the original order of variables
for (model in names(ols_v_list)) {
  variables <- ols_v_list[[model]]  
  temp_df <- data.frame(
    model = model,
    variable = variables,
    order = seq_along(variables), 
    stringsAsFactors = FALSE
  )
  ols_model_data <- rbind(ols_model_data, temp_df)
}

# Convert variable to a factor with levels ordered by 'order'
ols_model_data$variable <- factor(ols_model_data$variable, levels = sort(unique(ols_model_data$variable)))

# Generate plot
ols_models_v <- ggplot(ols_model_data, aes(x = model, y = variable, fill = model)) +
  geom_tile(color = "white") + 
  labs(
    title = "Variables Included in Each OLS Regression Model",
    x = "Model",
    y = "Variables"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 12),  
    axis.text.y = element_text(size = 12),  
    axis.title.x = element_text(size = 14), 
    axis.title.y = element_text(size = 14), 
    plot.title = element_text(hjust = 0.5, size = 16), 
    plot.title.position = "plot")

print(ols_models_v)

# Save plot for report
save_plot(ols_models_v, "ols_models_v")
```


#   3 Logistic Regressions

## Preparation
```{r}
# load library
library(stats)
library(pROC)
library(PRROC)
```

### Create price change dummy variable
```{r}
# Use the last and the first date PRC between COVID to create dummy variable represent price increase (1) or decrease (0)
task3_price_dummy <- dataset1_df9 %>% 
  arrange(PERMNO, date) %>% 
  group_by(PERMNO) %>%
  summarise(
    up_PRC = ifelse(last(na.omit(PRC[date <= "2020-03-20"])) > first(na.omit(PRC[date >= "2020-02-14"])), 1, 0))
```

### Inspect up_PRC accuracy
```{r}
up_PRC_summary <- dataset1_df9 %>% 
  filter(date >= "2020-02-14" & date <= "2020-03-20") %>% 
  arrange(PERMNO, date) %>% 
  group_by(PERMNO) %>%
  summarize(
    first_PRC = first(na.omit(PRC)),
    first_date = date[which(na.omit(PRC) == first_PRC)[1]],
    last_PRC = last(na.omit(PRC)),
    last_date = date[which(na.omit(PRC) == last_PRC)[1]],
    .groups = 'drop'
  ) %>%
  mutate(
    up_PRC = ifelse(last_PRC > first_PRC, 1, 0)
  )

up_PRC_check <- task3_price_dummy %>%
  distinct(PERMNO, up_PRC) %>%
  left_join(up_PRC_summary %>% select(PERMNO, up_PRC), by = "PERMNO", suffix = c("_dataset1", "_dataset2")) %>%
  filter(up_PRC_dataset1 != up_PRC_dataset2) %>%
  select(PERMNO, up_PRC_dataset1, up_PRC_dataset2)

if (nrow(up_PRC_check) > 0) {
  print("These PERMNO has wrong up_PRC")
  print(up_PRC_check$PERMNO)
} else {
  print("All values of up_PRC are checked and accurate.")
}
```

### Create dataset
```{r}
task3 <- task2_reg_p2_df1 %>% 
  left_join(task3_price_dummy, by = "PERMNO")

head(task3, n = 5)
```

## Model 1: Price Change dummy variable: 2020-02-14 to 2020-03-20 & X variable: before 2020-02-14

### Build Model 1 (Without industry)
```{r}
# Select independent variable
X_logit_m1 <- task3 %>%
  select(Mkt_beta, SMB_beta, HML_beta, log_spread, log_volatility, scaled_market_cap, log_tracking_error, log_turnover_ratio)

y_logit_m1 <- task3$up_PRC

# Split the dataset into training and testing sets
set.seed(675)  

trainIndex <- createDataPartition(y_logit_m1, p = 0.8, list = FALSE)
X_train_logit_m1 <- X_logit_m1[trainIndex, ]
X_test_logit_m1 <- X_logit_m1[-trainIndex, ]
y_train_logit_m1 <- y_logit_m1[trainIndex]
y_test_logit_m1 <- y_logit_m1[-trainIndex]


# Run the logistic regression model
logit_model_1 <- glm(y_train_logit_m1 ~ ., 
                     data = as.data.frame(X_train_logit_m1, y_train_logit_m1),
                     family = binomial())

# Display model summary
summary(logit_model_1)

# Make sure no multicollinearity
print(vif(logit_model_1))

# Calculate predicted probabilities on testing data
pred_prob_m1 <- predict(logit_model_1, 
                        newdata = as.data.frame(X_test_logit_m1), 
                        type = "response")

# Determine classification based on threshold of 0.5
class_m1 <- ifelse(pred_prob_m1 > 0.5, 1, 0)

# Calculate True Positives, True Negatives, False Positives, False Negatives
true_positives_m1 <- sum(y_test_logit_m1 == 1 & class_m1 == 1)
true_negatives_m1 <- sum(y_test_logit_m1 == 0 & class_m1 == 0)
false_positives_m1 <- sum(y_test_logit_m1 == 0 & class_m1 == 1)
false_negatives_m1 <- sum(y_test_logit_m1 == 1 & class_m1 == 0)

# Calculate Accuracy
accuracy_m1 <- (true_positives_m1 + true_negatives_m1) / (true_positives_m1 + true_negatives_m1 + false_positives_m1 + false_negatives_m1)

# Calculate Precision
precision_m1 <- ifelse((true_positives_m1 + false_positives_m1) == 0, 0, true_positives_m1 / (true_positives_m1 + false_positives_m1))

# Calculate Recall
recall_m1 <- ifelse((true_positives_m1 + false_negatives_m1) == 0, 0, true_positives_m1 / (true_positives_m1 + false_negatives_m1))

# Calculate F1 Score
f1_m1 <- 2 * (precision_m1 * recall_m1) / (precision_m1 + recall_m1)

# Compute ROC curve and AUC
roc_curve_m1 <- roc(y_test_logit_m1, pred_prob_m1)
auc_value_m1 <- auc(roc_curve_m1)

# Calculate McFadden’s R-squared
null_model_1 <- glm(y_train_logit_m1 ~ 1, data = as.data.frame(cbind(X_train_logit_m1, y_train_logit_m1)), family = binomial())
rsquared_logit_m1 <- 1 - (logLik(logit_model_1) / logLik(null_model_1))

# Create a dataframe for counts
metrics_counts_logit_m1 <- data.frame(
  Metric = c('True Positives', 'True Negatives', 'False Positives', 'False Negatives'),
  Count = c(
    as.integer(true_positives_m1), 
    as.integer(true_negatives_m1), 
    as.integer(false_positives_m1), 
    as.integer(false_negatives_m1)
  )
)

# Create a dataframe for performance metrics
metrics_evaluation_logit_m1 <- data.frame(
  Metric = c('Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC', 'R_squared'),
  Value = c(
    accuracy_m1, 
    precision_m1, 
    recall_m1,
    f1_m1,
    auc_value_m1,
    rsquared_logit_m1
  )
)

# Print the results
print(metrics_counts_logit_m1)

print(metrics_evaluation_logit_m1)
```

### Confusion Matrix
```{r}
true_false_m1 <- matrix(c(true_negatives_m1, false_positives_m1, false_negatives_m1, true_positives_m1),
                        nrow = 2,
                        dimnames = list(Predicted = c("Negative", "Positive"),
                                        Actual = c("Negative", "Positive")))

conf_matrix_m1 <- as.data.frame(as.table(true_false_m1))

# Plot the confusion matrix
palette <- brewer.pal(n = 4, 
                      name = "Greens")  # Define color

conf_matrix_m1_plot <- ggplot(conf_matrix_m1, 
                              aes(x = Predicted, 
                                  y = Actual, 
                                  fill = Freq)) +
  
  geom_tile(color = "black") + 
  
  geom_text(aes(label = Freq), 
            color = "black", 
            size = 6) +
  
  scale_fill_gradientn(colors = brewer.pal(n = 4, 
                                           name = "Greens"), 
                       guide = "none") +
  
  labs(title = "Confusion Matrix  for Model 1: Price Increase Predictions between February 14, 2020 to March 20, 2020",
       x = "Predicted",
       y = "Actual") +
  
  theme_minimal() +
  
  theme(
      plot.title = element_text(size = 16, 
                                face = "bold", 
                                hjust = 0.5),
      axis.title = element_text(size = 14),
      axis.text = element_text(size = 12)
    )

print(conf_matrix_m1_plot)  

# Save the plot for report
save_plot(conf_matrix_m1_plot, "Confusion Matrix for logit model 1")
```

### Important Factor
```{r}
# Extract coefficients
coefficients_logit_m1 <- coef(logit_model_1)

# Convert coefficients to odds ratios
odds_ratios_m1 <- exp(coefficients_logit_m1)

# Identify intercept and filter it out
importance_logit_m1 <- data.frame(
  Factors = names(coefficients_logit_m1),
  Odds_Ratio = odds_ratios_m1
)

# Exclude the intercept (assuming it's named 'Intercept')
importance_logit_m1 <- importance_logit_m1[importance_logit_m1$Factor != "(Intercept)", ]

# Add a column for fill color based on the odds ratio
importance_logit_m1$Fill_Color <- ifelse(importance_logit_m1$Odds_Ratio > 1, "steelblue", "coral")

# Plot the factor
ggplot(importance_logit_m1, 
       aes(x = reorder(Factors, Odds_Ratio), 
           y = Odds_Ratio, 
           fill = Fill_Color)) +
  
  geom_bar(stat = "identity") +
  
  coord_flip() +
  
  geom_text(aes(label = round(Odds_Ratio, 2)), 
            hjust = 1, 
            size = 3) + 
  
  labs(title = "Impact of Factors on Price increase likelihood for Model 1: Positive Vs Negative Associations",
       x = "Factors",
       y = "Odds Ratio\n(The exponentiation of coefficients)") +
  
  geom_hline(yintercept = 1, 
             linetype = "dashed", 
             color = "red") +
  
  annotate("text", 
           x = Inf, 
           y = 1, 
           label = "Odds Ratio = 1", 
           hjust = 0, 
           vjust = 0, 
           color = "grey", 
           size = 3.5, 
           fontface = "italic") +
  
  theme_minimal() +
  
  scale_fill_manual(
    values = c("steelblue" = "steelblue", "coral" = "coral"),
    labels = c("steelblue" = "Positively Associated with Price Increase", 
               "coral" = "Negatively Associated with Price Increase"),
    name = "") +
  
  theme(
    plot.title = element_text(size = 16, 
                              face = "bold", 
                              hjust = 0.5),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    axis.title.y = element_text(color = 'black'),
    axis.text.y = element_text(color = 'black'),
    legend.position = 'bottom'
  ) 
```

## Model 2: Continuous Variables from Model 1 with Industry Variable
```{r}
# Select independent variable
X_logit_m2 <- task3 %>%
  select(Mkt_beta, SMB_beta, HML_beta, log_spread, log_volatility, scaled_market_cap, log_tracking_error, log_turnover_ratio, SECTORMaterials, SECTORIndustrials, SECTORFinancials, SECTORInformation_Technology, SECTORConsumer_Discretionary, SECTORHealth_Care, SECTOREnergy, SECTORUtilities, SECTORReal_Estate, SECTORCommunication_Services, SECTORUnknown
)

y_logit_m2 <- task3$up_PRC  

# Split the dataset into training and testing sets
set.seed(675) 

trainIndex <- createDataPartition(y_logit_m2, p = 0.8, list = FALSE)
X_train_logit_m2 <- X_logit_m2[trainIndex, ]
X_test_logit_m2 <- X_logit_m2[-trainIndex, ]
y_train_logit_m2 <- y_logit_m2[trainIndex]
y_test_logit_m2 <- y_logit_m2[-trainIndex]


# Fit the logistic regression model
logit_model_2 <- glm(y_train_logit_m2 ~ ., 
                     data = as.data.frame(X_train_logit_m2, y_train_logit_m2),
                     family = binomial())

# Display model summary
summary(logit_model_2)

# Ensure no multicollinearity
print(vif(logit_model_2))

# Calculate predicted probabilities on testing data
pred_prob_m2 <- predict(logit_model_2, 
                        newdata = as.data.frame(X_test_logit_m2), 
                        type = "response")

# Determine classification based on threshold of 0.5
class_m2 <- ifelse(pred_prob_m2 > 0.5, 1, 0)

# Calculate True Positives, True Negatives, False Positives, False Negatives
true_positives_m2 <- sum(y_test_logit_m2 == 1 & class_m2 == 1)
true_negatives_m2 <- sum(y_test_logit_m2 == 0 & class_m2 == 0)
false_positives_m2 <- sum(y_test_logit_m2 == 0 & class_m2 == 1)
false_negatives_m2 <- sum(y_test_logit_m2 == 1 & class_m2 == 0)

# Calculate Accuracy
accuracy_m2 <- (true_positives_m2 + true_negatives_m2) / (true_positives_m2 + true_negatives_m2 + false_positives_m2 + false_negatives_m2)

# Calculate Precision
precision_m2 <- ifelse((true_positives_m2 + false_positives_m2) == 0, 0, true_positives_m2 / (true_positives_m2 + false_positives_m2))

# Calculate Recall
recall_m2 <- ifelse((true_positives_m2 + false_negatives_m2) == 0, 0, true_positives_m2 / (true_positives_m2 + false_negatives_m2))

# Calculate F1 Score
f1_m2 <- 2 * (precision_m2 * recall_m2) / (precision_m2 + recall_m2)

# Compute ROC curve and AUC
roc_curve_m2 <- roc(y_test_logit_m2, pred_prob_m2)
auc_value_m2 <- auc(roc_curve_m2)

# Calculate McFadden’s R-squared
null_model_2 <- glm(y_train_logit_m2 ~ 1, data = as.data.frame(cbind(X_train_logit_m2, y_train_logit_m2)), family = binomial())
rsquared_logit_m2 <- 1 - (logLik(logit_model_2) / logLik(null_model_2))

# Create a dataframe for counts
metrics_counts_logit_m2 <- data.frame(
  Metric = c('True Positives', 'True Negatives', 'False Positives', 'False Negatives'),
  Count = c(
    as.integer(true_positives_m2), 
    as.integer(true_negatives_m2), 
    as.integer(false_positives_m2), 
    as.integer(false_negatives_m2)
  )
)

# Create a dataframe for performance metrics
metrics_evaluation_logit_m2 <- data.frame(
  Metric = c('Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC', 'R_squared'),
  Value = c(
    accuracy_m2, 
    precision_m2, 
    recall_m2,
    f1_m2,
    auc_value_m2,
    rsquared_logit_m2
  )
)

# Print the results 
print(metrics_counts_logit_m2)

print(metrics_evaluation_logit_m2)
```

### Confusion Matrix
```{r}
true_false_m2 <- matrix(c(true_negatives_m2, false_positives_m2, false_negatives_m2, true_positives_m2),
                        nrow = 2,
                        dimnames = list(Predicted = c("Negative", "Positive"),
                                        Actual = c("Negative", "Positive")))
											 
conf_matrix_m2 <- as.data.frame(as.table(true_false_m2))

# Plot the confusion matrix
palette <- brewer.pal(n = 4, 
                      name = "Greens")  # Define color

conf_matrix_m2_plot <- ggplot(conf_matrix_m2, 
                              aes(x = Predicted, 
                                  y = Actual, 
                                  fill = Freq)) +
  
  geom_tile(color = "black") + 
  
  geom_text(aes(label = Freq), 
            color = "black", 
            size = 6) +
  
  scale_fill_gradientn(colors = brewer.pal(n = 4, 
                                           name = "Greens"), 
                       guide = "none") +
  
  labs(title = "Confusion Matrix for Model 2: Price Increase Predictions between February 14, 2020 to March 20, 2020",
       subtitle = "(Model 2: Continuous Variables from Model 1 with Industry Variable)",
       x = "Predicted", 
       y = "Actual") +
  
  theme_minimal() +
  
  theme(
      plot.title = element_text(size = 16, 
                                face = "bold", 
                                hjust = 0.5),
      axis.title = element_text(size = 14),
      axis.text = element_text(size = 12)
    )

print(conf_matrix_m2_plot)  

# Save the plot for report
save_plot(conf_matrix_m2_plot, "Confusion Matrix for logit Model 2")
```

### Important Factor
```{r}
# Extract coefficients
coefficients_logit_m2 <- coef(logit_model_2)

# Convert coefficients to odds ratios
odds_ratios_m2 <- exp(coefficients_logit_m2)

# Identify intercept and filter it out
importance_logit_m2 <- data.frame(
  Factors = names(coefficients_logit_m2),
  Odds_Ratio = odds_ratios_m2
)

# Exclude the intercept (assuming it's named 'Intercept')
importance_logit_m2 <- importance_logit_m2[importance_logit_m2$Factor != "(Intercept)", ]

# Add a column for fill color based on the odds ratio
importance_logit_m2$Fill_Color <- ifelse(importance_logit_m2$Odds_Ratio > 1, "steelblue", "coral")

# Plot the factor
ggplot(importance_logit_m2, 
       aes(x = reorder(Factors, Odds_Ratio), 
           y = Odds_Ratio, 
           fill = Fill_Color)) +
  
  geom_bar(stat = "identity") +
  
  coord_flip() +
  
  geom_text(aes(label = round(Odds_Ratio, 2)), 
            hjust = 1, 
            size = 3) + 
  
  labs(title = "Impact of Factors on Price increase likelihood for Model 2: Positive Vs Negative Associations",
       subtitle = "(Model 2: Continuous Variables from Model 1 with Industry Variable)",
       x = "Factors",
       y = "Odds Ratio\n(The exponentiation of coefficients)") +
  
  geom_hline(yintercept = 1, 
             linetype = "dashed", 
             color = "red") +
  
  annotate("text", 
           x = Inf, 
           y = 1, 
           label = "Odds Ratio = 1", 
           hjust = 0, 
           vjust = 0, 
           color = "grey", 
           size = 3.5, 
           fontface = "italic") +
  
  theme_minimal() +
  
  scale_fill_manual(
    values = c("steelblue" = "steelblue", "coral" = "coral"),
    labels = c("steelblue" = "Positively Associated with Price Increase", 
               "coral" = "Negatively Associated with Price Increase"),
    name = "") +
  
  theme(
    plot.title = element_text(size = 16, 
                              face = "bold", 
                              hjust = 0.5),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    axis.title.y = element_text(color = 'black'),
    axis.text.y = element_text(color = 'black'),
    legend.position = 'bottom'
  ) 
```

## Model 3: Different Continuous Variables with Industry Variable Compared to Model 2
```{r}
# Select independent variable
X_logit_m3 <- task3 %>%
  select(Mkt_beta, SMB_beta, scaled_dollar_vol, log_spread, log_volatility, log_tracking_error, log_turnover_ratio, SECTORMaterials, SECTORIndustrials, SECTORFinancials, SECTORInformation_Technology, SECTORConsumer_Discretionary, SECTORHealth_Care, SECTOREnergy, SECTORUtilities, SECTORReal_Estate, SECTORCommunication_Services, SECTORUnknown
)

y_logit_m3 <- task3$up_PRC  

# Split the dataset into training and testing sets
set.seed(675) 

trainIndex <- createDataPartition(y_logit_m3, p = 0.8, list = FALSE)
X_train_logit_m3 <- X_logit_m3[trainIndex, ]
X_test_logit_m3 <- X_logit_m3[-trainIndex, ]
y_train_logit_m3 <- y_logit_m3[trainIndex]
y_test_logit_m3 <- y_logit_m3[-trainIndex]


# Fit the logistic regression model
logit_model_3 <- glm(y_train_logit_m3 ~ ., 
                     data = as.data.frame(X_train_logit_m3, y_train_logit_m3),
                     family = binomial())

# Display model summary
summary(logit_model_3)

# Ensure no multicollinearity
print(vif(logit_model_3))

# Calculate predicted probabilities on testing data
pred_prob_m3 <- predict(logit_model_3, 
                        newdata = as.data.frame(X_test_logit_m3), 
                        type = "response")

# Determine classification based on threshold of 0.5
class_m3 <- ifelse(pred_prob_m3 > 0.5, 1, 0)

# Calculate True Positives, True Negatives, False Positives, False Negatives
true_positives_m3 <- sum(y_test_logit_m3 == 1 & class_m3 == 1)
true_negatives_m3 <- sum(y_test_logit_m3 == 0 & class_m3 == 0)
false_positives_m3 <- sum(y_test_logit_m3 == 0 & class_m3 == 1)
false_negatives_m3 <- sum(y_test_logit_m3 == 1 & class_m3 == 0)

# Calculate Accuracy
accuracy_m3 <- (true_positives_m3 + true_negatives_m3) / (true_positives_m3 + true_negatives_m3 + false_positives_m3 + false_negatives_m3)

# Calculate Precision
precision_m3 <- ifelse((true_positives_m3 + false_positives_m3) == 0, 0, true_positives_m3 / (true_positives_m3 + false_positives_m3))

# Calculate Recall
recall_m3 <- ifelse((true_positives_m3 + false_negatives_m3) == 0, 0, true_positives_m3 / (true_positives_m3 + false_negatives_m3))

# Calculate F1 Score
f1_m3 <- 2 * (precision_m3 * recall_m3) / (precision_m3 + recall_m3)

# Compute ROC curve and AUC
roc_curve_m3 <- roc(y_test_logit_m3, pred_prob_m3)
auc_value_m3 <- auc(roc_curve_m3)

# Calculate McFadden’s R-squared
null_model_3 <- glm(y_train_logit_m3 ~ 1, data = as.data.frame(cbind(X_train_logit_m3, y_train_logit_m3)), family = binomial())
rsquared_logit_m3 <- 1 - (logLik(logit_model_3) / logLik(null_model_3))

# Create a dataframe for counts
metrics_counts_logit_m3 <- data.frame(
  Metric = c('True Positives', 'True Negatives', 'False Positives', 'False Negatives'),
  Count = c(
    as.integer(true_positives_m3), 
    as.integer(true_negatives_m3), 
    as.integer(false_positives_m3), 
    as.integer(false_negatives_m3)
  )
)

# Create a dataframe for performance metrics
metrics_evaluation_logit_m3 <- data.frame(
  Metric = c('Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC', 'R_squared'),
  Value = c(
    accuracy_m3, 
    precision_m3, 
    recall_m3,
    f1_m3,
    auc_value_m3,
    rsquared_logit_m3
  )
)

# Print the results
print(metrics_counts_logit_m3)

print(metrics_evaluation_logit_m3)
```

### Confusion Matrix
```{r}
true_false_m3 <- matrix(c(true_negatives_m3, false_positives_m3, false_negatives_m3, true_positives_m3),
                        nrow = 2,
                        dimnames = list(Predicted = c("Negative", "Positive"),
                                        Actual = c("Negative", "Positive")))
											 
conf_matrix_m3 <- as.data.frame(as.table(true_false_m3))

# Plot the confusion matrix
palette <- brewer.pal(n = 4, 
                      name = "Greens")  # Define color

conf_matrix_m3_plot <- ggplot(conf_matrix_m3, 
                              aes(x = Predicted, 
                                  y = Actual, 
                                  fill = Freq)) +
  
  geom_tile(color = "black") + 
  
  geom_text(aes(label = Freq), 
            color = "black", 
            size = 6) +
  
  scale_fill_gradientn(colors = brewer.pal(n = 4, 
                                           name = "Greens"), 
                       guide = "none") +
  
  labs(title = "Confusion Matrix for Model 3: Price Increase Predictions between February 14, 2020 to March 20, 2020",
       subtitle = "(Model 3: Different Continuous Variables with Industry Variable Compared to Model 2)",
	   x = "Predicted", 
       y = "Actual") +
  
  theme_minimal() +
  
  theme(
      plot.title = element_text(size = 16, 
                                face = "bold", 
                                hjust = 0.5),
      axis.title = element_text(size = 14),
      axis.text = element_text(size = 12)
    )

print(conf_matrix_m3_plot)  

# Save the plot for report
save_plot(conf_matrix_m3_plot, "Confusion Matrix for logit Model 3")
```

### Important Factor
```{r}
# Extract coefficients
coefficients_logit_m3 <- coef(logit_model_3)

# Convert coefficients to odds ratios
odds_ratios_m3 <- exp(coefficients_logit_m3)

# Identify intercept and filter it out
importance_logit_m3 <- data.frame(
  Factors = names(coefficients_logit_m3),
  Odds_Ratio = odds_ratios_m3
)

# Exclude the intercept (assuming it's named 'Intercept')
importance_logit_m3 <- importance_logit_m3[importance_logit_m3$Factor != "(Intercept)", ]

# Add a column for fill color based on the odds ratio
importance_logit_m3$Fill_Color <- ifelse(importance_logit_m3$Odds_Ratio > 1, "steelblue", "coral")

# Plot with positive and negative impacts
# Plot the factor
ggplot(importance_logit_m3, 
       aes(x = reorder(Factors, Odds_Ratio), 
           y = Odds_Ratio, 
           fill = Fill_Color)) +
  
  geom_bar(stat = "identity") +
  
  coord_flip() +
  
  geom_text(aes(label = round(Odds_Ratio, 2)), 
            hjust = 1, 
            size = 3) + 
  
  labs(title = "Impact of Factors on Price increase likelihood for Model 3: Positive Vs Negative Associations",
       subtitle = "((Model 3: Different Continuous Variables with Industry Variable Compared to Model 2))",
       x = "Factors",
       y = "Odds Ratio\n(The exponentiation of coefficients)") +
  
  geom_hline(yintercept = 1, 
             linetype = "dashed", 
             color = "red") +
  
  annotate("text", 
           x = Inf, 
           y = 1, 
           label = "Odds Ratio = 1", 
           hjust = 0, 
           vjust = 0, 
           color = "grey", 
           size = 3.5, 
           fontface = "italic") +
  
  theme_minimal() +
  
  scale_fill_manual(
    values = c("steelblue" = "steelblue", "coral" = "coral"),
    labels = c("steelblue" = "Positively Associated with Price Increase", 
               "coral" = "Negatively Associated with Price Increase"),
    name = "") +
  
  theme(
    plot.title = element_text(size = 16, 
                              face = "bold", 
                              hjust = 0.5),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    axis.title.y = element_text(color = 'black'),
    axis.text.y = element_text(color = 'black'),
    legend.position = 'bottom'
  ) 
```

## Model 4: Continuous Variables from Model 3 without Industry Variable
```{r}
# Select independent variable
X_logit_m4 <- task3 %>%
  select(Mkt_beta, SMB_beta, scaled_dollar_vol, log_spread, log_volatility, log_tracking_error, log_turnover_ratio)

y_logit_m4 <- task3$up_PRC  

# Split the dataset into training and testing sets
set.seed(675)

trainIndex <- createDataPartition(y_logit_m4, p = 0.8, list = FALSE)
X_train_logit_m4 <- X_logit_m4[trainIndex, ]
X_test_logit_m4 <- X_logit_m4[-trainIndex, ]
y_train_logit_m4 <- y_logit_m4[trainIndex]
y_test_logit_m4 <- y_logit_m4[-trainIndex]


# Fit the logistic regression model
logit_model_4 <- glm(y_train_logit_m4 ~ ., 
                     data = as.data.frame(X_train_logit_m4, y_train_logit_m4),
                     family = binomial())

# Display model summary
summary(logit_model_4)

# Ensure no multicollinearity
vif(logit_model_4)

# Calculate predicted probabilities on testing data
pred_prob_m4 <- predict(logit_model_4, 
                        newdata = as.data.frame(X_test_logit_m4), 
                        type = "response")

# Determine classification based on threshold of 0.5
class_m4 <- ifelse(pred_prob_m4 > 0.5, 1, 0)

# Calculate True Positives, True Negatives, False Positives, False Negatives
true_positives_m4 <- sum(y_test_logit_m4 == 1 & class_m4 == 1)
true_negatives_m4 <- sum(y_test_logit_m4 == 0 & class_m4 == 0)
false_positives_m4 <- sum(y_test_logit_m4 == 0 & class_m4 == 1)
false_negatives_m4 <- sum(y_test_logit_m4 == 1 & class_m4 == 0)

# Calculate Accuracy
accuracy_m4 <- (true_positives_m4 + true_negatives_m4) / (true_positives_m4 + true_negatives_m4 + false_positives_m4 + false_negatives_m4)

# Calculate Precision
precision_m4 <- ifelse((true_positives_m4 + false_positives_m4) == 0, 0, true_positives_m4 / (true_positives_m4 + false_positives_m4))

# Calculate Recall
recall_m4 <- ifelse((true_positives_m4 + false_negatives_m4) == 0, 0, true_positives_m4 / (true_positives_m4 + false_negatives_m4))

# Calculate F1 Score
f1_m4 <- 2 * (precision_m4 * recall_m4) / (precision_m4 + recall_m4)

# Compute ROC curve and AUC
roc_curve_m4 <- roc(y_test_logit_m4, pred_prob_m4)
auc_value_m4 <- auc(roc_curve_m4)

# Calculate McFadden’s R-squared
null_model_4 <- glm(y_train_logit_m4 ~ 1, data = as.data.frame(cbind(X_train_logit_m4, y_train_logit_m4)), family = binomial())
rsquared_logit_m4 <- 1 - (logLik(logit_model_4) / logLik(null_model_4))

# Create a dataframe for counts
metrics_counts_logit_m4 <- data.frame(
  Metric = c('True Positives', 'True Negatives', 'False Positives', 'False Negatives'),
  Count = c(
    as.integer(true_positives_m4), 
    as.integer(true_negatives_m4), 
    as.integer(false_positives_m4), 
    as.integer(false_negatives_m4)
  )
)

# Create a dataframe for performance metrics
metrics_evaluation_logit_m4 <- data.frame(
  Metric = c('Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC', 'R_squared'),
  Value = c(
    accuracy_m4, 
    precision_m4, 
    recall_m4,
    f1_m4,
    auc_value_m4,
    rsquared_logit_m4
  )
)

# Print the results
print(metrics_counts_logit_m4)

print(metrics_evaluation_logit_m4)
```

### Confusion Matrix
```{r}
true_false_m4 <- matrix(c(true_negatives_m4, false_positives_m4, false_negatives_m4, true_positives_m4),
                        nrow = 2,
                        dimnames = list(Predicted = c("Negative", "Positive"),
                                        Actual = c("Negative", "Positive")))
											 
conf_matrix_m4 <- as.data.frame(as.table(true_false_m4))

# Plot the confusion matrix
palette <- brewer.pal(n = 4, 
                      name = "Greens")  # Define color

conf_matrix_m4_plot <- ggplot(conf_matrix_m4, 
                              aes(x = Predicted, 
                                  y = Actual, 
                                  fill = Freq)) +
  
  geom_tile(color = "black") + 
  
  geom_text(aes(label = Freq), 
            color = "black", 
            size = 6) +
  
  scale_fill_gradientn(colors = brewer.pal(n = 4, 
                                           name = "Greens"), 
                       guide = "none") +
  
  labs(title = "Confusion Matrix for Model 4: Price Increase Predictions between February 14, 2020 to March 20, 2020",
       subtitle = "(Model 4: Continuous Variables from Model 3 without Industry Variable)",
	   x = "Predicted", 
       y = "Actual") +
  
  theme_minimal() +
  
  theme(
      plot.title = element_text(size = 16, 
                                face = "bold", 
                                hjust = 0.5),
      axis.title = element_text(size = 14),
      axis.text = element_text(size = 12)
    )

print(conf_matrix_m4_plot)  

# Save the plot for report
save_plot(conf_matrix_m4_plot, "Confusion Matrix for logit Model 4")
```

### Important Factor
```{r}
# Extract coefficients
coefficients_logit_m4 <- coef(logit_model_4)

# Convert coefficients to odds ratios
odds_ratios_m4 <- exp(coefficients_logit_m4)

# Identify intercept and filter it out
importance_logit_m4 <- data.frame(
  Factors = names(coefficients_logit_m4),
  Odds_Ratio = odds_ratios_m4
)

# Exclude the intercept (assuming it's named 'Intercept')
importance_logit_m4 <- importance_logit_m4[importance_logit_m4$Factor != "(Intercept)", ]

# Add a column for fill color based on the odds ratio
importance_logit_m4$Fill_Color <- ifelse(importance_logit_m4$Odds_Ratio > 1, "steelblue", "coral")

# Plot with positive and negative impacts
# Plot the factor
ggplot(importance_logit_m4, 
       aes(x = reorder(Factors, Odds_Ratio), 
           y = Odds_Ratio, 
           fill = Fill_Color)) +
  
  geom_bar(stat = "identity") +
  
  coord_flip() +
  
  geom_text(aes(label = round(Odds_Ratio, 2)), 
            hjust = 1, 
            size = 3) + 
  
  labs(title = "Impact of Factors on Price increase likelihood for Model 4: Positive Vs Negative Associations",
       subtitle = "(Model 4: Continuous Variables from Model 3 without Industry Variable)", 
       x = "Factors",
       y = "Odds Ratio\n(The exponentiation of coefficients)") +
  
  geom_hline(yintercept = 1, 
             linetype = "dashed", 
             color = "red") +
  
  annotate("text", 
           x = Inf, 
           y = 1, 
           label = "Odds Ratio = 1", 
           hjust = 0, 
           vjust = 0, 
           color = "grey", 
           size = 3.5, 
           fontface = "italic") +
  
  theme_minimal() +
  
  scale_fill_manual(
    values = c("steelblue" = "steelblue", "coral" = "coral"),
    labels = c("steelblue" = "Positively Associated with Price Increase", 
               "coral" = "Negatively Associated with Price Increase"),
    name = "") +
  
  theme(
    plot.title = element_text(size = 16, 
                              face = "bold", 
                              hjust = 0.5),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    axis.title.y = element_text(color = 'black'),
    axis.text.y = element_text(color = 'black'),
    legend.position = 'bottom'
  ) 
```

## Model 5: Same variables with Model 1 and with Type of security variable
```{r}
# Select independent variable
X_logit_m5 <- task3 %>% 
  select(Mkt_beta, SMB_beta, HML_beta, log_spread, log_volatility, scaled_market_cap, log_tracking_error, log_turnover_ratio, security_typeETF)

y_logit_m5 <- task3$up_PRC  

# Split the dataset into training and testing sets
set.seed(675) 

trainIndex <- createDataPartition(y_logit_m5, p = 0.8, list = FALSE)
X_train_logit_m5 <- X_logit_m5[trainIndex, ]
X_test_logit_m5 <- X_logit_m5[-trainIndex, ]
y_train_logit_m5 <- y_logit_m5[trainIndex]
y_test_logit_m5 <- y_logit_m5[-trainIndex]


# Fit the logistic regression model
logit_model_5 <- glm(y_train_logit_m5 ~ ., 
                     data = as.data.frame(X_train_logit_m5, y_train_logit_m5),
                     family = binomial())

# Display model summary
summary(logit_model_5)

# Calculate predicted probabilities on testing data
pred_prob_m5 <- predict(logit_model_5, 
                        newdata = as.data.frame(X_test_logit_m5), 
                        type = "response")

# Determine classification based on threshold of 0.5
class_m5 <- ifelse(pred_prob_m5 > 0.5, 1, 0)

# Calculate True Positives, True Negatives, False Positives, False Negatives
true_positives_m5 <- sum(y_test_logit_m5 == 1 & class_m5 == 1)
true_negatives_m5 <- sum(y_test_logit_m5 == 0 & class_m5 == 0)
false_positives_m5 <- sum(y_test_logit_m5 == 0 & class_m5 == 1)
false_negatives_m5 <- sum(y_test_logit_m5 == 1 & class_m5 == 0)

# Calculate Accuracy
accuracy_m5 <- (true_positives_m5 + true_negatives_m5) / (true_positives_m5 + true_negatives_m5 + false_positives_m5 + false_negatives_m5)

# Calculate Precision
precision_m5 <- ifelse((true_positives_m5 + false_positives_m5) == 0, 0, true_positives_m5 / (true_positives_m5 + false_positives_m5))

# Calculate Recall
recall_m5 <- ifelse((true_positives_m5 + false_negatives_m5) == 0, 0, true_positives_m5 / (true_positives_m5 + false_negatives_m5))

# Calculate F1 Score
f1_m5 <- 2 * (precision_m5 * recall_m5) / (precision_m5 + recall_m5)

# Compute ROC curve and AUC
roc_curve_m5 <- roc(y_test_logit_m5, pred_prob_m5)
auc_value_m5 <- auc(roc_curve_m5)

# Calculate McFadden’s R-squared
null_model_5 <- glm(y_train_logit_m5 ~ 1, data = as.data.frame(cbind(X_train_logit_m5, y_train_logit_m5)), family = binomial())
rsquared_logit_m5 <- 1 - (logLik(logit_model_5) / logLik(null_model_5))

# Create a dataframe for counts
metrics_counts_logit_m5 <- data.frame(
  Metric = c('True Positives', 'True Negatives', 'False Positives', 'False Negatives'),
  Count = c(
    as.integer(true_positives_m5), 
    as.integer(true_negatives_m5), 
    as.integer(false_positives_m5), 
    as.integer(false_negatives_m5)
  )
)

# Create a dataframe for performance metrics
metrics_evaluation_logit_m5 <- data.frame(
  Metric = c('Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC', 'R_squared'),
  Value = c(
    accuracy_m5, 
    precision_m5, 
    recall_m5,
    f1_m5,
    auc_value_m5,
    rsquared_logit_m5
  )
)

# Print the results
print(metrics_counts_logit_m5)

print(metrics_evaluation_logit_m5)
```

### Confusion Matrix
```{r}
true_false_m5 <- matrix(c(true_negatives_m5, false_positives_m5, false_negatives_m5, true_positives_m5),
                        nrow = 2,
                        dimnames = list(Predicted = c("Negative", "Positive"),
                                        Actual = c("Negative", "Positive")))
											 
conf_matrix_m5 <- as.data.frame(as.table(true_false_m5))

# Plot the confusion matrix
palette <- brewer.pal(n = 4, 
                      name = "Greens")  # Define color

conf_matrix_m5_plot <- ggplot(conf_matrix_m5, 
                              aes(x = Predicted, 
                                  y = Actual, 
                                  fill = Freq)) +
  
  geom_tile(color = "black") + 
  
  geom_text(aes(label = Freq), 
            color = "black", 
            size = 6) +
  
  scale_fill_gradientn(colors = brewer.pal(n = 4, 
                                           name = "Greens"), 
                       guide = "none") +
  
  labs(title = "Confusion Matrix for Model 5: Price Increase Predictions between February 14, 2020 to March 20, 2020",
       subtitle = "(Model 5: Same variables with Model 1 and with Type of security variable)",
	   x = "Predicted", 
       y = "Actual") +
  
  theme_minimal() +
  
  theme(
      plot.title = element_text(size = 16, 
                                face = "bold", 
                                hjust = 0.5),
      axis.title = element_text(size = 14),
      axis.text = element_text(size = 12)
    )

print(conf_matrix_m5_plot)  

# Save the plot for report
save_plot(conf_matrix_m5_plot, "Confusion Matrix for logit Model 5")
```

### Important Factor
```{r}
# Extract coefficients
coefficients_logit_m5 <- coef(logit_model_5)

# Convert coefficients to odds ratios
odds_ratios_m5 <- exp(coefficients_logit_m5)

# Identify intercept and filter it out
importance_logit_m5 <- data.frame(
  Factors = names(coefficients_logit_m5),
  Odds_Ratio = odds_ratios_m5
)

# Exclude the intercept (assuming it's named 'Intercept')
importance_logit_m5 <- importance_logit_m5[importance_logit_m5$Factor != "(Intercept)", ]

# Add a column for fill color based on the odds ratio
importance_logit_m5$Fill_Color <- ifelse(importance_logit_m5$Odds_Ratio > 1, "steelblue", "coral")

# Plot the factor
ggplot(importance_logit_m5, 
       aes(x = reorder(Factors, Odds_Ratio), 
           y = Odds_Ratio, 
           fill = Fill_Color)) +
  
  geom_bar(stat = "identity") +
  
  coord_flip() +
  
  geom_text(aes(label = round(Odds_Ratio, 2)), 
            hjust = 1, 
            size = 3) + 
  
  labs(title = "Impact of Factors on Price increase likelihood for Model 5: Positive Vs Negative Associations",
       subtitle = "(Model 5: Continuous Variables from Model 1 with type of security variable)", 
       x = "Factors",
       y = "Odds Ratio\n(The exponentiation of coefficients)") +
  
  geom_hline(yintercept = 1, 
             linetype = "dashed", 
             color = "red") +
  
  annotate("text", 
           x = Inf, 
           y = 1, 
           label = "Odds Ratio = 1", 
           hjust = 0, 
           vjust = 0, 
           color = "grey", 
           size = 3.5, 
           fontface = "italic") +
  
  theme_minimal() +
  
  scale_fill_manual(
    values = c("steelblue" = "steelblue", "coral" = "coral"),
    labels = c("steelblue" = "Positively Associated with Price Increase", 
               "coral" = "Negatively Associated with Price Increase"),
    name = "") +
  
  theme(
    plot.title = element_text(size = 16, 
                              face = "bold", 
                              hjust = 0.5),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    axis.title.y = element_text(color = 'black'),
    axis.text.y = element_text(color = 'black'),
    legend.position = 'bottom'
  ) 
```

## Model 6: Same variables with Model 3 and with Type of security variable
```{r}
# Select independent variable
X_logit_m6 <- task3 %>%
  select(Mkt_beta, SMB_beta, scaled_dollar_vol, log_spread, log_volatility, log_tracking_error, log_turnover_ratio, SECTORMaterials, SECTORIndustrials, SECTORFinancials, SECTORInformation_Technology, SECTORConsumer_Discretionary, SECTORHealth_Care, SECTOREnergy, SECTORUtilities, SECTORReal_Estate, SECTORCommunication_Services, SECTORUnknown, security_typeETF)

y_logit_m6 <- task3$up_PRC  

# Split the dataset into training and testing sets
set.seed(675)  

trainIndex <- createDataPartition(y_logit_m6, p = 0.8, list = FALSE)
X_train_logit_m6 <- X_logit_m6[trainIndex, ]
X_test_logit_m6 <- X_logit_m6[-trainIndex, ]
y_train_logit_m6 <- y_logit_m6[trainIndex]
y_test_logit_m6 <- y_logit_m6[-trainIndex]


# Fit the logistic regression model
logit_model_6 <- glm(y_train_logit_m6 ~ ., 
                     data = as.data.frame(X_train_logit_m6, y_train_logit_m6),
                     family = binomial())

# Display model summary
summary(logit_model_6)

# Calculate predicted probabilities on testing data
pred_prob_m6 <- predict(logit_model_6, 
                        newdata = as.data.frame(X_test_logit_m6), 
                        type = "response")

# Determine classification based on threshold of 0.5
class_m6 <- ifelse(pred_prob_m6 > 0.5, 1, 0)

# Calculate True Positives, True Negatives, False Positives, False Negatives
true_positives_m6 <- sum(y_test_logit_m6 == 1 & class_m6 == 1)
true_negatives_m6 <- sum(y_test_logit_m6 == 0 & class_m6 == 0)
false_positives_m6 <- sum(y_test_logit_m6 == 0 & class_m6 == 1)
false_negatives_m6 <- sum(y_test_logit_m6 == 1 & class_m6 == 0)

# Calculate Accuracy
accuracy_m6 <- (true_positives_m6 + true_negatives_m6) / (true_positives_m6 + true_negatives_m6 + false_positives_m6 + false_negatives_m6)

# Calculate Precision
precision_m6 <- ifelse((true_positives_m6 + false_positives_m6) == 0, 0, true_positives_m6 / (true_positives_m6 + false_positives_m6))

# Calculate Recall
recall_m6 <- ifelse((true_positives_m6 + false_negatives_m6) == 0, 0, true_positives_m6 / (true_positives_m6 + false_negatives_m6))

# Calculate F1 Score
f1_m6 <- 2 * (precision_m6 * recall_m6) / (precision_m6 + recall_m6)

# Compute ROC curve and AUC
roc_curve_m6 <- roc(y_test_logit_m6, pred_prob_m6)
auc_value_m6 <- auc(roc_curve_m6)

# Calculate McFadden’s R-squared
null_model_6 <- glm(y_train_logit_m6 ~ 1, data = as.data.frame(cbind(X_train_logit_m6, y_train_logit_m6)), family = binomial())
rsquared_logit_m6 <- 1 - (logLik(logit_model_6) / logLik(null_model_6))

# Create a dataframe for counts
metrics_counts_logit_m6 <- data.frame(
  Metric = c('True Positives', 'True Negatives', 'False Positives', 'False Negatives'),
  Count = c(
    as.integer(true_positives_m6), 
    as.integer(true_negatives_m6), 
    as.integer(false_positives_m6), 
    as.integer(false_negatives_m6)
  )
)

# Create a dataframe for performance metrics
metrics_evaluation_logit_m6 <- data.frame(
  Metric = c('Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC', 'R_squared'),
  Value = c(
    accuracy_m6, 
    precision_m6, 
    recall_m6,
    f1_m6,
    auc_value_m6,
    rsquared_logit_m6
  )
)

# Print the results
print(metrics_counts_logit_m6)

print(metrics_evaluation_logit_m6)
```

### Confusion Matrix
```{r}
true_false_m6 <- matrix(c(true_negatives_m6, false_positives_m6, false_negatives_m6, true_positives_m6),
                        nrow = 2,
                        dimnames = list(Predicted = c("Negative", "Positive"),
                                        Actual = c("Negative", "Positive")))
											 
conf_matrix_m6 <- as.data.frame(as.table(true_false_m6))

# Plot the confusion matrix
palette <- brewer.pal(n = 4, 
                      name = "Greens")  # Define color

conf_matrix_m6_plot <- ggplot(conf_matrix_m6, 
                              aes(x = Predicted, 
                                  y = Actual, 
                                  fill = Freq)) +
  
  geom_tile(color = "black") + 
  
  geom_text(aes(label = Freq), 
            color = "black", 
            size = 6) +
  
  scale_fill_gradientn(colors = brewer.pal(n = 4, 
                                           name = "Greens"), 
                       guide = "none") +
  
  labs(title = "Confusion Matrix for Model 6: Price Increase Predictions between February 14, 2020 to March 20, 2020",
       subtitle = "(Model 6: Same variables with Model 3 and with Type of security variable)",
	   x = "Predicted", 
       y = "Actual") +
  
  theme_minimal() +
  
  theme(
      plot.title = element_text(size = 16, 
                                face = "bold", 
                                hjust = 0.5),
      axis.title = element_text(size = 14),
      axis.text = element_text(size = 12)
    )

print(conf_matrix_m6_plot)  

# Save the plot for report
save_plot(conf_matrix_m6_plot, "Confusion Matrix for logit Model 6")
```

### Important Factor
```{r}
# Extract coefficients
coefficients_logit_m6 <- coef(logit_model_6)

# Convert coefficients to odds ratios
odds_ratios_m6 <- exp(coefficients_logit_m6)

# Identify intercept and filter it out
importance_logit_m6 <- data.frame(
  Factors = names(coefficients_logit_m6),
  Odds_Ratio = odds_ratios_m6
)

# Exclude the intercept (assuming it's named 'Intercept')
importance_logit_m6 <- importance_logit_m6[importance_logit_m6$Factor != "(Intercept)", ]

# Add a column for fill color based on the odds ratio
importance_logit_m6$Fill_Color <- ifelse(importance_logit_m6$Odds_Ratio > 1, "steelblue", "coral")

# Plot with positive and negative impacts
# Plot the factor
ggplot(importance_logit_m6, 
       aes(x = reorder(Factors, Odds_Ratio), 
           y = Odds_Ratio, 
           fill = Fill_Color)) +
  
  geom_bar(stat = "identity") +
  
  coord_flip() +
  
  geom_text(aes(label = round(Odds_Ratio, 2)), 
            hjust = 1, 
            size = 3) + 
  
  labs(title = "Impact of Factors on Price increase likelihood for Model 5: Positive Vs Negative Associations",
       subtitle = "(Model 6: Same variables with Model 3 and with Type of security variable)", 
       x = "Factors",
       y = "Odds Ratio\n(The exponentiation of coefficients)") +
  
  geom_hline(yintercept = 1, 
             linetype = "dashed", 
             color = "red") +
  
  annotate("text", 
           x = Inf, 
           y = 1, 
           label = "Odds Ratio = 1", 
           hjust = 0, 
           vjust = 0, 
           color = "grey", 
           size = 3.5, 
           fontface = "italic") +
  
  theme_minimal() +
  
  scale_fill_manual(
    values = c("steelblue" = "steelblue", "coral" = "coral"),
    labels = c("steelblue" = "Positively Associated with Price Increase", 
               "coral" = "Negatively Associated with Price Increase"),
    name = "") +
  
  theme(
    plot.title = element_text(size = 16, 
                              face = "bold", 
                              hjust = 0.5),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    axis.title.y = element_text(color = 'black'),
    axis.text.y = element_text(color = 'black'),
    legend.position = 'bottom'
  ) 
```

## Model variables difference
```{r}
logit_m1_v <- get_variables(logit_model_1)
logit_m2_v <- get_variables(logit_model_2)
logit_m3_v <- get_variables(logit_model_3)
logit_m4_v <- get_variables(logit_model_4)
logit_m5_v <- get_variables(logit_model_5)
logit_m6_v <- get_variables(logit_model_6)

logit_v_list <- list(
  Model_1 = as.character(logit_m1_v),
  Model_2 = as.character(logit_m2_v),
  Model_3 = as.character(logit_m3_v),
  Model_4 = as.character(logit_m4_v),
  Model_5 = as.character(logit_m5_v),
  Model_6 = as.character(logit_m6_v)
)

# Initialise empty data frame
logit_model_data <- data.frame(
  model = character(),
  variable = character(),
  order = integer(),  
  stringsAsFactors = FALSE
)

# Populate the dataframe with the original order of variables
for (model in names(logit_v_list)) {
  variables <- logit_v_list[[model]]  # Get the variables for each model
  temp_df <- data.frame(
    model = model,
    variable = variables,
    order = seq_along(variables), 
    stringsAsFactors = FALSE
  )
  logit_model_data <- rbind(logit_model_data, temp_df)
}

# Convert variable to a factor with levels 
logit_model_data$variable <- factor(logit_model_data$variable, levels = unique(logit_model_data$variable[order(logit_model_data$order)]))

# Generate the Plot
logit_models_v <- ggplot(logit_model_data, 
                         aes(x = model, 
                             y = variable, 
                             fill = model)) +
  
  geom_tile(color = "white") + 
  
  labs(
    title = "Variables Included in Each Logistic Model",
    x = "Model",
    y = "Variables"
  ) +
  
  theme_minimal() +
  
  theme(
    axis.text.x = element_text(angle = 45, 
                               hjust = 1, 
                               size = 12),  
    axis.text.y = element_text(size = 12),  
    axis.title.x = element_text(size = 14), 
    axis.title.y = element_text(size = 14), 
    plot.title = element_text(hjust = 0.5, 
                              size = 16),  
    plot.title.position = "plot")

print(logit_models_v)

# Save the plot for report
save_plot(logit_models_v, "logit_models_v")
```

#   4 K-mean Clustering
```{r}
# Load library
library(randomForest)
library(cluster)
```

```{r}
# Import data
Stock_data_2 <- read.csv("Stock_data_part2.csv")

# Make sure all the "date" are converted to ISO 8601 format
Stock_data_2$public_date <- as.Date(Stock_data_2$public_date, format = "%d/%m/%Y")
Stock_data_2$public_date <- format(Stock_data_2$public_date, format = "%Y-%m-%d")
Stock_data_2$public_date <- as.Date(Stock_data_2$public_date, format = "%Y-%m-%d")

# Inspect data
str(Stock_data_2)
head(Stock_data_2, n = 5)
```

## Select the two most important variables for clustering
```{r}
# Create a cross-sectional dataset
task4_select_ratios <- na.omit(
  Stock_data_2 %>%
    filter(TICKER != "") %>%
    group_by(TICKER) %>%
    summarise(
      TICKER = first(TICKER),
      CAPEI = median(CAPEI, na.rm = TRUE),
      bm = median(bm, na.rm = TRUE),
      evm = median(evm, na.rm = TRUE),
      pe_exi = median(pe_exi, na.rm = TRUE),
      dpr = median(dpr, na.rm = TRUE),
      npm = median(npm, na.rm = TRUE),
      roa = median(roa, na.rm = TRUE),
      roe = median(roe, na.rm = TRUE),
      roce = median(roce, na.rm = TRUE),
      ptb = median(ptb, na.rm = TRUE),
      divyield = median(as.numeric(sub("%", "", divyield)), na.rm = TRUE),
      .groups = "drop"
    )) %>% 
  select(-TICKER)


# Random Forest Importance
task4_rf_model <- randomForest(x = task4_select_ratios, y = NULL, ntree = 100)
task4_select_ratios_rf <- importance(task4_rf_model)
task4_rf_df <- data.frame(Feature = rownames(task4_select_ratios_rf), 
                             MeanDecreaseGini = task4_select_ratios_rf[, "MeanDecreaseGini"])

# Plot the result
task4_select <- ggplot(task4_rf_df, 
                       aes(x = reorder(Feature, MeanDecreaseGini), 
                           y = MeanDecreaseGini)) +
  
  geom_bar(stat = "identity", fill = "lightblue") +
  
  coord_flip() +
  
  labs(title = "Feature Importance of financial ratio", 
       x = "Financial Ratio", 
       y = "Mean Decrease in Gini") +
  
  theme_minimal()+ 
  
    theme(
      plot.title = element_text(size = 16, 
                                face = "bold", 
                                hjust = 0.5),
      axis.title = element_text(size = 14),
      axis.text = element_text(size = 12)
    )

print(task4_select)

# Save the plot for report
save_plot(task4_select, "task4_select")
```

## Create function to transform dataset to cross-sectional
```{r}
task4_dataset_transformation <- function(data, filter_date, output_name_cleaned, output_name_clustering) {
  
  aggregated_data <- data %>%
    filter(TICKER != "") %>%
    filter(public_date < filter_date) %>% 
    group_by(TICKER) %>%
    summarise(
      TICKER = first(TICKER),
      # get the median (avoid outlier affect the result)
      bm = median(bm, na.rm = TRUE),
      roe = median(roe, na.rm = TRUE),
      .groups = "drop"
    )
  
  print(head(aggregated_data, n = 5))
  
  # Remove outliers for B/M ratio by IQR method
  Q1_BM <- quantile(aggregated_data$bm, 0.25, na.rm = TRUE)
  Q3_BM <- quantile(aggregated_data$bm, 0.75, na.rm = TRUE)
  IQR_BM <- Q3_BM - Q1_BM
  cleaned_data <- aggregated_data[aggregated_data$bm > (Q1_BM - 1.5 * IQR_BM) & 
                                     aggregated_data$bm < (Q3_BM + 1.5 * IQR_BM), ]
  
  # Remove outliers for ROE with IQR method
  Q1_ROE <- quantile(cleaned_data$roe, 0.25, na.rm = TRUE)
  Q3_ROE <- quantile(cleaned_data$roe, 0.75, na.rm = TRUE)
  IQR_ROE <- Q3_ROE - Q1_ROE
  cleaned_data <- cleaned_data[cleaned_data$roe > (Q1_ROE - 1.5 * IQR_ROE) & 
                                 cleaned_data$roe < (Q3_ROE + 1.5 * IQR_ROE), ]
  
  # Join with returns data - calculate return using the price of the last day and the first day from the period to get the % of price change
  cluster_data <- cleaned_data %>%
    left_join(task2 %>%  
                filter(date >= "2019-12-14" & date <= "2020-01-20") %>%
                group_by(TICKER) %>%
                summarise(
                  noncovid_RET = (PRC[which.max(date)] - PRC[which.min(date)]) / PRC[which.min(date)]), 
              by = "TICKER") %>%
    
    left_join(task2 %>%  
                filter(date >= "2020-02-14" & date <= "2020-03-20") %>%
                group_by(TICKER) %>%
                summarise(
                  COVID_RET = (PRC[which.max(date)] - PRC[which.min(date)]) / PRC[which.min(date)]),
              by = "TICKER") %>% 
    
    left_join(task2 %>%  
                filter(date >= "2019-12-14" & date <= "2020-01-20") %>%
                group_by(TICKER) %>%
                summarise(
                  noncovid_sharpe_ratio = mean(sharpe_ratio, na.rm = TRUE),
                  noncovid_volatility = mean(volatility, na.rm = TRUE)
                ), 
              by = "TICKER") %>% 
    
    left_join(task2 %>%  
                filter(date >= "2020-02-14" & date <= "2020-03-20") %>%
                group_by(TICKER) %>%
                summarise(
                  covid_sharpe_ratio = mean(sharpe_ratio, na.rm = TRUE),
                  covid_volatility = mean(volatility, na.rm = TRUE)
                ), 
              by = "TICKER") %>%
    
    # Remove missing value
    filter(!is.na(bm) & !is.na(roe)) %>%
    
    # Standardise variable to ensure they are on same scale
    mutate(
      bm_scaled = scale(bm),
      roe_scaled = scale(roe)
    )
  
  print(head(cluster_data, n = 5))
  
  # Select scaled variables
  output_data <- cluster_data[, c("bm_scaled", "roe_scaled")]
  
  print(head(output_data, n = 5))
  
  # Assign output name
  assign(output_name_cleaned, cluster_data, envir = .GlobalEnv)
  assign(output_name_clustering, output_data, envir = .GlobalEnv)
}
```

## Call function to create dataset
```{r}
# Entire Period
task4_dataset_transformation(Stock_data_2, "2022-12-31", "task4_entire", "task4_cluster_entire")

# Non-COVID period
task4_dataset_transformation(Stock_data_2, "2019-12-14", "task4_noncovid", "task4_cluster_noncovid")

# COVID period
task4_dataset_transformation(Stock_data_2, "2020-02-14", "task4_covid", "task4_cluster_covid")
```

## Create function to generate Elbow plot
```{r}
generate_elbow_plot <- function(find_k_data, elbow_plot_name) {
  inertia_cleaned <- numeric(10)
  
  for (n in 1:10) {
    k_mean_cleaned <- tryCatch({
      kmeans(find_k_data, centers = n, nstart = 10)
    }, error = function(e) {
      return(NULL)
    })
    # Debugging
    if (!is.null(k_mean_cleaned)) {
      inertia_cleaned[n] <- k_mean_cleaned$tot.withinss
      cat(sprintf("Cluster: %d, Inertia: %f\n", n, inertia_cleaned[n]))
    } else {
      cat(sprintf("An error occurred at n_clusters=%d\n", n))
    }
  }
  # Debugging
  if (length(inertia_cleaned) == 10) {
    elbow_data_cleaned <- data.frame(
      clusters_cleaned = 1:10,
      inertia_cleaned = inertia_cleaned
    )
    
    # Generate the plot
    elbow_plot <- ggplot(elbow_data_cleaned, 
                         aes(x = clusters_cleaned, 
                             y = inertia_cleaned)) +
      
      geom_line(color = "blue", size = 1) +  
      
      geom_point(color = "red", 
                 size = 4, 
                 shape = 19) +  
      
      scale_x_continuous(breaks = 1:10) + 
      
      ggtitle('Elbow Method for Clustering') + 
      
      xlab('Number of Clusters') +
      
      ylab('Inertia') +
      
      theme_light() + 
      
      theme(
        plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 14),
        axis.text = element_text(size = 12)
      )
    
    print(elbow_plot)
    
    # Save the plot for report
    save_plot(elbow_plot, elbow_plot_name)
    
  } else {
    cat("There is some error.")
  }
}
```

## Create function to generate cluster visualisation
```{r}
generate_cluster_visualisation <- function(clustering_data, cluster_data, cluster_plot_name, period_for_title, centers, output_name) {
  
  set.seed(42)
  
  k_mean_cleaned <- kmeans(clustering_data, centers = centers, nstart = 10)
  
  # Assign cluster to each stock
  cluster_data$cluster <- factor(k_mean_cleaned$cluster)
  
  # Convert centroids to data frame
  centroids_cleaned <- as.data.frame(k_mean_cleaned$centers)
  names(centroids_cleaned) <- c("bm_scaled", "roe_scaled")
  
  # Plot the scatterplot with specified clusters
  cluster_visual <- ggplot(cluster_data, 
                           aes(x = bm_scaled, 
                               y = roe_scaled, 
                               color = cluster)) +
    
    geom_point(size = 3) +
    
    geom_point(data = centroids_cleaned, 
               aes(x = bm_scaled, 
                   y = roe_scaled, 
                   shape = "Centroid"),
               color = 'red', 
               size = 6, 
               alpha = 1) +
    
    scale_shape_manual(name = "", 
                       values = c("Centroid" = 8)) + 
    
    labs(title = 'Cluster Visualisation: Market-to-Book ratio VS Return on Equity',
         subtitle = period_for_title,
         x = 'Standardised Book / Market ratio (bm)',
         y = 'Standardised Return on Equity (roe)',
         color = 'Cluster') +
    
    theme(plot.title = element_text(size = 16, 
                                    face = "bold"),
          plot.subtitle = element_text(size = 12)) +
    
    scale_color_brewer(palette = "Pastel2") +
    
    theme_minimal() +
    
    theme(panel.grid.major = element_line(color = "gray90", 
                                          size = 0.5),
          axis.title = element_text(size = 14),
          axis.text = element_text(size = 12),
          plot.title = element_text(size = 16, 
                                    face = "bold"),
          plot.subtitle = element_text(size = 12))
  
  print(cluster_visual)
  
  # Save plot for report
  save_plot(cluster_visual, cluster_plot_name)
  
  # Assign output name
  assign(output_name, cluster_data, envir = .GlobalEnv)
}

```

## Call function to generate elbow plot and cluster visualisation
```{r}
# Entire Period
generate_elbow_plot(task4_cluster_entire, "Task 4 - Elbow Plot (Entire)")
generate_cluster_visualisation(task4_cluster_entire, task4_entire, "Task 4 Clustering (Entire)", "Aggregate from Entire Period", 3, "task4_entire")

# Non-COVID period
generate_elbow_plot(task4_cluster_noncovid, "Task 4 - Elbow Plot (Non-COVID)")
generate_cluster_visualisation(task4_cluster_noncovid, task4_noncovid, "Task 4 Clustering (Non-COVID)","Aggregate from Non-COVID Period", 3, "task4_noncovid")

# COVID period
generate_elbow_plot(task4_cluster_covid, "Task 4 - Elbow Plot (COVID)")
generate_cluster_visualisation(task4_cluster_covid, task4_covid, "Task 4 Clustering (COVID)","Aggregate from COVID Period", 3, "task4_covid")
```

## Cluster Characteristics
```{r}
# Entire Period
Cluster_characteristics_entire <- task4_entire %>% 
  group_by(cluster) %>% 
  summarise(mean_bm = mean(bm, na.rm = TRUE),
            mean_roe = mean(roe, na.rm = TRUE))

print(Cluster_characteristics_entire)

# Non-COVID Period
Cluster_characteristics_noncovid <- task4_noncovid %>% 
  group_by(cluster) %>% 
  summarise(mean_bm = mean(bm, na.rm = TRUE),
            mean_roe = mean(roe, na.rm = TRUE))

print(Cluster_characteristics_noncovid)

# COVID period
Cluster_characteristics_covid <- task4_covid %>% 
  group_by(cluster) %>% 
  summarise(mean_bm = mean(bm, na.rm = TRUE),
            mean_roe = mean(roe, na.rm = TRUE))

print(Cluster_characteristics_covid)
```

## Average return by cluster for each period
```{r}
# Create function to calculate average return by cluster for each period
calculate_returns_cluster <- function(cluster_data, output_prefix) {
  output_covid <- cluster_data %>%
  filter(!is.na(COVID_RET) & !is.na(noncovid_RET)) %>% 
  group_by(cluster) %>%
  summarise(cluster_RET_covid = mean(COVID_RET, na.rm = TRUE))
  
  output_noncovid <- cluster_data %>%
  filter(!is.na(COVID_RET) & !is.na(noncovid_RET)) %>% 
  group_by(cluster) %>%
  summarise(cluster_RET_noncovid = mean(noncovid_RET, na.rm = TRUE))
  
  # Print the result
  print(output_noncovid)
  print(output_covid)

}

# Call function
calculate_returns_cluster(task4_entire, "entire")
calculate_returns_cluster(task4_noncovid, "non-covid")
calculate_returns_cluster(task4_covid, "covid")
```

## Investment implications
```{r}
# Function to normalise metrics and calculate composite scores
calculate_composite_scores <- function(data, selected_cluster, output_prefix) {
  
  # Normalisation for Non-COVID period
  normalised_data_noncovid <- data %>%
    filter(cluster == selected_cluster) %>%
    filter(!if_any(starts_with("noncovid_"), is.na)) %>%
    mutate(
      noncovid_RET_normalised = (noncovid_RET - min(noncovid_RET)) / (max(noncovid_RET) - min(noncovid_RET)),
      noncovid_volatility_normalised = 1 - ((noncovid_volatility - min(noncovid_volatility)) / (max(noncovid_volatility) - min(noncovid_volatility))),
      noncovid_sharpe_ratio_normalised = (noncovid_sharpe_ratio - min(noncovid_sharpe_ratio)) / (max(noncovid_sharpe_ratio) - min(noncovid_sharpe_ratio))
    )
  
  # Calculate scores based on different strategies for Non-COVID
  strategies_noncovid <- list(
    max_return_noncovid = list(weights = c(0.5, 0.3, 0.2), 
                               columns = c("noncovid_RET_normalised", 
                                           "noncovid_sharpe_ratio_normalised", 
                                           "noncovid_volatility_normalised")),
    risk_adjusted_noncovid = list(weights = c(0.3, 0.5, 0.2), 
                                  columns = c("noncovid_RET_normalised", 
                                              "noncovid_sharpe_ratio_normalised", 
                                              "noncovid_volatility_normalised")),
    low_risk_noncovid = list(weights = c(0.2, 0.3, 0.5), 
                             columns = c("noncovid_RET_normalised", 
                                         "noncovid_sharpe_ratio_normalised", 
                                         "noncovid_volatility_normalised"))
  )
  
  results_noncovid <- list()
  
  for (strategy in names(strategies_noncovid)) {
    weight_vector <- strategies_noncovid[[strategy]]$weights
    score_columns <- strategies_noncovid[[strategy]]$columns
    
    results_noncovid[[strategy]] <- normalised_data_noncovid %>%
      mutate(
        composite_score = rowSums(sweep(select(., all_of(score_columns)), 2, weight_vector, `*`))
      ) %>%
      arrange(desc(composite_score)) %>%
      select(TICKER, noncovid_RET_normalised, noncovid_volatility_normalised, noncovid_sharpe_ratio_normalised, composite_score)
    
    # Print the top 3 securities
    cat(paste("\nTop 3 for", strategy, ":\n"))
    print(head(results_noncovid[[strategy]], 3))
  }
  
  # Normalisation for COVID period
  normalised_data_covid <- data %>%
    filter(cluster == selected_cluster) %>%
    filter(!is.na(COVID_RET) & !if_any(starts_with("covid_"), is.na)) %>%
    mutate(
      covid_RET_normalised = (COVID_RET - min(COVID_RET)) / (max(COVID_RET) - min(COVID_RET)),
      covid_volatility_normalised = 1 - ((covid_volatility - min(covid_volatility)) / (max(covid_volatility) - min(covid_volatility))),
      covid_sharpe_ratio_normalised = (covid_sharpe_ratio - min(covid_sharpe_ratio)) / (max(covid_sharpe_ratio) - min(covid_sharpe_ratio))
    )
  
  # Calculate scores based on different strategies for COVID
  strategies_covid <- list(
    max_return_covid = list(weights = c(0.5, 0.3, 0.2), 
                            columns = c("covid_RET_normalised", 
                                        "covid_sharpe_ratio_normalised", 
                                        "covid_volatility_normalised")),
    risk_adjusted_covid = list(weights = c(0.3, 0.5, 0.2), 
                               columns = c("covid_RET_normalised", 
                                           "covid_sharpe_ratio_normalised", 
                                           "covid_volatility_normalised")),
    low_risk_covid = list(weights = c(0.2, 0.3, 0.5), 
                          columns = c("covid_RET_normalised", 
                                      "covid_sharpe_ratio_normalised", 
                                      "covid_volatility_normalised"))
  )
  
  results_covid <- list()
  
  for (strategy in names(strategies_covid)) {
    weight_vector <- strategies_covid[[strategy]]$weights
    score_columns <- strategies_covid[[strategy]]$columns
    
    results_covid[[strategy]] <- normalised_data_covid %>%
      mutate(
        composite_score = rowSums(sweep(select(., all_of(score_columns)), 2, weight_vector, `*`))
      ) %>%
      arrange(desc(composite_score)) %>%
      select(TICKER, covid_RET_normalised, covid_volatility_normalised, covid_sharpe_ratio_normalised, composite_score)
    
    # Print the top 3 securities
    cat(paste("\nTop 3 for", strategy, ":\n"))
    print(head(results_covid[[strategy]], 3))
  }
}

# Call function
calculate_composite_scores(task4_entire, 3, "entire")
calculate_composite_scores(task4_noncovid, 3, "noncovid")
calculate_composite_scores(task4_covid, 2, "covid")

```

#   5 Principal Component Analysis

## Data Preparation
```{r}
# Import data
Stock_data_3 <- read.csv("Stock_data_part3.csv")
sp500 <- read.csv("S&P500.csv")
long_term_rate <- read.csv("long-term-rates-2000-2023.csv")

# Make sure all the "date" are converted to ISO 8601 format
Stock_data_3$date <- as.Date(Stock_data_3$date, format = "%Y-%m-%d")
long_term_rate$date <- as.Date(long_term_rate$date, format = "%Y-%m-%d")

# Inspect data
str(Stock_data_3)
head(Stock_data_3, n = 5)

head(sp500, n = 5)
```

```{r}
# Data wrangling
sp500 <- sp500 %>%
  mutate(Symbol = case_when(
    Symbol == "BRK.B" ~ "BRK",
    Symbol == "BF.B" ~ "BF",
    TRUE ~ Symbol
  ))
```

```{r}
# Data cleaning: inspect missing value and extract month for later dataset join
task5_dataset1 <- Stock_data_3 %>%
  mutate(
    PRC = abs(PRC),
    PRC = ifelse(PRC == 0.0, NA, PRC),
    VOL = ifelse(VOL == -99, NA, VOL),
    RET = case_when(
      RET %in% c(-44, -55, -66, -77, -88, -99) ~ NA,  
      RET %in% c(".A", ".B", ".C", ".D") ~ NA,        
      TRUE ~ as.numeric(as.character(RET))
    ),
    SPREAD = ifelse(SPREAD <= 0, NA, SPREAD),
    month_year = as.yearmon(ymd(date))
  ) %>%
  
  # Remove duplicate
  distinct(PERMNO, date, .keep_all = TRUE) %>%
  mutate(
    dollar_vol = PRC * VOL
  ) %>%
  # Retain only S&P500 stock
  filter(TICKER %in% sp500$Symbol)

# Create month variable for left join and retain only specific date value, then convert to monthly data
long_term_rate <- long_term_rate %>%
  mutate(
    month_year = as.yearmon(ymd(date))
  ) %>%
  filter(month_year >= "Jan 2010" & month_year <= "Dec 2022") %>% 
  group_by(month_year) %>%
  summarise(
    long_term_rate = mean(long_term_rate, na.rm = TRUE)
  )

# Same process as long term rate
short_term_rate <- risk_free_rate %>%
  mutate(
    month_year = as.yearmon(ymd(date))
  ) %>% 
  rename(short_term_rate = risk_free_rate) %>% 
  filter(month_year >= "Jan 2010" & month_year <= "Dec 2022") %>% 
  group_by(month_year) %>% 
  summarise(
    short_term_rate = mean(short_term_rate, na.rm = TRUE)
  )

# Similar process as previous
FFF_monthly <- fama_french_factors %>%
  mutate(
    month_year = as.yearmon(ymd(date))
  ) %>% 
  filter(month_year >= "Jan 2010" & month_year <= "Dec 2022") %>% 
  group_by(month_year) %>% 
  summarise(
    Mkt = mean(Mkt.RF, na.rm = TRUE),
    SMB = mean(SMB, na.rm = TRUE),
    HML = mean(HML, na.rm = TRUE)
  )

# Process the financial ratio dataset: divyield have a lot of missing value, better not to include it
financial_ratio <- Stock_data_2 %>%
      filter(TICKER != "") %>%
      mutate(
        month_year = as.yearmon(ymd(public_date))) %>%
      select(-divyield, -public_date)

# Join the financial ratio dataset 
task5_dataset2 <- task5_dataset1 %>% 
  left_join(
    financial_ratio, 
    by = c("month_year", "TICKER")
  ) 

# Join all the other datasets
task5_dataset3 <- task5_dataset2 %>% 
  left_join(
    long_term_rate,
    by = "month_year"
  ) %>% 
  left_join(
    short_term_rate, 
    by = "month_year"
  ) %>% 
  left_join(
    FFF_monthly, 
    by = "month_year"
  ) 

# Inspect the latest dataset
head(task5_dataset3, n = 5)
```

```{r}
# Transform to time series dataset (after trying different aggregation methods, take the average value may be the best)
task5_dataset4 <- task5_dataset3 %>% 
  group_by(date) %>% 
  summarise(
    dollar_vol = mean(dollar_vol, na.rm = TRUE),
    RET = mean(RET, na.rm = TRUE),
    SHROUT = mean(SHROUT, na.rm = TRUE),
    long_term_rate = mean(long_term_rate, na.rm = TRUE),
    short_term_rate = mean(short_term_rate, na.rm = TRUE),
    Mkt = mean(Mkt, na.rm = TRUE),
    SMB = mean(SMB, na.rm = TRUE),
    HML = mean(HML, na.rm = TRUE),
    CAPEI = mean(CAPEI, na.rm = TRUE),
    bm = mean(bm, na.rm = TRUE),
    evm = mean(evm, na.rm = TRUE),
    pe_exi = mean(pe_exi, na.rm = TRUE),
    dpr = mean(dpr, na.rm = TRUE),
    npm = mean(npm, na.rm = TRUE),
    roa = mean(roa, na.rm = TRUE),
    roe = mean(roe, na.rm = TRUE),
    roce = mean(roce, na.rm = TRUE),
    ptb = mean(ptb, na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  mutate(
    interest_rate_spread = long_term_rate - short_term_rate,
    std_ret = rollapply(RET, width = 30, FUN = sd, fill = NA, align = "right", na.rm = TRUE),
    volatility = std_ret * sqrt(252)
    ) %>% 
  filter(!is.na(volatility))

# Make sure it is a dataframe (otherwise cannot run the PCA)
task5_dataset5 <- as.data.frame(task5_dataset4 %>% select(-c(date, long_term_rate, short_term_rate, Mkt, std_ret)))

# Inspect dataset
summary(task5_dataset4)
str(task5_dataset5)
```

## Run PCA
```{r}
# Perform PCA
task5_pca <- prcomp(task5_dataset5 , centre = TRUE, scale. = TRUE)
summary(task5_pca)
print(task5_pca$rotation)

# Extract eigenvalues and variance explained
eigenvalues <- task5_pca$sdev^2
variance_explained <- eigenvalues / sum(eigenvalues)
cumulative_variance <- cumsum(variance_explained)
components <- 1:length(eigenvalues)
scree_data <- data.frame(Components = components, Eigenvalues = eigenvalues)

# Create screeplot
task5_screeplot <- ggplot(scree_data, aes(x = Components, y = Eigenvalues)) +
  
  geom_line(color = "blue") +
  
  geom_point(color = "blue", size = 2) +
  
  geom_hline(yintercept = 1, linetype = "dashed", color = "red") +
  
  labs(title = "Scree Plot of Principal Components", 
       x = "Principal Components", 
       y = "Proportion of Variance Explained") +
  
  theme_minimal() +
  
  theme(
    axis.title.x = element_text(size = 12),
    axis.title.y = element_text(size = 12),
    title = element_text(size = 14)) +
  
  scale_x_continuous(breaks = components) + 
  
  annotate("text", 
           x = max(components), 
           y = 1, 
           label = "Kaiser Criterion", 
           color = "red", 
           hjust = 1.1, 
           vjust = -1) +
  
  annotate("text", 
           x = 1, 
           y = max(eigenvalues), 
           label = "Eigenvalues", 
           color = "blue", 
           hjust = -0.2)

print(task5_screeplot)

# Save the file for report
save_plot(task5_screeplot, "task5 - scree plot")

# Create the eigenvalues summary table
pca_result <- data.frame(
  Principal_Component = 1:length(eigenvalues),
  Eigenvalue = eigenvalues,
  Variance_Explained = variance_explained,
  Cumulative_Variance = cumulative_variance
)

print(pca_result)
```

## Run a regression with the PCA result
```{r}
# Only extract the significant PC to a new dataframe
pca_regression_df <- data.frame(task5_pca$x)[, 1:6]

# Replace the PCA's RET with the original dataset RET (the S&P500 stocks' average returns)
pca_regression_df$RET <- task5_dataset5$RET 

# Add the market factor inside the dataset
pca_regression_df <- pca_regression_df %>%
  bind_cols(Mkt = task5_dataset4$Mkt)

# Run the regression
pca_regression_model <- lm(RET ~ ., data = pca_regression_df)

summary(pca_regression_model)
```

## Create a plot to show the loadings of each variable
```{r}
# Prepare the data
pca_loading_factor <- as.data.frame(task5_pca$rotation) %>% 
  select(c(PC1, PC2, PC3, PC4, PC5, PC6))
pca_loading_factor$Variable <- rownames(pca_loading_factor)
pca_loadings_df <- pivot_longer(pca_loading_factor %>% 
  filter(Variable != "RET"), 
                               cols = -Variable, 
                               names_to = "PC", 
                               values_to = "Loading") 

# Generate the plot - can observe in each PC, the variable is positive / negative, and the different factor representation in each PC
ggplot(pca_loadings_df, 
       aes(x = Variable, 
           y = Loading, 
           fill = PC)) +
  
  geom_bar(stat = "identity", 
           position = "dodge") +
  
  labs(title = "Loadings of each variable for significant PCs",
       x = "Variables",
       y = "Loadings")+ 
  
  facet_wrap(~ PC) +
  
  coord_flip() + 
  
  theme(axis.text.y = element_text(angle = 0)) 
```

#   6 Difference-in-Difference Analysis
## Data Preparation
```{r}
# Load library
library(lme4)
library(plm)  

# Import data
tick_pilot <- read.csv("Tick_Pilot_Test_Group.csv")
stock_1415 <- read.csv("DailyStocks_2014_2015.csv")
stock_1617 <- read.csv("DailyStocks_2016_2017.csv")
stock_1820 <- read.csv("DailyStocks_2018_2020.csv")

# Make sure all the "date" are in same format
tick_pilot$Effective_Date <- as.Date(tick_pilot$Effective_Date, format = "%Y-%m-%d")
stock_1415$date <- as.Date(stock_1415$date, format = "%Y-%m-%d")
stock_1617$date <- as.Date(stock_1617$date, format = "%Y-%m-%d")
stock_1820$date <- as.Date(stock_1820$date, format = "%Y-%m-%d")
```

Inspect data
```{r echo=FALSE}
cat("The column name is the same: ", identical(names(stock_1415), names(stock_1617)))
cat("The column name is the same:", identical(names(stock_1415), names(stock_1820)))
cat("The column name is the same:", identical(names(stock_1617), names(stock_1820)))

if (any(sapply(stock_1415, class) != sapply(stock_1820, class))) {
  cat("Differences in structure between stock_1415 and stock_1820: ",
  names(sapply(stock_1415, class)[sapply(stock_1415, class) != sapply(stock_1820, class)]))
}

if (any(sapply(stock_1617, class) != sapply(stock_1820, class))) {
   cat("Differences in structure between stock_1617 and stock_1820: ", 
  names(sapply(stock_1415, class)[sapply(stock_1415, class) != sapply(stock_1820, class)]))
}

cat("Since we will not use NUMTRD in this analysis, no need to action.")

```

## Short-term effect
### Create dataset
The program effective date is 2016-10-01, consider the potential for initial volatility, allow the market a month to adjust, the start date of post period is chosen as November 1, 2016, and observe the short term effect within six months. For pre-period, the same dates in the previous year of post-period can better control for seasonal effects and other factors that may influence bid-ask spreads.
```{R}
# Combine datasets
short_period <- stock_1415 %>%  
  bind_rows(stock_1617) %>%
  filter(date >= "2015-11-01" & date < "2016-10-01" | date >= "2016-11-01" & date <= "2017-01-31") %>% 
  select(PERMNO, date, TICKER, BID, ASK) %>% 
  filter(TICKER %in% tick_pilot$Ticker_Symbol)

# Inspect dataset
head(short_period, n = 5)
```

### Data wrangling
```{r}
# Clean data
short_period_df1 <- short_period %>%
  mutate(
    # Handle NA values
    ASK = ifelse(ASK - BID < 0, NA, as.numeric(as.character(ASK))),
    BID = ifelse(ASK - BID < 0, NA, as.numeric(as.character(BID)))
  ) %>%
  # Remove duplicate
  distinct(PERMNO, date, .keep_all = TRUE) %>% 
  group_by(PERMNO) %>% 
  # Compute relative bid-ask spread
  mutate(
    bid_ask_spread = (ASK - BID) / ((ASK + BID) / 2)
    ) %>% 
  ungroup() %>% 
  select(date, TICKER, bid_ask_spread) %>% 
  # Define group for each security
  left_join(tick_pilot %>% 
              select(Ticker_Symbol, Tick_Size_Pilot_Program_Group), by = c("TICKER" = "Ticker_Symbol")) %>% 
  rename(Test_group_ = Tick_Size_Pilot_Program_Group) %>% 
  filter(!is.na(bid_ask_spread)) %>% 
  # Define period for pre and post period
  mutate(
    d_after = ifelse(date < "2016-10-01", 0, 1),
    Test_group_ = factor(Test_group_)
  ) 

# Inspect number of stocks for each group (Short period)
print(short_period_df1 %>%
  group_by(Test_group_) %>%
  summarise(Ticker_per_group = n_distinct(TICKER)))

head(short_period_df1, n = 5)
```

### Descriptive Statistics
```{r}
# Observe the average and standard deviation bid-ask spread value for each group
print(short_period_df1 %>% 
  group_by(Test_group_) %>% 
  summarise(
    mean_spread = mean(bid_ask_spread, na.rm = TRUE), 
    sd_spread = sd(bid_ask_spread, na.rm = TRUE)
  ))
```

### DiD regressions
```{R}
# Run regression with fixed effect
DiD_model_1 <- plm(bid_ask_spread ~ Test_group_ * d_after,
                   data = short_period_df1,
                   index = c("date", "TICKER"),
                   model = "within")
summary(DiD_model_1)

# Determine the impact of fixed effect
DiD_model_1_random <- plm(bid_ask_spread ~ Test_group_ * d_after,
                   data = short_period_df1,
                   index = c("date", "TICKER"),
                   model = "random")

summary(DiD_model_1_random)

```
The result is quite similar, but not exactly the same, suggesting that fixed effect is somehow influencing, it is better to use the model with fixed effect

## Long-term effect

The program effective for two years. Observe the long-term effect:
```{r}
# To compute volatility for specific period, separate into two dataset first

# The pre-period should be the same as the short-term effect (it is the baseline, should be consistent)
pre_treatment <- stock_1415 %>% 
  bind_rows(stock_1617) %>% 
  filter(date >= "2015-10-01" & date <= "2016-09-30") %>% 
  select(PERMNO, date, TICKER, PRC, VOL, RET, BID, ASK, SHROUT) %>% 
  filter(TICKER %in% tick_pilot$Ticker_Symbol) 

# Using the first effective month to compute the rolling standard deviation, therefore, the post-period actually is from November 1, 2016
post_treatment <- stock_1617 %>% 
  bind_rows(stock_1820) %>% 
  filter(date >= "2016-10-01" & date <= "2018-09-30") %>% 
  select(PERMNO, date, TICKER, PRC, VOL, RET, BID, ASK, SHROUT) %>% 
  filter(TICKER %in% tick_pilot$Ticker_Symbol)

```

### Data wrangling
```{r}
# Define trading days (for calculate volatility) - typically trading day is 252, this code also make sure the number of observation for the data
trading_days_pre <- as.integer(length(unique(pre_treatment$date)) / 2)
trading_days_post <- as.integer(length(unique(post_treatment$date)) / 2)

# Clean data (pre-period)
pre_treatment_df1 <- pre_treatment %>%
  mutate(
    PERMNO = as.character(PERMNO),
    PRC = abs(PRC),
    # Handle missing values
    PRC = ifelse(PRC == 0.0, NA, PRC),
    VOL = ifelse(VOL == -99, NA, VOL),
    RET = case_when(
      RET %in% c(-44, -55, -66, -77, -88, -99) ~ NA,  
      RET %in% c(".A", ".B", ".C", ".D") ~ NA,        
      TRUE ~ as.numeric(as.character(RET))
    ),
    ASK = ifelse(ASK - BID < 0, NA, as.numeric(as.character(ASK))),
    BID = ifelse(ASK - BID < 0, NA, as.numeric(as.character(BID)))
  ) %>%
  # Remove duplicate 
  distinct(PERMNO, date, .keep_all = TRUE) %>% 
  arrange(PERMNO, date) %>% 
  group_by(PERMNO) %>% 
  # Compute relevant variables
  mutate(
    mean_ret = rollmean(RET, k = 30, fill = NA, align = "right", na.rm = TRUE),
    std_ret = rollapply(RET, width = 30, FUN = sd, fill = NA, align = "right", na.rm = TRUE),
    volatility = std_ret * sqrt(trading_days_pre),
    dollar_vol = PRC * VOL,
    market_cap = SHROUT * PRC * 1000,
    bid_ask_spread = (ASK - BID) / ((ASK + BID) / 2)
    ) %>% 
  ungroup() %>% 
  select(PERMNO, date, TICKER, bid_ask_spread, volatility, dollar_vol, market_cap)

# Clean data (post-period)
post_treatment_df1 <- post_treatment %>%
  mutate(
    PERMNO = as.character(PERMNO),
    PRC = abs(PRC),
    # Handle missing values
    PRC = ifelse(PRC == 0.0, NA, PRC),
    VOL = ifelse(VOL == -99, NA, VOL),
    RET = case_when(
      RET %in% c(-44, -55, -66, -77, -88, -99) ~ NA,  
      RET %in% c(".A", ".B", ".C", ".D") ~ NA,        
      TRUE ~ as.numeric(as.character(RET))
    ),
    ASK = ifelse(ASK - BID < 0, NA, as.numeric(as.character(ASK))),
    BID = ifelse(ASK - BID < 0, NA, as.numeric(as.character(BID)))
  ) %>%
  # Remove duplicate
  distinct(PERMNO, date, .keep_all = TRUE) %>% 
  group_by(PERMNO) %>% 
  # Compute relevant variables
  mutate(
    mean_ret = rollmean(RET, k = 30, fill = NA, align = "right", na.rm = TRUE),
    std_ret = rollapply(RET, width = 30, FUN = sd, fill = NA, align = "right", na.rm = TRUE),
    volatility = std_ret * sqrt(trading_days_post),
    dollar_vol = PRC * VOL,
    market_cap = SHROUT * PRC * 1000,
    bid_ask_spread = (ASK - BID) / ((ASK + BID) / 2)
    ) %>% 
  ungroup() %>% 
  select(PERMNO, date, TICKER, bid_ask_spread, volatility, dollar_vol, market_cap)

# Combined dataset and change the column name (it will look clearer in the regression coefficients)
task6_panel <- pre_treatment_df1 %>% bind_rows(post_treatment_df1) %>% 
  left_join(tick_pilot %>% 
              select(Ticker_Symbol, Tick_Size_Pilot_Program_Group), by = c("TICKER" = "Ticker_Symbol")) %>% 
  rename(Test_group_ = Tick_Size_Pilot_Program_Group) 

# Remove missing values, create period binary variable, and standardise variables to make sure they are on the same scale
task6_panel_cleaned <- task6_panel %>% 
  filter(!is.na(bid_ask_spread) & !is.na(volatility))%>% 
  mutate(
    d_after = ifelse(date < "2016-10-01", 0, 1),
    scaled_spread = scale(bid_ask_spread),
    scaled_volatility = scale(volatility),
    scaled_dollar_vol = scale(dollar_vol),
    scaled_market_cap = scale(market_cap),
    Test_group_ = factor(Test_group_)
  ) 

# Inspect number of stocks for each group
print(task6_panel_cleaned %>%
  group_by(Test_group_) %>%
  summarise(Ticker_per_group = n_distinct(TICKER)))
```

### Descriptive Statistics
```{r}
# Observe the average value for each group
print(task6_panel %>% 
  group_by(Test_group_) %>% 
  summarise(
    mean_spread = mean(bid_ask_spread, na.rm = TRUE), 
    sd_spread = sd(bid_ask_spread, na.rm = TRUE)
  ))

# Visualise the average bid-ask spread across the entire period
task6_time_series <- ggplot(task6_panel, 
                            aes(x = date, 
                                y = bid_ask_spread, 
                                color = as.factor(Test_group_))) +
  
  geom_line(stat = "summary", 
            fun = "mean", 
            show.legend = FALSE) +
  
  labs(title = "Overall Trends in Bid-Ask Spread", 
       subtitle = "(Program Began on October 2016)",
       x = "Date", 
       y = "Average Relative Bid-Ask Spread") + 
  
  scale_x_date(breaks = seq(as.Date("2015-11-01"), 
                            as.Date("2019-11-01"), 
                            by = "6 months"),
               labels = date_format("%Y-%m-%d")) + 
  
  # Increase readiness
  facet_wrap(~ as.factor(Test_group_), scales = "free",
             labeller = as_labeller(c("C" = "Control Group",
                                      "G1" = "Test Group One",
                                      "G2" = "Test Group Two",
                                      "G3" = "Test Group Three"))) +
  
  # Add a line to underlying the effective date
  geom_vline(xintercept = as.Date("2016-10-01"), linetype = "dashed", color = "black") + 
  
  theme(axis.text.x = element_text(angle = 55, hjust=1))


print(task6_time_series)

# Save the plot for report
save_plot(task6_time_series, "task6 time series plot")
```

### DiD regressions
```{R}
# Run regression with fixed effect
DiD_model_2 <- plm(bid_ask_spread ~ Test_group_ * d_after,
                   data = task6_panel_cleaned,
                   index = c("date", "PERMNO"),
                   model = "within")
summary(DiD_model_2)

# Observe the importance of fixed effect
DiD_model_2_random <- plm(bid_ask_spread ~ Test_group_ * d_after,
                   data = task6_panel_cleaned,
                   index = c("date", "TICKER"),
                   model = "random")

summary(DiD_model_2_random)

# Similar, but better to use the fixed effect version
```

### Other factors that may influence bid-ask spreads
```{R}
# Interact with volatility
DiD_model_3 <- plm(scaled_spread ~ Test_group_ * d_after + Test_group_ * d_after * scaled_volatility,
                   data = task6_panel_cleaned,
                   index = c("date", "TICKER"),
                   model = "within")

summary(DiD_model_3)
```

```{R}
# Interact with dollar volume
DiD_model_4 <- plm(scaled_spread ~ Test_group_ * d_after + Test_group_ * d_after * scaled_dollar_vol,
                   data = task6_panel_cleaned,
                   index = c("date", "TICKER"),
                   model = "within")

summary(DiD_model_4)
```

```{R}
# Interact with market capitalisation
DiD_model_5 <- plm(scaled_spread ~ Test_group_ * d_after + Test_group_ * d_after * scaled_market_cap,
                   data = task6_panel_cleaned,
                   index = c("date", "TICKER"),
                   model = "within")

summary(DiD_model_5)
```